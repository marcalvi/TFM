

</file_summary>

<directory_structure>
assets/
  healnet_overview_caption.png
  healnet_overview.png
config/
  best_hyperparams.yml
  hyperparameters.yml
  latent_array_sweep.yaml
  main_gpu.yml
  main_local.yml
  main.yml
  perceiver_sweep.yaml
  sweep_bayesian.yaml
  sweep_grid.yaml
  sweep.yaml
data/
  tcga/
    gdc_manifests/
      filtered/
        blca_tmp.txt
        blca_wsi_manifest_filtered.txt
        brca_wsi_manifest_filtered.txt
        hnsc_wsi_manifest_filtered.txt
        kirp_wsi_manifest_filtered.txt
        luad_wsi_manifest_filtered.txt
        lusc_wsi_manifest_filtered.txt
        paad_wsi_manifest_filtered.txt
        ucec_wsi_manifest_filtered.txt
      full/
        blca_wsi_manifest_full.txt
        brca_wsi_manifest_full.txt
        kirp_wsi_manifest_full.txt
        ucec_wsi_manifest_full.txt
    omic/
      tcga_blca_all_clean.csv.zip
      tcga_brca_all_clean.csv.zip
      tcga_coadread_all_clean.csv.zip
      tcga_gbmlgg_all_clean.csv.zip
      tcga_hnsc_all_clean.csv.zip
      tcga_kirc_all_clean.csv.zip
      tcga_kirp_all_clean.csv.zip
      tcga_lihc_all_clean.csv.zip
      tcga_luad_all_clean.csv.zip
      tcga_lusc_all_clean.csv.zip
      tcga_paad_all_clean.csv.zip
      tcga_skcm_all_clean.csv.zip
      tcga_stad_all_clean.csv.zip
      tcga_ucec_all_clean.csv.zip
    omic_xena/
      blca_master.csv
      brca_master.csv
      kirp_master.csv
      ucec_master.csv
healnet/
  baselines/
    __init__.py
    generic.py
    mcat.py
    mm_prognosis.py
    motcat.py
  etl/
    __init__.py
    base.py
    loaders.py
    preprocess.py
    utils.py
  models/
    __init__.py
    base.py
    baselines.py
    classification_utils.py
    explainer.py
    healnet.py
    optimizer.py
    survival_loss.py
  tests/
    test_baselines.py
    test_healnet.py
  utils/
    __init__.py
    config.py
    loading.py
    train_utils.py
    wb.py
  __init__.py
  fusion.py
  main.py
  train_utils.py
  train.py
  utils.py
tutorial/
  01_Getting_Started.ipynb
.gitattributes
.gitignore
environment.yml
False
LICENSE
pyproject.toml
README.md
run_plan.sh
setup.cfg
sweep.yaml
tasks.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="config/best_hyperparams.yml">
blca:
  output_dims: 4 # refers to n_classes for classification, n_bins for survival
  class_weights: inverse # one of inverse, inverse_root, None; only relevant for classification, not survival
  l1: 0.00001165096910978314
  num_freq_bands: 2
  depth: 2
  max_freq: 2.
  num_latents: 25
  latent_dim: 119
  cross_dim_head: 16
  latent_dim_head: 127
  cross_heads: 1
  latent_heads: 8
  attn_dropout: 0.08301797961946294
  ff_dropout: 0.47333212210978054
  fourier_encode_data: True
  self_per_cross_attn: 0  # if 0, no self attention at all
  weight_tie_layers: False # share weights between layers if False | KEEP THIS, otherwise model size is quite large
  snn: True

brca:
  output_dims: 4 # refers to n_classes for classification, n_bins for survival
  class_weights: inverse # one of inverse, inverse_root, None; only relevant for classification, not survival
  l1: 0.00000682386175773137
  num_freq_bands: 2
  depth: 2
  max_freq: 2.
  num_latents: 17
  latent_dim: 126
  cross_dim_head: 63
  latent_dim_head: 20
  cross_heads: 1
  latent_heads: 8
  attn_dropout: 0.45526926537716805
  ff_dropout: 0.364741344399059
  fourier_encode_data: True
  self_per_cross_attn: 0  # if 0, no self attention at all
  weight_tie_layers: False # share weights between layers if False | KEEP THIS, otherwise model size is quite large
  snn: True

kirp:
  output_dims: 4 # refers to n_classes for classification, n_bins for survival
  class_weights: inverse # one of inverse, inverse_root, None; only relevant for classification, not survival
  l1: 0.00004627399922284714
  num_freq_bands: 2
  depth: 5
  max_freq: 2.
  num_latents: 17
  latent_dim: 62
  cross_dim_head: 27
  latent_dim_head: 113
  cross_heads: 1
  latent_heads: 8
  attn_dropout: 0.31789955176609086
  ff_dropout: 0.04735283995174411
  fourier_encode_data: True
  self_per_cross_attn: 0  # if 0, no self attention at all
  weight_tie_layers: False # share weights between layers if False | KEEP THIS, otherwise model size is quite large
  snn: True

ucec:
  output_dims: 4 # refers to n_classes for classification, n_bins for survival
  class_weights: inverse # one of inverse, inverse_root, None; only relevant for classification, not survival
  l1: 0.00031851345313479773
  num_freq_bands: 2
  depth: 2
  max_freq: 2.
  num_latents: 16
  latent_dim: 65
  cross_dim_head: 103
  latent_dim_head: 51
  cross_heads: 1
  latent_heads: 8
  attn_dropout: 0.24884810910180033
  ff_dropout: 0.05707504857865214
  fourier_encode_data: True
  self_per_cross_attn: 0  # if 0, no self attention at all
  weight_tie_layers: False # share weights between layers if False | KEEP THIS, otherwise model size is quite large
  snn: True
</file>

<file path="config/hyperparameters.yml">
blca:
  output_dims: 4 # refers to n_classes for classification, n_bins for survival
  class_weights: inverse # one of inverse, inverse_root, None; only relevant for classification, not survival
  l1: 0.0001
  num_freq_bands: 2
  depth: 2
  max_freq: 2.
  num_latents: 4
  latent_dim: 16
  cross_dim_head: 16
  latent_dim_head: 16
  cross_heads: 1
  latent_heads: 8
  attn_dropout: 0.5
  ff_dropout: 0.5
  fourier_encode_data: True
  self_per_cross_attn: 0  # if 0, no self attention at all
  weight_tie_layers: False # share weights between layers if False | KEEP THIS, otherwise model size is quite large
brca:
  output_dims: 4 # refers to n_classes for classification, n_bins for survival
  class_weights: inverse # one of inverse, inverse_root, None; only relevant for classification, not survival
  l1: 0.0001
  num_freq_bands: 2
  depth: 2
  max_freq: 2.
  num_latents: 4
  latent_dim: 16
  cross_dim_head: 16
  latent_dim_head: 16
  cross_heads: 1
  latent_heads: 8
  attn_dropout: 0.5
  ff_dropout: 0.5
  fourier_encode_data: True
  self_per_cross_attn: 0  # if 0, no self attention at all
  weight_tie_layers: False # share weights between layers if False | KEEP THIS, otherwise model size is quite large
kirp:
  output_dims: 4 # refers to n_classes for classification, n_bins for survival
  class_weights: inverse # one of inverse, inverse_root, None; only relevant for classification, not survival
  l1: 0.0001
  num_freq_bands: 2
  depth: 2
  max_freq: 2.
  num_latents: 4
  latent_dim: 16
  cross_dim_head: 16
  latent_dim_head: 16
  cross_heads: 1
  latent_heads: 8
  attn_dropout: 0.5
  ff_dropout: 0.5
  fourier_encode_data: True
  self_per_cross_attn: 0  # if 0, no self attention at all
  weight_tie_layers: False # share weights between layers if False | KEEP THIS, otherwise model size is quite large
ucec:
  output_dims: 4 # refers to n_classes for classification, n_bins for survival
  class_weights: inverse # one of inverse, inverse_root, None; only relevant for classification, not survival
  l1: 0.0001
  num_freq_bands: 2
  depth: 2
  max_freq: 2.
  num_latents: 4
  latent_dim: 16
  cross_dim_head: 16
  latent_dim_head: 16
  cross_heads: 1
  latent_heads: 8
  attn_dropout: 0.5
  ff_dropout: 0.5
  fourier_encode_data: True
  self_per_cross_attn: 0  # if 0, no self attention at all
  weight_tie_layers: False # share weights between layers if False | KEEP THIS, otherwise model size is quite large
</file>

<file path="config/latent_array_sweep.yaml">
method: bayes
metric:
  goal: minimize
  name: val_c_index
parameters:
  model_params.num_latents:
    values:
      - 4
      - 32
      - 64
      - 256
      - 1024
  model_params.latent_dim:
    values:
      - 4
      - 32
      - 64
      - 256
      - 1024
program: x_perceiver/main.py
</file>

<file path="config/main_gpu.yml">
data_path: /net/archive/export/tcga
tcga_path: /net/archive/export/tcga/tcga
gdc_client: healnet/gdc-client
log_path: healnet/logs
seed: 1
hyperparams: config/best_hyperparams.yml

dataset: kirp #  [blca, brca, ucec, kirp]
model: healnet # [fcnn, healnet, healnet_early, mcat]

explainer: False
missing_ablation: False
omic_attention: True 

n_folds: 1

wandb: False # log to weights and biases (login required)


data:
  # Parameters for running on raw WSIs
  resize: True #
  resize_height: 1024 # if resize is True
  resize_width: 1024 # if resize is True
  wsi_level: 2 # WSI resolution level (if available)
  # Parameters for patch pre-processing
  patch_size: 256 # don't change if used for pre-processing

sources: # select which data sources to use, list of string, valid values: omic, slides
  - omic
  - slides

survival: # all parameters related to survival task
  loss: nll # valid: nll, ce_survival, cox
  subset: uncensored # subset used to calculate survival bin cutoffs, valid: all, censored, uncensored

train_loop:
  eval_interval: 1 # evaluate on validation set every n epochs
  batch_size: 4
  epochs: 50 # max training epochs
  early_stopping: True # stop training if validation loss doesn't improve for `patience` epochs
  patience: 5 # number of epochs to wait for improvement before stopping training

optimizer:
  max_lr: 0.008 # maximum learning rate for OneCycleLR scheduler
  lr: 0.007765016508403882 # used for all
  momentum: 0.92 # momentum
  weight_decay: None # L2 regularisation


# other `model_params` are specified in config/best_hyperparams.yml
</file>

<file path="config/main_local.yml">
data_path: data/
tcga_path: data/tcga/
gdc_client: ./gdc-client
log_path: logs/
seed: 42

dataset: blca

wandb: True

data:
  # image preprocessing
  resize: True # ONLY use for debugging - manually resize images
  resize_height: 1024 # if resize is True
  resize_width: 1024 # if resize is True
  patch_size: 256 # don't changed if used for pre-processing
  wsi_level: 2 # WSI resolution level (if available)
  patching: False # if True, patching is done in the data loader, else whole image is used



task: survival # [survival, classification]
sources:
#  - omic
  - slides

model: perceiver # [fcnn, perceiver]
#model: fcnn



survival: # all parameters related to survival task
  loss: nll # valid: nll, ce_survival, cox
  subset: uncensored # subset used to calculate survival bin cutoffs

train_loop:
  checkpoint_interval: 10
  eval_interval: 5
  batch_size: 4
  epochs: 100

optimizer:
  max_lr: 0.005
  lr: 0.002
  momentum: 0.95
  weight_decay: 0.0001 # or None


model_params:
  # OMIC
#  output_dims: 4 # refers to n_classes for classification, n_bins for survival
#  class_weights: inverse # one of inverse, inverse_root, None; only relevant for classification, not survival
#  num_freq_bands: 2
#  depth: 1
#  max_freq: 2.
#  num_latents: 4
#  latent_dim: 4
#  cross_dim_head: 16
#  latent_dim_head: 16
#  cross_heads: 1
#  latent_heads: 8
#  attn_dropout: 0
#  ff_dropout: 0
#  fourier_encode_data: True
#  self_per_cross_attn: 1  # if 0, no self attention at all
#  weight_tie_layers: False # share weights between layers if False | KEEP THIS, otherwise model size is quite large

  # SLIDES
  output_dims: 4 # refers to n_classes for classification, n_bins for survival
  class_weights: inverse # one of inverse, inverse_root, None; only relevant for classification, not survival
  num_freq_bands: 2
  depth: 3
  max_freq: 2.
  num_latents: 4
  latent_dim: 16
  cross_dim_head: 64
  latent_dim_head: 64
  cross_heads: 1
  latent_heads: 8
  attn_dropout: 0.3
  ff_dropout: 0.3
  fourier_encode_data: True
  self_per_cross_attn: 1  # if 0, no self attention at all
  weight_tie_layers: False # share weights between layers if False | KEEP THIS, otherwise model size is quite large
</file>

<file path="config/main.yml">
data_path: data/
tcga_path: data/tcga/
gdc_client: ./gdc-client
log_path: logs/
eed: 42

wandb: True

data:
  level: 2
  patch_size: 256

task: survival # [survival, classification]
sources:
#  - omic
  - slides

model: perceiver # [fcnn, perceiver]

survival: # all parameters related to survival task
  loss: nll # valid: nll, ce_survival, cox

train_loop:
  checkpoint_interval: 10
  eval_interval: 2
  batch_size: 4
  epochs: 50

optimizer:
  max_lr: 0.005
  lr: 0.001
  momentum: 0.9
  weight_decay: 0.0001 # or None

preprocessing:
  resize_height: 256
  resize_width: 256

model_params:
  output_dims: 4 # refers to n_classes for classificaiton, n_bins for survival
  weight_decay: True
  class_weights: inverse # one of inverse, inverse_root, None; only relevant for classification, not survival
  num_freq_bands: 6
  depth: 6
  max_freq: 2.
  num_latents: 256 #
  latent_dim: 32
  cross_dim_head: 16
  latent_dim_head: 16
  cross_heads: 1
  latent_heads: 8
  attn_dropout: 0.5
  ff_dropout: 0.5
  fourier_encode_data: True
  self_per_cross_attn: 1  # if 0, no self attention at all
  weight_tie_layers: False # share weights between layers if False | KEEP THIS, otherwise model size is quite large
</file>

<file path="config/perceiver_sweep.yaml">
method: grid
metric:
  goal: maximize
  name: val_f1
parameters:
  model_params.weight_decay:
    values:
      - 0
      - 0.0001
      - 0.0005
  model_params.num_freq_bands:
    values:
      - 2
      - 8
  model_params.depth:
    values:
      - 1
      - 3
      - 5
  model_params.max_freq:
    values:
      - 2.
      - 8.
  model_params.num_latents:
    values:
      - 4
      - 16
      - 32
      - 64
  model_params.latent_dim:
    values:
      - 4
      - 16
      - 32
      - 64
  model_params.attn_dropout:
    values:
      - 0
      - 0.5
      - 0.25
  model_params.ff_dropout:
    values:
      - 0
      - 0.5
      - 0.25
  optimizer.lr:
    values:
      - 0.002
      - 0.001
      - 0.0005
  optimizer.momentum:
    values:
      - 0.9
      - 0.95
      - 0.8
program: x_perceiver/main.py
</file>

<file path="config/sweep_bayesian.yaml">
method: bayes
metric:
  goal: maximize
  name: mean_test_c_index
parameters:
  sources:
    value: ["omic", "slides"]
  explainer:
    value: False
  n_folds:
    value: 5
  train_loop.epochs:
      value: 50
  train_loop.patience:
      value: 5
  train_loop.batch_size:
      value: 8
  model_params.l1:
    distribution: uniform
    min: 0.0
    max: 0.0005
  model_params.depth:
    distribution: int_uniform
    min: 2
    max: 5
  model_params.num_latents:
    distribution: int_uniform
    min: 8
    max: 32
  model_params.latent_dim:
    distribution: int_uniform
    min: 16
    max: 128
  model_params.cross_heads:
    distribution: int_uniform
    min: 1
    max: 32
  model_params.cross_dim_head:
    distribution: int_uniform
    min: 16
    max: 128
  model_params.attn_dropout:
    distribution: uniform
    min: 0.0
    max: 0.5
  model_params.ff_dropout:
    min: 0.0
    max: 0.5
program: healnet/main.py
</file>

<file path="config/sweep_grid.yaml">
method: grid
metric:
  goal: maximize
  name: val_f1
parameters:
  model_params.weight_decay:
    values:
      - 0
      - 0.0001
      - 0.0005
  model_params.num_freq_bands:
    values:
      - 2
      - 8
  model_params.depth:
    values:
      - 1
      - 3
      - 5
  model_params.max_freq:
    values:
      - 2.
      - 8.
  model_params.num_latents:
    values:
      - 4
      - 16
      - 32
      - 64
  model_params.latent_dim:
    values:
      - 4
      - 16
      - 32
      - 64
  model_params.attn_dropout:
    values:
      - 0
      - 0.5
      - 0.25
  model_params.ff_dropout:
    values:
      - 0
      - 0.5
      - 0.25
  optimizer.lr:
    values:
      - 0.002
      - 0.001
      - 0.0005
  optimizer.momentum:
    values:
      - 0.9
      - 0.95
      - 0.8
program: healnet/main.py
</file>

<file path="config/sweep.yaml">
method: grid
metric:
  goal: minimize
  name: val_f1
parameters:
  clf.n_classes:
    value: 4
  data.level:
    value: 3
  data_path:
    value: /net/archive/export/tcga
  gdc_client:
    value: /home/kh701/gdc-client
  log_path:
    value: /home/kh701/pycharm/x-perceiver/logs
  model.output_dims:
    value: 4
  model_params.output_dims:
    value: 4
  model_params.weight_decay:
    distribution: uniform
    min: 0
    max: 0.001
  model_params.num_freq_bands:
    distribution: int_uniform
    min: 2
    max: 8
  model_params.depth:
    distribution: int_uniform
    min: 1
    max: 4
  model_params.max_freq:
    distribution: int_uniform
    min: 2.
    max: 8.
  model_params.num_latents:
    values:
      - 4
      - 16
      - 32
      - 64
  model_params.latent_dim:
    values:
      - 4
      - 16
      - 32
      - 64
  model_params.attn_dropout:
    distribution: uniform
    min: 0
    max: 0.9
  model_params.ff_dropout:
    distribution: uniform
    min: 0
    max: 0.9
  optimizer.lr:
    distribution: uniform
    max: 0.002
    min: 0.0005
  optimizer.max_lr:
    value: 0.002
  optimizer.momentum:
    distribution: uniform
    max: 0.9
    min: 0.0
  seed:
    value: 42
  survival.loss:
    value: nll
  task:
    value: classification
  tcga_path:
    value: /net/archive/export/tcga/tcga
  train_loop.batch_size:
    value: 4
  train_loop.checkpoint_interval:
    value: 20
  train_loop.epochs:
    value: 20
  train_loop.eval_interval:
    value: 10
  wandb:
    value: "True"
program: x_perceiver/main.py
</file>


<file path="data/tcga/omic_xena/blca_master.csv">
version https://git-lfs.github.com/spec/v1
oid sha256:9fa2cd83906c00e1f50113ba8e806ea4537806aeb2aacbee60e86c80b53f6929
size 9318450
</file>

<file path="data/tcga/omic_xena/brca_master.csv">
version https://git-lfs.github.com/spec/v1
oid sha256:de50ac72b3f77b7598b04225c9a4726c2f6904e093e241884afb69c72847cdcc
size 24207044
</file>

<file path="data/tcga/omic_xena/kirp_master.csv">
version https://git-lfs.github.com/spec/v1
oid sha256:520f911b110b28635047b6267662080e6350a9888c697919ab906721bb4d6751
size 6361784
</file>

<file path="data/tcga/omic_xena/ucec_master.csv">
version https://git-lfs.github.com/spec/v1
oid sha256:735bd9675d3e771a4b22d8abbcd3f608801815fc631595449d9655b5d5896e18
size 5717257
</file>

<file path="healnet/baselines/__init__.py">
from healnet.baselines.mm_prognosis import MMPrognosis
from healnet.baselines.generic import FCNN, RegularizedFCNN
from healnet.baselines.mcat import MCAT, SNN, MILAttentionNet
from healnet.baselines.multimodn.better_multimodn import MultiModNModule

__all__ = ["MMPrognosis", "FCNN", "RegularizedFCNN", "MCAT", "SNN", "MILAttentionNet"]
</file>

<file path="healnet/baselines/generic.py">
"""
Some relatively generic baseline models for comparison - mainly using the regularised FCNN
"""

import torch.nn as nn
import torch
import torch.nn.functional as F
from typing import *

class FCNN(nn.Module):

    def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int, dropout: float = 0.5):
        super().__init__()

        # Construct topology
        self.input_layer = nn.Linear(input_size, hidden_sizes[0])
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.hidden_layers = nn.ModuleList([
            nn.Linear(hidden_sizes[i], hidden_sizes[i+1]) for i in range(len(hidden_sizes)-1)
        ])

        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)

    def forward(self, x):
        # only select second to last dimension
        x = x.squeeze()
        # x = einops.rearrange(x, 'b feat c -> feat c')
        x = self.relu(self.input_layer(x))
        for layer in self.hidden_layers:
            # apply droput and acvitations to hidden layers
            x = self.relu(self.dropout(layer(x)))
        x = self.output_layer(x)
        return x

class RegularizedFCNN(nn.Module):
    def __init__(self, output_dim, dropout_rate=0.2, l1_penalty=0.01, l2_penalty=0.01):
        super(RegularizedFCNN, self).__init__()

        # Store the attributes
        self.output_dim = output_dim
        self.dropout_rate = dropout_rate
        self.l1_penalty = l1_penalty
        self.l2_penalty = l2_penalty

        # Placeholder for layers
        self.input_layer = None
        self.hidden_layer = nn.Linear(128, 64)
        self.dropout_layer = nn.Dropout(dropout_rate)
        self.output_layer = nn.Linear(64, output_dim)

    def forward(self, inputs: List[torch.Tensor]):

        if type(inputs) == list:
            inputs = inputs[0]

        # Get the input dimension and create the input layer if it doesn't exist
        if self.input_layer is None:
            input_dim = inputs.shape[1]
            self.input_layer = nn.Linear(input_dim, 128).to(inputs.device)

        x = F.relu(self.input_layer(inputs))
        x = F.relu(self.hidden_layer(x))
        x = self.dropout_layer(x)
        return torch.sigmoid(self.output_layer(x))

    def l1_regularization(self):
        l1_reg = torch.tensor(0., requires_grad=True)
        for name, param in self.named_parameters():
            if 'weight' in name:
                l1_reg = l1_reg + torch.norm(param, 1)
        return self.l1_penalty * l1_reg

    def l2_regularization(self):
        l2_reg = torch.tensor(0., requires_grad=True)
        for name, param in self.named_parameters():
            if 'weight' in name:
                l2_reg = l2_reg + torch.norm(param, 2)
        return self.l2_penalty * l2_reg
</file>

<file path="healnet/baselines/mcat.py">
"""
Baseline implementation of paper:
Multimodal Co-Attention Transformer for Survival Prediction in Gigapixel Whole Slide Images
Mostly based on their GitHub repository and adjusted for our pipeline.
https://github.com/mahmoodlab/MCAT/tree/master
"""
import einops
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.functional import *
import torch
from torch import Tensor
from typing import *
# from torch.nn.modules.linear import _LinearWithBias
from torch.overrides import has_torch_function, handle_torch_function
from torch.nn.init import xavier_uniform_
from torch.nn.init import constant_
from torch.nn.init import xavier_normal_
from torch.nn.parameter import Parameter
from torch.nn import Module
from collections import OrderedDict
from os.path import join
import pdb
import warnings


# from models.model_utils import *


class MCAT(nn.Module):
    def __init__(self, omic_shape: Tuple, wsi_shape: Tuple, fusion='concat', n_classes=4,
                 model_size_wsi: str='small', model_size_omic: str='small', dropout=0.25):
        """
        Multimodal Co-Attention Transformer
        Args:
            fusion:
            omic_shape:
            n_classes:
            model_size_wsi:
            model_size_omic:
            dropout:
        """
        super(MCAT, self).__init__()
        self.fusion = fusion
        self.omic_shape = omic_shape
        self.wsi_shape = wsi_shape
        self.n_classes = n_classes
        self.size_dict_WSI = {"small": [1024, 256, 256], "big": [1024, 512, 384]}
        self.size_dict_omic = {'small': [256, 256], 'big': [1024, 1024, 1024, 256]}

        ### FC Layer over WSI bag
        size = self.size_dict_WSI[model_size_wsi]
        size[0] = wsi_shape[0] # other embedding dims
        # fc = [nn.Linear(wsi_shape[0], 256), nn.ReLU()]
        fc = [nn.Linear(size[0], size[1]), nn.ReLU()]
        # wsi_size
        fc.append(nn.Dropout(0.25))
        self.wsi_net = nn.Sequential(*fc)

        ### Constructing Genomic SNN
        hidden = self.size_dict_omic[model_size_omic]
        sig_networks = []
        for input_dim in self.omic_shape:
            fc_omic = [SNN_Block(dim1=input_dim, dim2=hidden[0])]
            for i, _ in enumerate(hidden[1:]):
                fc_omic.append(SNN_Block(dim1=hidden[i], dim2=hidden[i+1], dropout=0.25))
            sig_networks.append(nn.Sequential(*fc_omic))
        self.sig_networks = nn.ModuleList(sig_networks)

        ### Multihead Attention
        self.coattn = MultiheadAttention(embed_dim=256, num_heads=1)

        ### Path Transformer + Attention Head
        path_encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, dim_feedforward=512, dropout=dropout, activation='relu')
        self.path_transformer = nn.TransformerEncoder(path_encoder_layer, num_layers=2)
        self.path_attention_head = Attn_Net_Gated(L=size[2], D=size[2], dropout=dropout, n_classes=1)

        self.path_rho = nn.Sequential(*[nn.Linear(size[2], size[2]), nn.ReLU(), nn.Dropout(dropout)])

        ### Omic Transformer + Attention Head
        omic_encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, dim_feedforward=512, dropout=dropout, activation='relu')
        self.omic_transformer = nn.TransformerEncoder(omic_encoder_layer, num_layers=2)
        self.omic_attention_head = Attn_Net_Gated(L=size[2], D=size[2], dropout=dropout, n_classes=1)
        self.omic_rho = nn.Sequential(*[nn.Linear(size[2], size[2]), nn.ReLU(), nn.Dropout(dropout)])

        ### Fusion Layer
        if self.fusion == 'concat':
            self.mm = nn.Sequential(*[nn.Linear(256*2, size[2]), nn.ReLU(), nn.Linear(size[2], size[2]), nn.ReLU()])
        elif self.fusion == 'bilinear':
            self.mm = BilinearFusion(dim1=256, dim2=256, scale_dim1=8, scale_dim2=8, mmhid=256)
        else:
            self.mm = None

        ### Classifier
        self.classifier = nn.Linear(size[2], n_classes)

    def forward(self, data, **kwargs):
        # x_path = kwargs['x_path']
        # x_omic = [kwargs['x_omic%d' % i] for i in range(1,7)]
        x_omic = [data[0]] # needs to be wrapped in list (used to expect multiple omic sources)
        h_omic = [self.sig_networks[idx].forward(sig_feat) for idx, sig_feat in enumerate(x_omic)] ### each omic signature goes through it's own FC layer

        # h_omic_bag = torch.stack(h_omic).unsqueeze(1)
        h_omic_bag = torch.stack(h_omic)  ### omic embeddings are stacked (to be used in co-attention)

        x_path = data[1]
        x_path = einops.rearrange(x_path, "b dim patches -> b patches dim")
        h_path_bag = self.wsi_net(x_path).unsqueeze(1) ### path embeddings are fed through a FC layer

        # Coattn
        h_path_coattn, A_coattn = self.coattn(h_omic_bag, h_path_bag, h_path_bag)

        ### Path
        h_path_trans = self.path_transformer(h_path_coattn)
        A_path, h_path = self.path_attention_head(h_path_trans.squeeze(1))
        A_path = torch.transpose(A_path, 1, 0)
        A_path = A_path.squeeze(-1)
        h_path = h_path.squeeze(0)
        # h_path = torch.mm(F.softmax(A_path.t(), dim=1) , h_path) # keep batch dimension
        h_path = self.path_rho(h_path).squeeze()

        ### Omic
        h_omic_trans = self.omic_transformer(h_omic_bag)
        A_omic, h_omic = self.omic_attention_head(h_omic_trans.squeeze(1))
        A_omic = torch.transpose(A_omic, 1, 0)
        A_omic = A_omic.squeeze(-1)
        h_omic = h_omic.squeeze(0)
        # h_omic = torch.mm(F.softmax(A_omic.t(), dim=1), h_omic)
        h_omic = self.omic_rho(h_omic).squeeze()

        if self.fusion == 'bilinear':
            h = self.mm(h_path.unsqueeze(dim=0), h_omic.unsqueeze(dim=0)).squeeze()
        elif self.fusion == 'concat':
            if len(h_omic.shape) == 1: # if batch size is 1
                h_omic = h_omic.unsqueeze(dim=0)
                h_path = h_path.unsqueeze(dim=0)
            h = self.mm(torch.cat([h_path, h_omic], axis=1))

        ### Survival Layer
        logits = self.classifier(h)

        # happens in train loop
        # Y_hat = torch.topk(logits, 1, dim = 1)[1]
        # hazards = torch.sigmoid(logits)
        # S = torch.cumprod(1 - hazards, dim=1)
        #
        # attention_scores = {'coattn': A_coattn, 'path': A_path, 'omic': A_omic}

        return logits


    def captum(self, x_path, x_omic1, x_omic2, x_omic3, x_omic4, x_omic5, x_omic6):
        #x_path = torch.randn((10, 500, 1024))
        #x_omic1, x_omic2, x_omic3, x_omic4, x_omic5, x_omic6 = [torch.randn(10, size) for size in omic_sizes]
        x_omic = [x_omic1, x_omic2, x_omic3, x_omic4, x_omic5, x_omic6]
        h_path_bag = self.wsi_net(x_path)#.unsqueeze(1) ### path embeddings are fed through a FC layer
        h_path_bag = torch.reshape(h_path_bag, (500, 10, 256))
        h_omic = [self.sig_networks[idx].forward(sig_feat) for idx, sig_feat in enumerate(x_omic)] ### each omic signature goes through it's own FC layer
        h_omic_bag = torch.stack(h_omic) ### omic embeddings are stacked (to be used in co-attention)

        # Coattn
        h_path_coattn, A_coattn = self.coattn(h_omic_bag, h_path_bag, h_path_bag)

        ### Path
        h_path_trans = self.path_transformer(h_path_coattn)
        h_path_trans = torch.reshape(h_path_trans, (10, 6, 256))
        A_path, h_path = self.path_attention_head(h_path_trans)
        A_path = F.softmax(A_path.squeeze(dim=2), dim=1).unsqueeze(dim=1)
        h_path = torch.bmm(A_path, h_path).squeeze(dim=1)

        ### Omic
        h_omic_trans = self.omic_transformer(h_omic_bag)
        h_omic_trans = torch.reshape(h_omic_trans, (10, 6, 256))
        A_omic, h_omic = self.omic_attention_head(h_omic_trans)
        A_omic = F.softmax(A_omic.squeeze(dim=2), dim=1).unsqueeze(dim=1)
        h_omic = torch.bmm(A_omic, h_omic).squeeze(dim=1)

        if self.fusion == 'bilinear':
            h = self.mm(h_path.unsqueeze(dim=0), h_omic.unsqueeze(dim=0)).squeeze()
        elif self.fusion == 'concat':
            h = self.mm(torch.cat([h_path, h_omic], axis=1))

        logits  = self.classifier(h)
        hazards = torch.sigmoid(logits)
        S = torch.cumprod(1 - hazards, dim=1)

        risk = -torch.sum(S, dim=1)
        return risk



################################
# Attention MIL Implementation #
################################
class MILAttentionNet(nn.Module):
    def __init__(self, input_dim: Tuple,
                 omic_input_dim=None, fusion=None, size_arg = "small", dropout=0.25, n_classes=4):
        r"""
        Attention MIL Implementation

        Args:
            omic_input_dim (int): Dimension size of genomic features.
            fusion (str): Fusion method (Choices: concat, bilinear, or None)
            size_arg (str): Size of NN architecture (Choices: small or large)
            dropout (float): Dropout rate
            n_classes (int): Output shape of NN
        """
        super(MILAttentionNet, self).__init__()
        self.fusion = fusion
        self.size_dict_path = {"small": [1024, 512, 256], "big": [1024, 512, 384]}
        self.size_dict_omic = {'small': [256, 256]}

        ### Deep Sets Architecture Construction
        size = self.size_dict_path[size_arg]
        size[0] = input_dim[0]
        fc = [nn.Linear(size[0], size[1]), nn.ReLU(), nn.Dropout(dropout)]
        attention_net = Attn_Net_Gated(L=size[1], D=size[2], dropout=dropout, n_classes=1)
        fc.append(attention_net)
        self.attention_net = nn.Sequential(*fc)
        self.rho = nn.Sequential(*[nn.Linear(size[1], size[2]), nn.ReLU(), nn.Dropout(dropout)])

        ### Constructing Genomic SNN
        if self.fusion is not None:
            hidden = [256, 256]
            fc_omic = [SNN_Block(dim1=omic_input_dim, dim2=hidden[0])]
            for i, _ in enumerate(hidden[1:]):
                fc_omic.append(SNN_Block(dim1=hidden[i], dim2=hidden[i+1], dropout=0.25))
            self.fc_omic = nn.Sequential(*fc_omic)

            if self.fusion == 'concat':
                self.mm = nn.Sequential(*[nn.Linear(256*2, size[2]), nn.ReLU(), nn.Linear(size[2], size[2]), nn.ReLU()])
            elif self.fusion == 'bilinear':
                self.mm = BilinearFusion(dim1=256, dim2=256, scale_dim1=8, scale_dim2=8, mmhid=256)
            else:
                self.mm = None

        self.classifier = nn.Linear(size[2], n_classes)


    def relocate(self):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if torch.cuda.device_count() >= 1:
            device_ids = list(range(torch.cuda.device_count()))
            self.attention_net = nn.DataParallel(self.attention_net, device_ids=device_ids).to('cuda:0')

        if self.fusion is not None:
            self.fc_omic = self.fc_omic.to(device)
            self.mm = self.mm.to(device)

        self.rho = self.rho.to(device)
        self.classifier = self.classifier.to(device)


    def forward(self, data, **kwargs):
        x_path = data[0]
        x_path = einops.rearrange(x_path, "b dim p -> b p dim")

        A, h_path = self.attention_net(x_path)
        A = torch.transpose(A, 1, 0)

        A_raw = A
        A = F.softmax(A, dim=1)
        A_transposed = einops.rearrange(A, "p b n -> b p n")
        # multiply and sum across patches -> (2, dim)
        h_path = (A_transposed * h_path).sum(1)
        h_path = self.rho(h_path).squeeze()

        logits  = self.classifier(h_path)

        if len(logits.shape) == 1: # if batch size is 1
            logits = logits.unsqueeze(0)

        return logits




##########################
#### Genomic FC Model ####
##########################
class SNN(nn.Module):
    def __init__(self, input_dim: int, model_size_omic: str='small', n_classes: int=4):
        super(SNN, self).__init__()
        self.n_classes = n_classes
        self.size_dict_omic = {'small': [256, 256, 256, 256], 'big': [1024, 1024, 1024, 256]}

        ### Constructing Genomic SNN
        hidden = self.size_dict_omic[model_size_omic]
        fc_omic = [SNN_Block(dim1=input_dim, dim2=hidden[0])]
        for i, _ in enumerate(hidden[1:]):
            fc_omic.append(SNN_Block(dim1=hidden[i], dim2=hidden[i+1], dropout=0.25))
        self.fc_omic = nn.Sequential(*fc_omic)
        self.classifier = nn.Linear(hidden[-1], n_classes)
        init_max_weights(self)


    def forward(self, data, **kwargs):
        x = data[0]
        # x = kwargs['x_omic']
        features = self.fc_omic(x)

        logits = self.classifier(features)
        return logits

    def relocate(self):
            device=torch.device("cuda" if torch.cuda.is_available() else "cpu")

            if torch.cuda.device_count() > 1:
                device_ids = list(range(torch.cuda.device_count()))
                self.fc_omic = nn.DataParallel(self.fc_omic, device_ids=device_ids).to('cuda:0')
            else:
                self.fc_omic = self.fc_omic.to(device)


            self.classifier = self.classifier.to(device)


class BilinearFusion(nn.Module):
    r"""
    Late Fusion Block using Bilinear Pooling

    args:
        skip (int): Whether to input features at the end of the layer
        use_bilinear (bool): Whether to use bilinear pooling during information gating
        gate1 (bool): Whether to apply gating to modality 1
        gate2 (bool): Whether to apply gating to modality 2
        dim1 (int): Feature mapping dimension for modality 1
        dim2 (int): Feature mapping dimension for modality 2
        scale_dim1 (int): Scalar value to reduce modality 1 before the linear layer
        scale_dim2 (int): Scalar value to reduce modality 2 before the linear layer
        mmhid (int): Feature mapping dimension after multimodal fusion
        dropout_rate (float): Dropout rate
    """
    def __init__(self, skip=0, use_bilinear=0, gate1=1, gate2=1, dim1=128, dim2=128, scale_dim1=1, scale_dim2=1, mmhid=256, dropout_rate=0.25):
        super(BilinearFusion, self).__init__()
        self.skip = skip
        self.use_bilinear = use_bilinear
        self.gate1 = gate1
        self.gate2 = gate2

        dim1_og, dim2_og, dim1, dim2 = dim1, dim2, dim1//scale_dim1, dim2//scale_dim2
        skip_dim = dim1_og+dim2_og if skip else 0

        self.linear_h1 = nn.Sequential(nn.Linear(dim1_og, dim1), nn.ReLU())
        self.linear_z1 = nn.Bilinear(dim1_og, dim2_og, dim1) if use_bilinear else nn.Sequential(nn.Linear(dim1_og+dim2_og, dim1))
        self.linear_o1 = nn.Sequential(nn.Linear(dim1, dim1), nn.ReLU(), nn.Dropout(p=dropout_rate))

        self.linear_h2 = nn.Sequential(nn.Linear(dim2_og, dim2), nn.ReLU())
        self.linear_z2 = nn.Bilinear(dim1_og, dim2_og, dim2) if use_bilinear else nn.Sequential(nn.Linear(dim1_og+dim2_og, dim2))
        self.linear_o2 = nn.Sequential(nn.Linear(dim2, dim2), nn.ReLU(), nn.Dropout(p=dropout_rate))

        self.post_fusion_dropout = nn.Dropout(p=dropout_rate)
        self.encoder1 = nn.Sequential(nn.Linear((dim1+1)*(dim2+1), 256), nn.ReLU(), nn.Dropout(p=dropout_rate))
        self.encoder2 = nn.Sequential(nn.Linear(256+skip_dim, mmhid), nn.ReLU(), nn.Dropout(p=dropout_rate))

    def forward(self, vec1, vec2):
        ### Gated Multimodal Units
        if self.gate1:
            h1 = self.linear_h1(vec1)
            z1 = self.linear_z1(vec1, vec2) if self.use_bilinear else self.linear_z1(torch.cat((vec1, vec2), dim=1))
            o1 = self.linear_o1(nn.Sigmoid()(z1)*h1)
        else:
            h1 = self.linear_h1(vec1)
            o1 = self.linear_o1(h1)

        if self.gate2:
            h2 = self.linear_h2(vec2)
            z2 = self.linear_z2(vec1, vec2) if self.use_bilinear else self.linear_z2(torch.cat((vec1, vec2), dim=1))
            o2 = self.linear_o2(nn.Sigmoid()(z2)*h2)
        else:
            h2 = self.linear_h2(vec2)
            o2 = self.linear_o2(h2)

        ### Fusion
        o1 = torch.cat((o1, torch.cuda.FloatTensor(o1.shape[0], 1).fill_(1)), 1)
        o2 = torch.cat((o2, torch.cuda.FloatTensor(o2.shape[0], 1).fill_(1)), 1)
        o12 = torch.bmm(o1.unsqueeze(2), o2.unsqueeze(1)).flatten(start_dim=1) # BATCH_SIZE X 1024
        out = self.post_fusion_dropout(o12)
        out = self.encoder1(out)
        if self.skip: out = torch.cat((out, vec1, vec2), 1)
        out = self.encoder2(out)
        return out


def SNN_Block(dim1, dim2, dropout=0.25):
    r"""
    Multilayer Reception Block w/ Self-Normalization (Linear + ELU + Alpha Dropout)

    args:
        dim1 (int): Dimension of input features
        dim2 (int): Dimension of output features
        dropout (float): Dropout rate
    """
    import torch.nn as nn

    return nn.Sequential(
            nn.Linear(dim1, dim2),
            nn.ELU(),
            nn.AlphaDropout(p=dropout, inplace=False))


def Reg_Block(dim1, dim2, dropout=0.25):
    r"""
    Multilayer Reception Block (Linear + ReLU + Dropout)

    args:
        dim1 (int): Dimension of input features
        dim2 (int): Dimension of output features
        dropout (float): Dropout rate
    """
    import torch.nn as nn

    return nn.Sequential(
            nn.Linear(dim1, dim2),
            nn.ReLU(),
            nn.Dropout(p=dropout, inplace=False))


class Attn_Net_Gated(nn.Module):
    def __init__(self, L = 1024, D = 256, dropout = False, n_classes = 1):
        r"""
        Attention Network with Sigmoid Gating (3 fc layers)

        args:
            L (int): input feature dimension
            D (int): hidden layer dimension
            dropout (bool): whether to apply dropout (p = 0.25)
            n_classes (int): number of classes
        """
        super(Attn_Net_Gated, self).__init__()
        self.attention_a = [
            nn.Linear(L, D),
            nn.Tanh()]

        self.attention_b = [nn.Linear(L, D), nn.Sigmoid()]
        if dropout:
            self.attention_a.append(nn.Dropout(0.25))
            self.attention_b.append(nn.Dropout(0.25))

        self.attention_a = nn.Sequential(*self.attention_a)
        self.attention_b = nn.Sequential(*self.attention_b)
        self.attention_c = nn.Linear(D, n_classes)

    def forward(self, x):
        a = self.attention_a(x)
        b = self.attention_b(x)
        A = a.mul(b)
        A = self.attention_c(A)  # N x n_classes
        return A, x


def init_max_weights(module):
    r"""
    Initialize Weights function.

    args:
        modules (torch.nn.Module): Initalize weight using normal distribution
    """
    import math
    import torch.nn as nn

    for m in module.modules():
        if type(m) == nn.Linear:
            stdv = 1. / math.sqrt(m.weight.size(1))
            m.weight.data.normal_(0, stdv)
            m.bias.data.zero_()

class MultiheadAttention(Module):
    r"""Allows the model to jointly attend to information
    from different representation subspaces.
    See reference: Attention Is All You Need

    .. math::
        \text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O
        \text{where} head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)

    Args:
        embed_dim: total dimension of the model.
        num_heads: parallel attention heads.
        dropout: a Dropout layer on attn_output_weights. Default: 0.0.
        bias: add bias as module parameter. Default: True.
        add_bias_kv: add bias to the key and value sequences at dim=0.
        add_zero_attn: add a new batch of zeros to the key and
                       value sequences at dim=1.
        kdim: total number of features in key. Default: None.
        vdim: total number of features in value. Default: None.

        Note: if kdim and vdim are None, they will be set to embed_dim such that
        query, key, and value have the same number of features.

    Examples::

        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)
        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)
    """
    bias_k: Optional[torch.Tensor]
    bias_v: Optional[torch.Tensor]

    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None):
        super(MultiheadAttention, self).__init__()
        self.embed_dim = embed_dim
        self.kdim = kdim if kdim is not None else embed_dim
        self.vdim = vdim if vdim is not None else embed_dim
        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim

        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == self.embed_dim, "embed_dim must be divisible by num_heads"

        if self._qkv_same_embed_dim is False:
            self.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))
            self.k_proj_weight = Parameter(torch.Tensor(embed_dim, self.kdim))
            self.v_proj_weight = Parameter(torch.Tensor(embed_dim, self.vdim))
            self.register_parameter('in_proj_weight', None)
        else:
            self.in_proj_weight = Parameter(torch.empty(3 * embed_dim, embed_dim))
            self.register_parameter('q_proj_weight', None)
            self.register_parameter('k_proj_weight', None)
            self.register_parameter('v_proj_weight', None)

        if bias:
            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim))
        else:
            self.register_parameter('in_proj_bias', None)
        # self.out_proj = _LinearWithBias(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=True)

        if add_bias_kv:
            self.bias_k = Parameter(torch.empty(1, 1, embed_dim))
            self.bias_v = Parameter(torch.empty(1, 1, embed_dim))
        else:
            self.bias_k = self.bias_v = None

        self.add_zero_attn = add_zero_attn

        self._reset_parameters()

    def _reset_parameters(self):
        if self._qkv_same_embed_dim:
            xavier_uniform_(self.in_proj_weight)
        else:
            xavier_uniform_(self.q_proj_weight)
            xavier_uniform_(self.k_proj_weight)
            xavier_uniform_(self.v_proj_weight)

        if self.in_proj_bias is not None:
            constant_(self.in_proj_bias, 0.)
            constant_(self.out_proj.bias, 0.)
        if self.bias_k is not None:
            xavier_normal_(self.bias_k)
        if self.bias_v is not None:
            xavier_normal_(self.bias_v)

    def __setstate__(self, state):
        # Support loading old MultiheadAttention checkpoints generated by v1.1.0
        if '_qkv_same_embed_dim' not in state:
            state['_qkv_same_embed_dim'] = True

        super(MultiheadAttention, self).__setstate__(state)

    def forward(self, query, key, value, key_padding_mask=None,
                need_weights=True, need_raw=True, attn_mask=None):
        # type: (Tensor, Tensor, Tensor, Optional[Tensor], bool, Optional[Tensor]) -> Tuple[Tensor, Optional[Tensor]]
        r"""
    Args:
        query, key, value: map a query and a set of key-value pairs to an output.
            See "Attention Is All You Need" for more details.
        key_padding_mask: if provided, specified padding elements in the key will
            be ignored by the attention. When given a binary mask and a value is True,
            the corresponding value on the attention layer will be ignored. When given
            a byte mask and a value is non-zero, the corresponding value on the attention
            layer will be ignored
        need_weights: output attn_output_weights.
        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
            the batches while a 3D mask allows to specify a different mask for the entries of each batch.

    Shape:
        - Inputs:
        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
          the embedding dimension.
        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
          the embedding dimension.
        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
          the embedding dimension.
        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.
          If a ByteTensor is provided, the non-zero positions will be ignored while the position
          with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the
          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked
          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend
          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``
          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
          is provided, it will be added to the attention weight.

        - Outputs:
        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
          E is the embedding dimension.
        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,
          L is the target sequence length, S is the source sequence length.
        """
        if not self._qkv_same_embed_dim:
            return multi_head_attention_forward(
                query, key, value, self.embed_dim, self.num_heads,
                self.in_proj_weight, self.in_proj_bias,
                self.bias_k, self.bias_v, self.add_zero_attn,
                self.dropout, self.out_proj.weight, self.out_proj.bias,
                training=self.training,
                key_padding_mask=key_padding_mask, need_weights=need_weights, need_raw=need_raw,
                attn_mask=attn_mask, use_separate_proj_weight=True,
                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,
                v_proj_weight=self.v_proj_weight)
        else:
            return multi_head_attention_forward(
                query, key, value, self.embed_dim, self.num_heads,
                self.in_proj_weight, self.in_proj_bias,
                self.bias_k, self.bias_v, self.add_zero_attn,
                self.dropout, self.out_proj.weight, self.out_proj.bias,
                training=self.training,
                key_padding_mask=key_padding_mask, need_weights=need_weights, need_raw=need_raw,
                attn_mask=attn_mask)

def multi_head_attention_forward(
    query: Tensor,
    key: Tensor,
    value: Tensor,
    embed_dim_to_check: int,
    num_heads: int,
    in_proj_weight: Tensor,
    in_proj_bias: Tensor,
    bias_k: Optional[Tensor],
    bias_v: Optional[Tensor],
    add_zero_attn: bool,
    dropout_p: float,
    out_proj_weight: Tensor,
    out_proj_bias: Tensor,
    training: bool = True,
    key_padding_mask: Optional[Tensor] = None,
    need_weights: bool = True,
    need_raw: bool = True,
    attn_mask: Optional[Tensor] = None,
    use_separate_proj_weight: bool = False,
    q_proj_weight: Optional[Tensor] = None,
    k_proj_weight: Optional[Tensor] = None,
    v_proj_weight: Optional[Tensor] = None,
    static_k: Optional[Tensor] = None,
    static_v: Optional[Tensor] = None,
):
    r"""
    Args:
        query, key, value: map a query and a set of key-value pairs to an output.
            See "Attention Is All You Need" for more details.
        embed_dim_to_check: total dimension of the model.
        num_heads: parallel attention heads.
        in_proj_weight, in_proj_bias: input projection weight and bias.
        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
        add_zero_attn: add a new batch of zeros to the key and
                       value sequences at dim=1.
        dropout_p: probability of an element to be zeroed.
        out_proj_weight, out_proj_bias: the output projection weight and bias.
        training: apply dropout if is ``True``.
        key_padding_mask: if provided, specified padding elements in the key will
            be ignored by the attention. This is an binary mask. When the value is True,
            the corresponding value on the attention layer will be filled with -inf.
        need_weights: output attn_output_weights.
        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
            the batches while a 3D mask allows to specify a different mask for the entries of each batch.
        use_separate_proj_weight: the function accept the proj. weights for query, key,
            and value in different forms. If false, in_proj_weight will be used, which is
            a combination of q_proj_weight, k_proj_weight, v_proj_weight.
        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
        static_k, static_v: static key and value used for attention operators.
    Shape:
        Inputs:
        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
          the embedding dimension.
        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
          the embedding dimension.
        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
          the embedding dimension.
        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.
          If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions
          will be unchanged. If a BoolTensor is provided, the positions with the
          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend
          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``
          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
          is provided, it will be added to the attention weight.
        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
        Outputs:
        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
          E is the embedding dimension.
        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,
          L is the target sequence length, S is the source sequence length.
    """
    tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v, out_proj_weight, out_proj_bias)
    if has_torch_function(tens_ops):
        return handle_torch_function(
            multi_head_attention_forward,
            tens_ops,
            query,
            key,
            value,
            embed_dim_to_check,
            num_heads,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            add_zero_attn,
            dropout_p,
            out_proj_weight,
            out_proj_bias,
            training=training,
            key_padding_mask=key_padding_mask,
            need_weights=need_weights,
            need_raw=need_raw,
            attn_mask=attn_mask,
            use_separate_proj_weight=use_separate_proj_weight,
            q_proj_weight=q_proj_weight,
            k_proj_weight=k_proj_weight,
            v_proj_weight=v_proj_weight,
            static_k=static_k,
            static_v=static_v,
        )
    tgt_len, bsz, embed_dim = query.size()
    assert embed_dim == embed_dim_to_check
    # allow MHA to have different sizes for the feature dimension
    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)

    head_dim = embed_dim // num_heads
    assert head_dim * num_heads == embed_dim, "embed_dim must be divisible by num_heads"
    scaling = float(head_dim) ** -0.5

    if not use_separate_proj_weight:
        if (query is key or torch.equal(query, key)) and (key is value or torch.equal(key, value)):
            # self-attention
            q, k, v = linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)

        elif key is value or torch.equal(key, value):
            # encoder-decoder attention
            # This is inline in_proj function with in_proj_weight and in_proj_bias
            _b = in_proj_bias
            _start = 0
            _end = embed_dim
            _w = in_proj_weight[_start:_end, :]
            if _b is not None:
                _b = _b[_start:_end]
            q = linear(query, _w, _b)

            if key is None:
                assert value is None
                k = None
                v = None
            else:

                # This is inline in_proj function with in_proj_weight and in_proj_bias
                _b = in_proj_bias
                _start = embed_dim
                _end = None
                _w = in_proj_weight[_start:, :]
                if _b is not None:
                    _b = _b[_start:]
                k, v = linear(key, _w, _b).chunk(2, dim=-1)

        else:
            # This is inline in_proj function with in_proj_weight and in_proj_bias
            _b = in_proj_bias
            _start = 0
            _end = embed_dim
            _w = in_proj_weight[_start:_end, :]
            if _b is not None:
                _b = _b[_start:_end]
            q = linear(query, _w, _b)

            # This is inline in_proj function with in_proj_weight and in_proj_bias
            _b = in_proj_bias
            _start = embed_dim
            _end = embed_dim * 2
            _w = in_proj_weight[_start:_end, :]
            if _b is not None:
                _b = _b[_start:_end]
            k = linear(key, _w, _b)

            # This is inline in_proj function with in_proj_weight and in_proj_bias
            _b = in_proj_bias
            _start = embed_dim * 2
            _end = None
            _w = in_proj_weight[_start:, :]
            if _b is not None:
                _b = _b[_start:]
            v = linear(value, _w, _b)
    else:
        q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)
        len1, len2 = q_proj_weight_non_opt.size()
        assert len1 == embed_dim and len2 == query.size(-1)

        k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)
        len1, len2 = k_proj_weight_non_opt.size()
        assert len1 == embed_dim and len2 == key.size(-1)

        v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)
        len1, len2 = v_proj_weight_non_opt.size()
        assert len1 == embed_dim and len2 == value.size(-1)

        if in_proj_bias is not None:
            q = linear(query, q_proj_weight_non_opt, in_proj_bias[0:embed_dim])
            k = linear(key, k_proj_weight_non_opt, in_proj_bias[embed_dim : (embed_dim * 2)])
            v = linear(value, v_proj_weight_non_opt, in_proj_bias[(embed_dim * 2) :])
        else:
            q = linear(query, q_proj_weight_non_opt, in_proj_bias)
            k = linear(key, k_proj_weight_non_opt, in_proj_bias)
            v = linear(value, v_proj_weight_non_opt, in_proj_bias)
    q = q * scaling

    if attn_mask is not None:
        assert (
            attn_mask.dtype == torch.float32
            or attn_mask.dtype == torch.float64
            or attn_mask.dtype == torch.float16
            or attn_mask.dtype == torch.uint8
            or attn_mask.dtype == torch.bool
        ), "Only float, byte, and bool types are supported for attn_mask, not {}".format(attn_mask.dtype)
        if attn_mask.dtype == torch.uint8:
            warnings.warn("Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
            attn_mask = attn_mask.to(torch.bool)

        if attn_mask.dim() == 2:
            attn_mask = attn_mask.unsqueeze(0)
            if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:
                raise RuntimeError("The size of the 2D attn_mask is not correct.")
        elif attn_mask.dim() == 3:
            if list(attn_mask.size()) != [bsz * num_heads, query.size(0), key.size(0)]:
                raise RuntimeError("The size of the 3D attn_mask is not correct.")
        else:
            raise RuntimeError("attn_mask's dimension {} is not supported".format(attn_mask.dim()))
        # attn_mask's dim is 3 now.

    # convert ByteTensor key_padding_mask to bool
    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:
        warnings.warn(
            "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
        )
        key_padding_mask = key_padding_mask.to(torch.bool)

    if bias_k is not None and bias_v is not None:
        if static_k is None and static_v is None:
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
    else:
        assert bias_k is None
        assert bias_v is None

    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
    if k is not None:
        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)
    if v is not None:
        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)

    if static_k is not None:
        assert static_k.size(0) == bsz * num_heads
        assert static_k.size(2) == head_dim
        k = static_k

    if static_v is not None:
        assert static_v.size(0) == bsz * num_heads
        assert static_v.size(2) == head_dim
        v = static_v

    src_len = k.size(1)

    if key_padding_mask is not None:
        assert key_padding_mask.size(0) == bsz
        assert key_padding_mask.size(1) == src_len

    if add_zero_attn:
        src_len += 1
        k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)
        v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)
        if attn_mask is not None:
            attn_mask = pad(attn_mask, (0, 1))
        if key_padding_mask is not None:
            key_padding_mask = pad(key_padding_mask, (0, 1))

    attn_output_weights = torch.bmm(q, k.transpose(1, 2))
    assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]

    if attn_mask is not None:
        if attn_mask.dtype == torch.bool:
            attn_output_weights.masked_fill_(attn_mask, float("-inf"))
        else:
            attn_output_weights += attn_mask

    if key_padding_mask is not None:
        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
        attn_output_weights = attn_output_weights.masked_fill(
            key_padding_mask.unsqueeze(1).unsqueeze(2),
            float("-inf"),
        )
        attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)

    attn_output_weights_raw = attn_output_weights
    attn_output_weights = softmax(attn_output_weights, dim=-1)
    attn_output_weights = dropout(attn_output_weights, p=dropout_p, training=training)

    attn_output = torch.bmm(attn_output_weights, v)
    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]
    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)
    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)

    if need_weights:
        if need_raw:

            attn_output_weights_raw = attn_output_weights_raw.view(bsz, num_heads, tgt_len, src_len)
            return attn_output,attn_output_weights_raw

            #attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            #return attn_output, attn_output_weights.sum(dim=1) / num_heads, attn_output_weights_raw, attn_output_weights_raw.sum(dim=1) / num_heads
        else:
            # average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            return attn_output, attn_output_weights.sum(dim=1) / num_heads
    else:
        return attn_output, None
</file>

<file path="healnet/baselines/mm_prognosis.py">
"""
Baseline model and required helper and base classes from the MM prognosis
repository: https://github.com/gevaertlab/MultimodalPrognosis/tree/master
We use this as the state-of-the-art baseline for intermediate fusion models, but
slightly modify the class to run within our main pipeline. Note that no hyperparameters
were changed from the original paper.
Used in paper Deep Learning with Multimodal Representation for Pancancer Prognosis Prediction
https://www.biorxiv.org/content/10.1101/577197v1
"""
import os, sys, random, yaml
from itertools import product
from tqdm import tqdm
from typing import *
import itertools
from box import Box
import numpy as np
import matplotlib as mpl
#mpl.use('Agg')
from sklearn.utils import shuffle
from torchvision import models

import torch
import torch.nn as nn
import torch.nn.functional as F

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class AbstractModel(nn.Module):

    def __init__(self):
        super(AbstractModel, self).__init__()
        self.compiled = False

    # Compile module and assign optimizer + params
    def compile(self, optimizer=None, **kwargs):

        if optimizer is not None:
            self.optimizer_class = optimizer
            self.optimizer_kwargs = kwargs

        self.optimizer = self.optimizer_class(self.parameters(), **self.optimizer_kwargs)
        self.compiled = True
        self.to(DEVICE)

    # Process a batch of data from a generator to get tensors + masks
    def __process_batch(self, data):

        data, mask = stack(data), {}
        for key in data:
            mask[key] = [int(x is not None) for x in data[key]]
            template = next((x for x in data[key] if x is not None))
            data[key] = [x if x is not None else torch.zeros_like(template) \
                            for x in data[key]]
            data[key] = torch.stack(data[key]).to(DEVICE)
            mask[key] = torch.tensor(mask[key]).to(DEVICE)

        return data, mask

    # Predict scores from a batch of data
    def predict_on_batch(self, data):

        self.eval()
        with torch.no_grad():
            data, mask = self.__process_batch(data)
            pred = self.forward(data, mask)
            pred = {key: pred[key].cpu().data.numpy() for key in pred}
            return pred

    # Fit (make one optimizer step) on a batch of data
    def fit_on_batch(self, data, target):

        self.train()
        self.zero_grad()
        self.optimizer.zero_grad()

        data, mask = self.__process_batch(data)
        pred = self.forward(data, mask)
        target = stack(target)
        target = {key: torch.stack(target[key]).to(DEVICE) for key in target}

        loss = self.loss(pred, target)
        loss.backward()
        self.optimizer.step()

        pred = {key: pred[key].cpu().data.numpy() for key in pred}
        return pred, float(loss)

    # Subclasses: please override for custom loss + forward functions
    def loss(self, pred, target):
        raise NotImplementedError()

    def forward(self, data, mask):
        raise NotImplementedError()

class TrainableModel(AbstractModel):

    def __init__(self):
        super(AbstractModel, self).__init__()
        self.compiled = False
        self.losses = []

    # Predict on generator for one epoch
    def predict(self, data, verbose=False):

        self.eval()
        with torch.no_grad():
            iterator = tqdm(data) if verbose else data
            pred = [self.predict_on_batch(batch) for batch in iterator]
            pred = np.hstack(pred)
        return pred

    # Fit on generator for one epoch
    def fit(self, datagen, validation=None, verbose=True):

        self.train()

        target = []
        iterator = tqdm(datagen) if verbose else datagen
        pred = []

        for batch, y in iterator:

            y_pred, loss = self.fit_on_batch(batch, y)
            self.losses.append(loss)

            target.append(stack(y))
            pred.append(y_pred)

            if verbose and len(self.losses) % 16 == 0:
                iterator.set_description(f"Loss: {np.mean(self.losses[-32:]):0.3f}")
                #plt.plot(self.losses)
                #plt.savefig(f"{OUTPUT_DIR}/loss.jpg");

        if verbose:
            pred, target = stack(pred), stack(target)
            pred = {key: np.concatenate(pred[key], axis=0) for key in pred}
            target = {key: np.concatenate(target[key], axis=0) for key in target}

            print (f"(training) {self.evaluate(pred, target)}")

            if validation != None:
                val_data, val_target = zip(*list(validation))
                val_pred = self.predict(val_data)

                val_target = [stack(x) for x in val_target]
                val_pred, val_target = stack(val_pred), stack(val_target)
                val_pred = {key: np.concatenate(val_pred[key], axis=0) for key in val_pred}
                val_target = {key: np.concatenate(val_target[key], axis=0) for key in val_target}

                print (f"(validation) {self.evaluate(val_pred, val_target)}")
            return self.score(val_pred, val_target)

    # Evaluate predictions and targets
    def evaluate(self, pred, target):

        scores = self.score(pred, target)
        base_scores = self.score(pred, {key: shuffle(target[key]) for key in target})

        display = []
        for key in scores:
            display.append(f"{key}={scores[key]:.4f}/{base_scores[key]:.4f}")
        return ", ".join(display)

    def eval_data(self, datagen):

        val_data, val_target = zip(*list(datagen))
        val_pred = self.predict(val_data)

        val_target = [stack(x) for x in val_target]
        val_pred, val_target = stack(val_pred), stack(val_target)
        val_pred = {key: np.concatenate(val_pred[key], axis=0) for key in val_pred}
        val_target = {key: np.concatenate(val_target[key], axis=0) for key in val_target}

        return self.score(val_pred, val_target)["C-index"]




    # Score generator based on predictions and targets
    def score(self, pred, targets):
        return NotImplementedError()

    def load(self, file_path):
        self.load_state_dict(torch.load(file_path))

    def save(self, file_path):
        torch.save(self.state_dict(), file_path)

def stack(batch, targets=None):
    """
    Turns an array (batch) of dictionaries into a dictionary of arrays
    """
    keys = batch[0].keys()
    data = {key: [] for key in keys}

    for item, key in product(batch, keys):
        data[key].append(item.get(key, None))
    return data

def masked_mean(data, masks):

    num = sum((X*mask[:, None].float() for X, mask in zip(data, masks)))
    denom = sum((mask for mask in masks))[:, None].float()
    return num/denom

def unmasked_mean(data):
    stacked_data = torch.stack(data, dim=0)
    return torch.mean(stacked_data, dim=0)

def masked_variance(data, masks):
    EX2 = masked_mean(data, masks)**2
    E2X = masked_mean((x**2 for x in data), masks)
    return E2X - EX2


class MMPrognosis(TrainableModel):
    def __init__(self,
                 output_dims: int,
                 # input_dim: int,
                 sources: List[str],
                 config: Box,
                 final_classifier_head: bool = True):
        super(MMPrognosis, self).__init__()
        self.embedding_dims = 256
        self.output_dims = output_dims
        self.config = config

        self.fcm = nn.Linear(1881, self.embedding_dims)
        self.fcc = nn.Linear(7, self.embedding_dims)
        # self.fcg = nn.Linear(60483, embedding_dims)
        self.highway = Highway(256, 10, f=F.relu)
        self.fc2 = nn.Linear(self.embedding_dims, self.output_dims)
        self.fcd = nn.Linear(self.embedding_dims, self.output_dims)
        self.bn1 = nn.BatchNorm1d(self.embedding_dims)
        self.bn2 = nn.BatchNorm1d(self.embedding_dims)
        # self.bn3 = nn.BatchNorm1d(1, affine=True)
        self.modalities = len(sources)
        self.sources = sources
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.squeezenet = models.squeezenet1_0()

        self.to_logits = nn.Sequential(
            # Reduce('b n d -> b d', 'mean'),
            nn.LayerNorm(self.embedding_dims),
            nn.Linear(self.embedding_dims, self.output_dims)
        ) if final_classifier_head else nn.Identity()


    def forward(self, data: List[torch.Tensor]):

        mask = {}

        if self.sources == ["omic"]:
            z, mask = self.omic_encoder(data[0], mask)

            mean = masked_mean((z, 1), mask["omic"])
            # mean = uasked_mean(z)
            var = masked_variance((z), mask["omic"]).mean()
            var2 = masked_mean (((z - mean.mean())**2), mask["omic"])

        elif self.sources == ["slides"]:
            w, mask = self.wsi_encoder(data[0], mask)
            mean = masked_mean((w, 1), mask["slides"])
            var = masked_variance((w), mask["slides"]).mean()
            var2 = masked_mean (((w - mean.mean())**2), mask["slides"])

        elif self.sources == ["omic", "slides"]:
            z, mask = self.omic_encoder(data[0], mask)
            w, mask = self.wsi_encoder(data[1], mask)
            mean = masked_mean((z, w), (mask["omic"], mask["slides"]))

            var = masked_variance((z, w), (mask["omic"], mask["slides"])).mean()
            var2 = masked_mean (( (z - mean.mean())**2, (w - mean.mean())**2),\
                                (mask["omic"], mask["slides"]))


        ratios = var/var2.mean(dim=1)
        ratio = ratios.clamp(min=0.02, max=1.0).mean()

        x = mean

        if self.config["train_loop.batch_size"] > 1:
            x = self.bn1(x)
        x = F.dropout(x, 0.5, training=self.training)
        x = self.highway(x)
        if self.config["train_loop.batch_size"] > 1:
            x = self.bn2(x)

        # TODO: convert to logits
        logits = self.to_logits(x)

        return logits

        # score = F.log_softmax(self.fc2(x), dim=1)
        # hazard = self.fcd(x)
        #
        # return {"score": score, "hazard": hazard, "ratio": ratio.unsqueeze(0)}

    def wsi_encoder(self, data, mask):
        w = data
        input_dim = w.shape[1]
        # convolution across patches
        conv1 = nn.Conv1d(in_channels=input_dim, out_channels=512, kernel_size=5, stride=2, padding=2,
                          device=self.device)
        conv2 = nn.Conv1d(in_channels=512, out_channels=self.embedding_dims, kernel_size=5, stride=2, padding=2,
                          device=self.device)
        global_avg_pool = nn.AdaptiveAvgPool1d(1)
        # encoder instead of sqeeze net
        w = conv1(w)
        w = F.relu(w)
        w = conv2(w)
        w = F.relu(w)
        w = global_avg_pool(w)
        w = w.squeeze(-1)
        mask["slides"] = torch.ones(w.shape[1], 1).to(self.device)
        return w, mask

    def omic_encoder(self, data, mask):
        z = data
        z = z.view(z.shape[0], -1)
        self.fcg = nn.Linear(in_features=z.shape[1], out_features=self.embedding_dims, device=self.device)
        z = torch.tanh(self.fcg(z))
        mask["omic"] = torch.ones(z.shape[1], 1).to(self.device)
        return z, mask

    def loss(self, pred, target):

        vital_status = target["vital_status"]
        days_to_death = target["days_to_death"]
        hazard = pred["hazard"].squeeze()

        loss = F.nll_loss(pred["score"], vital_status)

        _, idx = torch.sort(days_to_death)
        hazard_probs = F.softmax(hazard[idx].squeeze()[1-vital_status.byte()])
        hazard_cum = torch.stack([torch.tensor(0.0)] + list(accumulate(hazard_probs)))
        N = hazard_probs.shape[0]
        weights_cum = torch.range(1, N)
        p, q = hazard_cum[1:], 1-hazard_cum[:-1]
        w1, w2 = weights_cum, N - weights_cum

        probs = torch.stack([p, q], dim=1)
        logits = torch.log(probs)
        ll1 = (F.nll_loss(logits, torch.zeros(N).long(), reduce=False) * w1)/N
        ll2 = (F.nll_loss(logits, torch.ones(N).long(), reduce=False) * w2)/N
        loss2 = torch.mean(ll1 + ll2)

        loss3 = pred["ratio"].mean()

        return loss + loss2 + loss3*0.3

    # def score(self, pred, target):
    #     vital_status = target["vital_status"]
    #     days_to_death = target["days_to_death"]
    #     score_pred = pred["score"][:, 1]
    #     hazard = pred["hazard"][:, 0]
    #
    #     auc = roc_auc_score(vital_status, score_pred)
    #     cscore = concordance_index(days_to_death, -hazard, np.logical_not(vital_status))
    #
    #     return {"AUC": auc, "C-index": cscore, "Ratio": pred["ratio"].mean()}





class Highway(nn.Module):

    def __init__(self, size, num_layers, f):

        super(Highway, self).__init__()

        self.num_layers = num_layers
        self.nonlinear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])
        self.linear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])
        self.gate = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])
        self.f = f

    def forward(self, x):

        for layer in range(self.num_layers):
            gate = torch.sigmoid(self.gate[layer](x))
            nonlinear = self.f(self.nonlinear[layer](x))
            linear = self.linear[layer](x)
            x = gate * nonlinear + (1 - gate) * linear

        return x
</file>

<file path="healnet/baselines/motcat.py">
import torch
import torch.nn.functional as F
from torch import linalg as LA
import torch.nn as nn

import ot

from models.model_utils import *


class OT_Attn_assem(nn.Module):
    def __init__(self, impl='pot-uot-l2', ot_reg=0.1, ot_tau=0.5) -> None:
        super().__init__()
        self.impl = impl
        self.ot_reg = ot_reg
        self.ot_tau = ot_tau
        print("ot impl: ", impl)

    def normalize_feature(self, x):
        x = x - x.min(-1)[0].unsqueeze(-1)
        return x

    def OT(self, weight1, weight2):
        """
        Parmas:
            weight1 : (N, D)
            weight2 : (M, D)

        Return:
            flow : (N, M)
            dist : (1, )
        """

        if self.impl == "pot-sinkhorn-l2":
            self.cost_map = torch.cdist(weight1, weight2) ** 2  # (N, M)

            src_weight = weight1.sum(dim=1) / weight1.sum()
            dst_weight = weight2.sum(dim=1) / weight2.sum()

            cost_map_detach = self.cost_map.detach()
            flow = ot.sinkhorn(a=src_weight.detach(), b=dst_weight.detach(),
                               M=cost_map_detach / cost_map_detach.max(), reg=self.ot_reg)
            dist = self.cost_map * flow
            dist = torch.sum(dist)
            return flow, dist

        elif self.impl == "pot-uot-l2":
            a, b = ot.unif(weight1.size()[0]).astype('float64'), ot.unif(weight2.size()[0]).astype('float64')
            self.cost_map = torch.cdist(weight1, weight2) ** 2  # (N, M)

            cost_map_detach = self.cost_map.detach()
            M_cost = cost_map_detach / cost_map_detach.max()

            flow = ot.unbalanced.sinkhorn_knopp_unbalanced(a=a, b=b,
                                                           M=M_cost.double().cpu().numpy(), reg=self.ot_reg,
                                                           reg_m=self.ot_tau)
            flow = torch.from_numpy(flow).type(torch.FloatTensor).cuda()

            dist = self.cost_map * flow  # (N, M)
            dist = torch.sum(dist)  # (1,) float
            return flow, dist

        else:
            raise NotImplementedError

    def forward(self, x, y):
        '''
        x: (N, 1, D)
        y: (M, 1, D)
        '''
        x = x.squeeze()
        y = y.squeeze()

        x = self.normalize_feature(x)
        y = self.normalize_feature(y)

        pi, dist = self.OT(x, y)
        return pi.T.unsqueeze(0).unsqueeze(0), dist


#############################
### MOTCAT Implementation ###
#############################
class MOTCAT_Surv(nn.Module):
    def __init__(self, fusion='concat', omic_sizes=[100, 200, 300, 400, 500, 600], n_classes=4,
                 model_size_wsi: str = 'small', model_size_omic: str = 'small', dropout=0.25, ot_reg=0.1, ot_tau=0.5,
                 ot_impl="pot-uot-l2"):
        super(MOTCAT_Surv, self).__init__()
        self.fusion = fusion
        self.omic_sizes = omic_sizes
        self.n_classes = n_classes
        self.size_dict_WSI = {"small": [1024, 256, 256], "big": [1024, 512, 384]}
        self.size_dict_omic = {'small': [256, 256], 'big': [1024, 1024, 1024, 256]}

        ### FC Layer over WSI bag
        size = self.size_dict_WSI[model_size_wsi]
        fc = [nn.Linear(size[0], size[1]), nn.ReLU()]
        fc.append(nn.Dropout(0.25))
        self.wsi_net = nn.Sequential(*fc)

        ### Constructing Genomic SNN
        hidden = self.size_dict_omic[model_size_omic]
        sig_networks = []
        for input_dim in omic_sizes:
            fc_omic = [SNN_Block(dim1=input_dim, dim2=hidden[0])]
            for i, _ in enumerate(hidden[1:]):
                fc_omic.append(SNN_Block(dim1=hidden[i], dim2=hidden[i + 1], dropout=0.25))
            sig_networks.append(nn.Sequential(*fc_omic))
        self.sig_networks = nn.ModuleList(sig_networks)

        ### OT-based Co-attention
        self.coattn = OT_Attn_assem(impl=ot_impl, ot_reg=ot_reg, ot_tau=ot_tau)

        ### Path Transformer + Attention Head
        path_encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, dim_feedforward=512, dropout=dropout,
                                                        activation='relu')
        self.path_transformer = nn.TransformerEncoder(path_encoder_layer, num_layers=2)
        self.path_attention_head = Attn_Net_Gated(L=size[2], D=size[2], dropout=dropout, n_classes=1)
        self.path_rho = nn.Sequential(*[nn.Linear(size[2], size[2]), nn.ReLU(), nn.Dropout(dropout)])

        ### Omic Transformer + Attention Head
        omic_encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, dim_feedforward=512, dropout=dropout,
                                                        activation='relu')
        self.omic_transformer = nn.TransformerEncoder(omic_encoder_layer, num_layers=2)
        self.omic_attention_head = Attn_Net_Gated(L=size[2], D=size[2], dropout=dropout, n_classes=1)
        self.omic_rho = nn.Sequential(*[nn.Linear(size[2], size[2]), nn.ReLU(), nn.Dropout(dropout)])

        ### Fusion Layer
        if self.fusion == 'concat':
            self.mm = nn.Sequential(*[nn.Linear(256 * 2, size[2]), nn.ReLU(), nn.Linear(size[2], size[2]), nn.ReLU()])
        elif self.fusion == 'bilinear':
            self.mm = BilinearFusion(dim1=256, dim2=256, scale_dim1=8, scale_dim2=8, mmhid=256)
        else:
            self.mm = None

        ### Classifier
        self.classifier = nn.Linear(size[2], n_classes)

    def forward(self, **kwargs):
        x_path = kwargs['x_path']
        x_omic = [kwargs['x_omic%d' % i] for i in range(1, 7)]

        h_path_bag = self.wsi_net(x_path).unsqueeze(1)  ### path embeddings are fed through a FC layer

        h_omic = [self.sig_networks[idx].forward(sig_feat) for idx, sig_feat in
                  enumerate(x_omic)]  ### each omic signature goes through it's own FC layer
        h_omic_bag = torch.stack(h_omic).unsqueeze(1)  ### omic embeddings are stacked (to be used in co-attention)

        ### Coattn
        A_coattn, _ = self.coattn(h_path_bag, h_omic_bag)
        h_path_coattn = torch.mm(A_coattn.squeeze(), h_path_bag.squeeze()).unsqueeze(1)

        ### Path
        h_path_trans = self.path_transformer(h_path_coattn)
        A_path, h_path = self.path_attention_head(h_path_trans.squeeze(1))
        A_path = torch.transpose(A_path, 1, 0)
        h_path = torch.mm(F.softmax(A_path, dim=1), h_path)
        h_path = self.path_rho(h_path).squeeze()

        ### Omic
        h_omic_trans = self.omic_transformer(h_omic_bag)
        A_omic, h_omic = self.omic_attention_head(h_omic_trans.squeeze(1))
        A_omic = torch.transpose(A_omic, 1, 0)
        h_omic = torch.mm(F.softmax(A_omic, dim=1), h_omic)
        h_omic = self.omic_rho(h_omic).squeeze()

        if self.fusion == 'bilinear':
            h = self.mm(h_path.unsqueeze(dim=0), h_omic.unsqueeze(dim=0)).squeeze()
        elif self.fusion == 'concat':
            h = self.mm(torch.cat([h_path, h_omic], axis=0))

        logits = self.classifier(h).unsqueeze(0)
        return logits


        # ### Survival Layer --> done in main pipeline
        # Y_hat = torch.topk(logits, 1, dim=1)[1]
        # hazards = torch.sigmoid(logits)
        # S = torch.cumprod(1 - hazards, dim=1)
        #
        # attention_scores = {'coattn': A_coattn, 'path': A_path, 'omic': A_omic}
        #
        # return hazards, S, Y_hat, attention_scores
</file>

<file path="healnet/etl/__init__.py">
from healnet.etl.loaders import TCGADataset, MMDataset

__all__ = ["TCGADataset", "MMDataset"]
</file>

<file path="healnet/etl/base.py">
"""
Base classes to load different data modalities
"""
from abc import abstractmethod


class Dataset(object):

    def __init__(self, name: str):
        self.name=name

    @abstractmethod
    def load_tabular(self):
        pass

    @abstractmethod
    def load_image(self):
        pass

    @abstractmethod
    def load_text(self):
        pass
</file>

<file path="healnet/etl/loaders.py">
import einops
from torch.utils.data import Dataset
from torchvision import transforms
from healnet.utils import Config
from openslide import OpenSlide
import os
from multiprocessing import Lock
from multiprocessing import Manager
import h5py
import torch
import pprint
from einops import rearrange, repeat
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from pathlib import Path
from typing import *
from box import Box


class MMDataset(Dataset):
    """
    Generic torch dataset object for supervised multi-modal data.
    """
    def __init__(self, tensors: List[torch.Tensor], target: Optional[torch.Tensor]=None):
        """
        Args:
            tensors(List[torch.Tensor]): modalities for each sample
            target(torch.Tensor): label for each sample
        """
        self.tensors = tensors
        self.target = target

    def __getitem__(self, idx) -> [Tuple[List[torch.Tensor], torch.Tensor], List[torch.Tensor]]:
        if self.target is None:
            return [t[idx] for t in self.tensors]
        else:
            return [t[idx] for t in self.tensors], self.target[idx]

    def __len__(self):
        return self.tensors[0].size()[0]


class TCGADataset(Dataset):
    """
    Main dataset class for TCGA data. Loads in omic data and WSI data and returns a tuple of tensors when
    __getitem__ is called along with survival information (censorship, event_time, discretised survival).
    """

    def __init__(self, dataset: str,
                 config: Box,
                 level: int=2,
                 filter_overlap: bool = True,
                 survival_analysis: bool = True,
                 num_classes: int = 2,
                 n_bins: int = 4,
                 sources: List = ["omic", "slides"],
                 log_dir = None,
                 ):
        """
        Dataset wrapper to load different TCGA data modalities (omic and WSI data).
        Args:
            dataset (str): TCGA dataset to load (e.g. "brca", "blca")
            config (Box): Config object
            filter_overlap: filter omic data and/or slides that do not have a corresponding sample in the other modality
            n_bins: number of discretised bins for survival analysis

        Examples:
            >>> from healnet.etl.loaders import TCGADataset
            >>> from healnet.utils import Config
            >>> config = Config("config/main.yml").read()
            >>> dataset = TCGADataset("blca", config)
            # get omic data
            >>> dataset.omic_df
            # get sample slide
            >>> slide, tensor = dataset.load_omic(blca.sample_slide_id, resolution="lowest")
            # get overall sample
            >>> (slide, tensor), censorship, event_time, y_disc = next(iter(dataset))
        """
        self.config = config
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.dataset = dataset
        self.log_dir = log_dir
        self.sources = sources
        self.filter_overlap = filter_overlap
        self.survival_analysis = survival_analysis
        self.sample_missing = False
        self.num_classes = num_classes
        self.n_bins = n_bins
        self.subset = self.config["survival.subset"]
        self.raw_path = Path(config.tcga_path).joinpath(f"wsi/{dataset}")
        prep_path = Path(config.tcga_path).joinpath(f"wsi/{dataset}_preprocessed_level{level}")
        self.prep_path = prep_path
        # create patch feature directory for first-time run
        os.makedirs(self.prep_path.joinpath("patch_features"), exist_ok=True)
        self.slide_ids = [slide_id.rsplit(".", 1)[0] for slide_id in os.listdir(prep_path.joinpath("patches"))]



        # for early fusion baseline, we need to concatenate omic and slide features into a single tensor
        self.concat = True if self.config.model in ["fcnn", "healnet_early"] and len(self.sources) > 1 else False

        valid_sources = ["omic", "slides"]
        assert all([source in valid_sources for source in sources]), f"Invalid source specified. Valid sources are {valid_sources}"
        self.wsi_paths: dict = self._get_slide_dict() # {slide_id: path}
        self.sample_slide_id = self.slide_ids[0] + ".svs"
        self.sample_slide = OpenSlide(self.wsi_paths[self.sample_slide_id])
        # pre-load and transform omic data
        self.omic_df = self.load_omic()
        self.features = self.omic_df.drop(["site", "oncotree_code", "case_id", "slide_id", "train", "censorship", "survival_months", "y_disc"], axis=1)
        self.omic_tensor = torch.Tensor(self.features.values)
        if self.config.model in ["healnet", "healnet_early"]:
            # Healnet expects inputs of the shape (batch_size, input_dim, channels)
            if self.config.omic_attention:
                self.omic_tensor = einops.repeat(self.omic_tensor, "n feat -> n channels feat", channels=1)
            else:
                self.omic_tensor = einops.repeat(self.omic_tensor, "n feat -> n feat channels", channels=1)


        self.level = level
        self.slide_idx: dict = self._get_slide_idx() # {idx (molecular_df): slide_id}
        self.wsi_width, self.wsi_height = self.get_resize_dims(level=self.level, override=config["data.resize"])
        self.censorship = self.omic_df["censorship"].values
        self.survival_months = self.omic_df["survival_months"].values
        self.y_disc = self.omic_df["y_disc"].values

        manager = Manager()
        self.patch_cache = manager.dict()
        # self.patch_cache = SharedLRUCache(capacity=256) # capacity should be multiple of num_workers
        print(f"Dataloader initialised for {dataset} dataset")
        self.get_info(full_detail=False)

    def __getitem__(self, index):
        y_disc = self.y_disc[index]
        censorship = self.censorship[index]
        event_time = self.survival_months[index]


        if len(self.sources) == 1 and self.sources[0] == "omic":
            omic_tensor = self.omic_tensor[index]
            return [omic_tensor], censorship, event_time, y_disc

        elif len(self.sources) == 1 and self.sources[0] == "slides":
            slide_id = self.omic_df.iloc[index]["slide_id"].rsplit(".", 1)[0]

           
            if index not in self.patch_cache:
                slide_tensor = self.load_patch_features(slide_id)
                self.patch_cache[index] = slide_tensor

            else:
                slide_tensor = self.patch_cache[index]
            if self.config.model == "fcnn": # for fcnn baseline
                slide_tensor = torch.flatten(slide_tensor)

            return [slide_tensor], censorship, event_time, y_disc

        else: # both
            omic_tensor = self.omic_tensor[index]
            slide_id = self.omic_df.iloc[index]["slide_id"].rsplit(".", 1)[0]

            if index not in self.patch_cache:
                slide_tensor = self.load_patch_features(slide_id)
                self.patch_cache[index] = slide_tensor
            else:
                slide_tensor = self.patch_cache[index]

            if self.concat: # for early fusion baseline
                slide_flat = torch.flatten(slide_tensor)
                omic_flat = torch.flatten(omic_tensor)
                concat_tensor = torch.cat([omic_flat, slide_flat], dim=0)
                if self.config.model == "healnet_early":
                    concat_tensor = concat_tensor.unsqueeze(0)
                return [concat_tensor], censorship, event_time, y_disc
            else: # keep separate for HEALNet
                return [omic_tensor, slide_tensor], censorship, event_time, y_disc

    def get_resize_dims(self, level: int, patch_height: int = 128, patch_width: int = 128, override=False):
        # TODO - use TIA to handle resizing
        if override is False:
            width = self.sample_slide.level_dimensions[level][0]
            height = self.sample_slide.level_dimensions[level][1]
            # take nearest multiple of 128 of height and width (for patches)
            width = round(width/patch_width)*patch_width
            height = round(height/patch_height)*patch_height
        else:
            width = self.config["data.resize_width"]
            height = self.config["data.resize_height"]
        return width, height

    def _get_slide_idx(self):
        # filter slide index to only include samples with WSIs availables
        filter_keys = [slide_id + ".svs" for slide_id in self.slide_ids]
        tmp_df = self.omic_df[self.omic_df.slide_id.isin(filter_keys)]
        return dict(zip(tmp_df.index, tmp_df["slide_id"]))

    def __len__(self):
        if self.sources == ["omic"]:
            # use all omic samples when running single modality
            return self.omic_df.shape[0]
        else:
            # only use overlap otherwise
            return len(self.slide_ids)
    def _get_slide_dict(self):
        """
        Given the download structure of the gdc-client, each slide is stored in a folder
        with a non-meaningful name. This function returns a dictionary of slide_id to
        the path of the slide.
        Returns:
            svs_dict (dict): Dictionary of slide_id to path of slide
        """
        slide_path = Path(self.config.tcga_path).joinpath(f"wsi/{self.dataset}")
        svs_files = list(slide_path.glob("**/*.svs"))
        svs_dict = {path.name: path for path in svs_files}
        return svs_dict

    # def _load_patch_coords(self):
    #     """
    #     Loads all patch coordinates for the dataset and level specified in the config and writes it to a dictionary
    #     with key: slide_id and value: patch coordinates (where each coordinate is a x,y tupe)
    #     """
    #     coords = {}
    #     for slide_id in self.slide_ids:
    #         patch_path = self.prep_path.joinpath(f"patches/{slide_id}.h5")
    #         h5_file = h5py.File(patch_path, "r")
    #         patch_coords = h5_file["coords"][:]
    #         coords[slide_id] = patch_coords
    #     return coords

    def get_info(self, full_detail: bool = False):
        """
        Logging util to print some basic dataset information. Normally called at the start of a pipeline run
        Args:
            full_detail (bool): Print all slide properties

        Returns:
            None
        """
        slide_path = Path(self.config.tcga_path).joinpath(f"wsi/{self.dataset}/")
        print(f"Dataset: {self.dataset.upper()}")
        print(f"Molecular data shape: {self.omic_df.shape}")
        sample_overlap = (set(self.omic_df["slide_id"]) & set(self.wsi_paths.keys()))
        print(f"Molecular/Slide match: {len(sample_overlap)}/{len(self.omic_df)}")
        # print(f"Slide dimensions: {slide.dimensions}")
        print(f"Slide level count: {self.sample_slide.level_count}")
        print(f"Slide level dimensions: {self.sample_slide.level_dimensions}")
        print(f"Slide resize dimensions: w: {self.wsi_width}, h: {self.wsi_height}")
        print(f"Sources selected: {self.sources}")
        print(f"Censored share: {np.round(len(self.omic_df[self.omic_df['censorship'] == 1])/len(self.omic_df), 3)}")
        print(f"Survival_bin_sizes: {dict(self.omic_df['y_disc'].value_counts().sort_values())}")

        if full_detail:
            pprint(dict(self.sample_slide.properties))

    def show_samples(self, n=1):
        """
        Logging util to show some detailed sample stats and render the whole slide image (e.g., in a notebook)
        Args:
            n (int): Number of samples to show

        Returns:
            None
        """
        # sample_df = self.omic_df.sample(n=n)
        sample_df = self.omic_df[self.omic_df["slide_id"].isin(self.wsi_paths.keys())].sample(n=n)
        for idx, row in sample_df.iterrows():
            print(f"Case ID: {row['case_id']}")
            print(f"Patient age: {row['age']}")
            print(f"Gender: {'female' if row['is_female'] else 'male'}")
            print(f"Survival months: {row['survival_months']}")
            print(f"Survival years:  {np.round(row['survival_months']/12, 1)}")
            print(f"Censored (survived follow-up period): {'yes' if row['censorship'] else 'no'}")
            # print(f"Risk: {'high' if row['high_risk'] else 'low'}")
            # plot wsi
            slide, slide_tensor = self.load_wsi(row["slide_id"], level=self.level)
            print(f"Shape:", slide_tensor.shape)
            plt.figure(figsize=(10, 10))
            plt.imshow(slide_tensor)
            plt.show()




    def load_omic(self,
                  eps: float = 1e-6
                  ) -> pd.DataFrame:
        """
        Loads in omic data and returns a dataframe and filters depending on which whole slide images
        are available, such that only samples with both omic and WSI data are kept.
        Also calculates the discretised survival time for each sample.
        Args:
            eps (float): Epsilon value to add to min and max survival time to ensure all samples are included

        Returns:
            pd.DataFrame: Dataframe with omic data and discretised survival time (target)
        """
        data_path = Path(self.config.tcga_path).joinpath(f"omic/tcga_{self.dataset}_all_clean.csv.zip")
        df = pd.read_csv(data_path, compression="zip", header=0, index_col=0, low_memory=False)
        valid_subsets = ["all", "uncensored", "censored"]
        assert self.subset in valid_subsets, "Invalid cut specified. Must be one of 'all', 'uncensored', 'censored'"

        # handle missing values
        num_nans = df.isna().sum().sum()
        nan_counts = df.isna().sum()[df.isna().sum() > 0]
        df = df.fillna(df.mean(numeric_only=True))
        print(f"Filled {num_nans} missing values with mean")
        print(f"Missing values per feature: \n {nan_counts}")

        # filter samples for which there are no slides available
        if self.filter_overlap:
            slides_available = self.slide_ids
            omic_available = [id[:-4] for id in df["slide_id"]]
            overlap = set(slides_available) & set(omic_available)
            print(f"Slides available: {len(slides_available)}")
            print(f"Omic available: {len(omic_available)}")
            print(f"Overlap: {len(overlap)}")
            if len(slides_available) < len(omic_available):
                print(f"Filtering out {len(omic_available) - len(slides_available)} samples for which there are no omic data available")
                overlap_filter = [id + ".svs" for id in overlap]
                df = df[df["slide_id"].isin(overlap_filter)]
            elif len(slides_available) > len(omic_available):
                print(f"Filtering out {len(slides_available) - len(omic_available)} samples for which there are no slides available")
                self.slide_ids = overlap
            else:
                print("100% modality overlap, no samples filtered out")

        # assign target column (high vs. low risk in equal parts of survival)
        label_col = "survival_months"
        if self.subset == "all":
            df["y_disc"] = pd.qcut(df[label_col], q=self.n_bins, labels=False).values
        else:
            if self.subset == "censored":
                subset_df = df[df["censorship"] == 1]
            elif self.subset == "uncensored":
                subset_df = df[df["censorship"] == 0]
            # take q_bins from uncensored patients
            disc_labels, q_bins = pd.qcut(subset_df[label_col], q=self.n_bins, retbins=True, labels=False)
            q_bins[-1] = df[label_col].max() + eps
            q_bins[0] = df[label_col].min() - eps
            # use bin cuts to discretize all patients
            df["y_disc"] = pd.cut(df[label_col], bins=q_bins, retbins=False, labels=False, right=False, include_lowest=True).values

        df["y_disc"] = df["y_disc"].astype(int)

        if self.log_dir is not None:
            df.to_csv(self.log_dir.joinpath(f"{self.dataset}_omic_overlap.csv.zip"), compression="zip")

        return df

    def load_wsi(self, slide_id: str, level: int = None) -> Tuple:
        """
        Load in single slide and get region at specified resolution level
        Args:
            slide_id:
            level:
            resolution:

        Returns:
            Tuple (openslide object, tensor of region)
        """

        slide = OpenSlide(self.raw_path.joinpath(f"{slide_id}.svs"))

        # specify resolution level
        if level is None:
            level = slide.level_count # lowest resolution by default
        if level > slide.level_count - 1:
            level = slide.level_count - 1
        # load in region
        size = slide.level_dimensions[level]
        region = slide.read_region((0,0), level, size)
        # add transforms
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Lambda(lambda x: x[:3, :, :]), # remove alpha channel
            transforms.Resize((self.wsi_height, self.wsi_width)),
            RearrangeTransform("c h w -> h w c") # rearrange for Healnet architecture
        ])
        region_tensor = transform(region)
        return slide, region_tensor

    def load_patch_features(self, slide_id: str) -> torch.Tensor:
        """
        Loads patch features for a single slide from torch.pt file
        Args:
            slide_id (str): Slide ID

        Returns:
            torch.Tensor: Patch features
        """
        load_path = self.prep_path.joinpath(f"patch_features/{slide_id}.pt")
        with open(load_path, "rb") as file:
            patch_features = torch.load(file, weights_only=True)
        patch_features = patch_features.permute(1, 0)
        return patch_features



class SharedLRUCache:
    """
    Shared LRU cache for multiprocessing
    """
    def __init__(self, capacity: int):
        """

        Args:
            capacity (int): Number of items to be stored in the cache
        """
        manager = Manager()
        self.capacity = capacity
        self.cache = manager.dict()
        self.order = manager.list()
        self.lock = Lock()
    def get(self, key: int):
        with self.lock:
            if key in self.cache:
                # Move key to end to show it was recently used.
                self.order.remove(key)
                self.order.append(key)
                return self.cache[key]
            else:
                return None

    def set(self, key: int, value):
        with self.lock:
            if key in self.cache:
                self.order.remove(key)
            else:
                if len(self.order) >= self.capacity:
                    removed_key = self.order.pop(0)  # Remove the first (least recently used) item.
                    del self.cache[removed_key]

            self.order.append(key)
            self.cache[key] = value

    def __contains__(self, key):
        return key in self.cache


class RearrangeTransform(object):
    """
    Wrapper for einops.rearrange to pass into torchvision.transforms.Compose
    """
    def __init__(self, pattern):
        self.pattern = pattern

    def __call__(self, img):
        img = rearrange(img, self.pattern)
        return img

class RepeatTransform(object):
    """
    Wrapper for einops.repeat to pass into torchvision.transforms.Compose
    """
    def __init__(self, pattern, b):
        self.pattern = pattern
        self.b = b
    def __call__(self, img):
        img = repeat(img, self.pattern, b=self.b)
        return img


if __name__ == '__main__':
    # os.chdir("../../")
    # config = Config("config/main.yml").read()
    # brca = TCGADataset("brca", config)
    # blca = TCGADataset("blca", config)
    # print(config)
    # print(brca.omic_df.shape, blca.omic_df.shape)
    # blca.load_wsi("TCGA-2F-A9KT-01Z-00-DX1.ADD6D87C-0CC2-4B1F-A75F-108C9EB3970F", resolution="lowest")

    from torch.utils.data import DataLoader
    
    n=50
    tab_tensor = torch.rand(size=(n, 1, 10))
    img_tensor = torch.rand(size=(n, 224, 224, 1))
    vid_tensor = torch.rand(size=(n, 12, 224, 224, 1))
    
    target = torch.rand(size=(n,))
    
    data = MMDataset([tab_tensor, img_tensor, vid_tensor], target)
    
    loader = DataLoader(data, batch_size=4, shuffle=True)
    
    # fetch batch
    tensors, target = next(iter(loader))
    
    print([t.shape for t in tensors])


# if __name__ == "__main__":
</file>

<file path="healnet/etl/preprocess.py">
# preprocess WSI images



# tissue segmentation

# image patch sizes: 256 x 256, no overlap
# extracted at the 20x pyramid level
# In paper: used ResNet50 to convert each patch into 1024-dim feature vector
</file>

<file path="healnet/etl/utils.py">
from healnet.etl import TCGADataset
from healnet.utils import Config
from pathlib import Path

def filter_manifest_files(config: Config, dataset: str):
    """
    Temporary util to filter the manifest files to only include the
    WSI images required
    Returns:
    """
    mol_df = TCGADataset(dataset, config).omic_df
    manifest_path = Path(config.tcga_path).joinpath(f"gdc_manifests/full/{dataset}_wsi_manifest_full.txt")
    manifest_df = pd.read_csv(manifest_path, sep="\t")
    manifest_filtered = manifest_df.loc[manifest_df.filename.isin(mol_df["slide_id"])]

    assert manifest_filtered.shape[0] == mol_df.shape[0], "Number of filtered manifest files does not match number of omic files"

    write_path = Path(config.tcga_path).joinpath(f"gdc_manifests/filtered/{dataset}_wsi_manifest_filtered.txt")
    manifest_filtered.to_csv(write_path, sep="\t", index=False)
    print(f"Saved filtered manifest file to {write_path}")
    return None
</file>

<file path="healnet/models/__init__.py">
from healnet.models.healnet import HealNet, Attention
from healnet.baselines import FCNN
from healnet.models.survival_loss import CrossEntropySurvLoss, CoxPHSurvLoss

__all__ = [
           "CrossEntropySurvLoss",
           "CoxPHSurvLoss",
           "FCNN",
           "HealNet",
           "Attention",
           ]
</file>

<file path="healnet/models/base.py">
"""
Some abstract base classes required to run baselines
"""

import os, sys, random, yaml
from itertools import product
from tqdm import tqdm

import numpy as np
import matplotlib as mpl
#mpl.use('Agg')
import matplotlib.pyplot as plt
from sklearn.utils import shuffle

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
from torch import optim

# from utils import *
import IPython

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class AbstractModel(nn.Module):

    def __init__(self):
        super(AbstractModel, self).__init__()
        self.compiled = False

    # Compile module and assign optimizer + params
    def compile(self, optimizer=None, **kwargs):

        if optimizer is not None:
            self.optimizer_class = optimizer
            self.optimizer_kwargs = kwargs

        self.optimizer = self.optimizer_class(self.parameters(), **self.optimizer_kwargs)
        self.compiled = True
        self.to(DEVICE)

    # Process a batch of data from a generator to get tensors + masks
    def __process_batch(self, data):

        data, mask = stack(data), {}
        for key in data:
            mask[key] = [int(x is not None) for x in data[key]]
            template = next((x for x in data[key] if x is not None))
            data[key] = [x if x is not None else torch.zeros_like(template) \
                            for x in data[key]]
            data[key] = torch.stack(data[key]).to(DEVICE)
            mask[key] = torch.tensor(mask[key]).to(DEVICE)

        return data, mask

    # Predict scores from a batch of data
    def predict_on_batch(self, data):

        self.eval()
        with torch.no_grad():
            data, mask = self.__process_batch(data)
            pred = self.forward(data, mask)
            pred = {key: pred[key].cpu().data.numpy() for key in pred}
            return pred

    # Fit (make one optimizer step) on a batch of data
    def fit_on_batch(self, data, target):

        self.train()
        self.zero_grad()
        self.optimizer.zero_grad()

        data, mask = self.__process_batch(data)
        pred = self.forward(data, mask)
        target = stack(target)
        target = {key: torch.stack(target[key]).to(DEVICE) for key in target}

        loss = self.loss(pred, target)
        loss.backward()
        self.optimizer.step()

        pred = {key: pred[key].cpu().data.numpy() for key in pred}
        return pred, float(loss)

    # Subclasses: please override for custom loss + forward functions
    def loss(self, pred, target):
        raise NotImplementedError()

    def forward(self, data, mask):
        raise NotImplementedError()



class TrainableModel(AbstractModel):

    def __init__(self):
        super(AbstractModel, self).__init__()
        self.compiled = False
        self.losses = []

    # Predict on generator for one epoch
    def predict(self, data, verbose=False):

        self.eval()
        with torch.no_grad():
            iterator = tqdm(data) if verbose else data
            pred = [self.predict_on_batch(batch) for batch in iterator]
            pred = np.hstack(pred)
        return pred

    # Fit on generator for one epoch
    def fit(self, datagen, validation=None, verbose=True):

        self.train()

        target = []
        iterator = tqdm(datagen) if verbose else datagen
        pred = []

        for batch, y in iterator:

            y_pred, loss = self.fit_on_batch(batch, y)
            self.losses.append(loss)

            target.append(stack(y))
            pred.append(y_pred)

            if verbose and len(self.losses) % 16 == 0:
                iterator.set_description(f"Loss: {np.mean(self.losses[-32:]):0.3f}")
                #plt.plot(self.losses)
                #plt.savefig(f"{OUTPUT_DIR}/loss.jpg");

        if verbose:
            pred, target = stack(pred), stack(target)
            pred = {key: np.concatenate(pred[key], axis=0) for key in pred}
            target = {key: np.concatenate(target[key], axis=0) for key in target}

            print (f"(training) {self.evaluate(pred, target)}")

            if validation != None:
                val_data, val_target = zip(*list(validation))
                val_pred = self.predict(val_data)

                val_target = [stack(x) for x in val_target]
                val_pred, val_target = stack(val_pred), stack(val_target)
                val_pred = {key: np.concatenate(val_pred[key], axis=0) for key in val_pred}
                val_target = {key: np.concatenate(val_target[key], axis=0) for key in val_target}

                print (f"(validation) {self.evaluate(val_pred, val_target)}")
            return self.score(val_pred, val_target)

    # Evaluate predictions and targets
    def evaluate(self, pred, target):

        scores = self.score(pred, target)
        base_scores = self.score(pred, {key: shuffle(target[key]) for key in target})

        display = []
        for key in scores:
            display.append(f"{key}={scores[key]:.4f}/{base_scores[key]:.4f}")
        return ", ".join(display)

    def eval_data(self, datagen):

        val_data, val_target = zip(*list(datagen))
        val_pred = self.predict(val_data)

        val_target = [stack(x) for x in val_target]
        val_pred, val_target = stack(val_pred), stack(val_target)
        val_pred = {key: np.concatenate(val_pred[key], axis=0) for key in val_pred}
        val_target = {key: np.concatenate(val_target[key], axis=0) for key in val_target}

        return self.score(val_pred, val_target)["C-index"]




    # Score generator based on predictions and targets
    def score(self, pred, targets):
        return NotImplementedError()

    def load(self, file_path):
        self.load_state_dict(torch.load(file_path))

    def save(self, file_path):
        torch.save(self.state_dict(), file_path)


def stack(batch, targets=None):
    """
    Turns an array (batch) of dictionaries into a dictionary of arrays
    """
    keys = batch[0].keys()
    data = {key: [] for key in keys}

    for item, key in product(batch, keys):
        data[key].append(item.get(key, None))
    return data

def masked_mean(data, masks):

    num = sum((X*mask[:, None].float() for X, mask in zip(data, masks)))
    denom = sum((mask for mask in masks))[:, None].float()
    return num/denom

def masked_variance(data, masks):
    EX2 = masked_mean(data, masks)**2
    E2X = masked_mean((x**2 for x in data), masks)
    return E2X - EX2
</file>

<file path="healnet/models/baselines.py">
import torch.nn as nn
import torch
import torch.nn.functional as F
import random, itertools
from torch import optim
from typing import List
from sklearn.model_selection import train_test_split
from torchvision import datasets, transforms, models
from sklearn.metrics import accuracy_score, roc_auc_score
from lifelines.utils import concordance_index
from scipy.stats import pearsonr
from healnet.models.base import TrainableModel, stack, masked_variance, masked_mean

import einops
class FCNN(nn.Module):

    def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int, dropout: float = 0.5):
        super().__init__()

        # Construct topology
        self.input_layer = nn.Linear(input_size, hidden_sizes[0])
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.hidden_layers = nn.ModuleList([
            nn.Linear(hidden_sizes[i], hidden_sizes[i+1]) for i in range(len(hidden_sizes)-1)
        ])

        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)

    def forward(self, x):
        # only select second to last dimension
        x = x.squeeze()
        # x = einops.rearrange(x, 'b feat c -> feat c')
        x = self.relu(self.input_layer(x))
        for layer in self.hidden_layers:
            # apply droput and acvitations to hidden layers
            x = self.relu(self.dropout(layer(x)))
        x = self.output_layer(x)
        return x

class RegularizedFCNN(nn.Module):
    def __init__(self, output_dim, dropout_rate=0.2, l1_penalty=0.01, l2_penalty=0.01):
        super(RegularizedFCNN, self).__init__()

        # Store the attributes
        self.output_dim = output_dim
        self.dropout_rate = dropout_rate
        self.l1_penalty = l1_penalty
        self.l2_penalty = l2_penalty

        # Placeholder for layers
        self.input_layer = None
        self.hidden_layer = nn.Linear(128, 64)
        self.dropout_layer = nn.Dropout(dropout_rate)
        self.output_layer = nn.Linear(64, output_dim)

    def forward(self, inputs: List[torch.Tensor]):

        if type(inputs) == list:
            inputs = inputs[0]

        # Get the input dimension and create the input layer if it doesn't exist
        if self.input_layer is None:
            input_dim = inputs.shape[1]
            self.input_layer = nn.Linear(input_dim, 128).to(inputs.device)

        x = F.relu(self.input_layer(inputs))
        x = F.relu(self.hidden_layer(x))
        x = self.dropout_layer(x)
        return torch.sigmoid(self.output_layer(x))

    def l1_regularization(self):
        l1_reg = torch.tensor(0., requires_grad=True)
        for name, param in self.named_parameters():
            if 'weight' in name:
                l1_reg = l1_reg + torch.norm(param, 1)
        return self.l1_penalty * l1_reg

    def l2_regularization(self):
        l2_reg = torch.tensor(0., requires_grad=True)
        for name, param in self.named_parameters():
            if 'weight' in name:
                l2_reg = l2_reg + torch.norm(param, 2)
        return self.l2_penalty * l2_reg
</file>

<file path="healnet/models/classification_utils.py">
"""
Old functions that used to be part of the main pipeline, but are now deprecated.
"""

def train_clf(self,
                  model: nn.Module,
                  train_data: DataLoader,
                  test_data: DataLoader,
                  **kwargs):
        """
        Trains model and evaluates classification model
        Args:
            model:
            train_data:
            test_data:
            **kwargs:
        Returns:
        """
        print(f"Training classification model")
        optimizer = optim.SGD(model.parameters(),
                              lr=self.config["optimizer.lr"],
                              momentum=self.config["optimizer.momentum"],
                              weight_decay=self.config["optimizer.weight_decay"]
                              )
        # set efficient OneCycle scheduler, significantly reduces required training iters
        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,
                                                  max_lr=self.config["optimizer.max_lr"],
                                                  epochs=self.config["train_loop.epochs"],
                                                  steps_per_epoch=len(train_data))


        criterion = nn.CrossEntropyLoss(weight=self.class_weights)

        # use survival loss for survival analysis which accounts for censored data
        model.train()

        majority_train_acc = np.round(majority_classifier_acc(train_data.dataset.dataset.y_disc), 5)

        for epoch in range(self.config["train_loop.epochs"]):
            print(f"Epoch {epoch}")
            running_loss = 0.0
            predictions = []
            labels = []
            for batch, (features, _, _, y_disc) in enumerate(tqdm(train_data)):
                # only move to GPU now (use CPU for preprocessing)
                labels.append(y_disc.tolist())
                y_disc = y_disc.to(self.device)
                features = features.to(self.device)
                # features, y_disc = features.to(self.device), y_disc.to(self.device)
                if batch == 0 and epoch == 0: # print model summary
                    print(features.shape)
                    print(features.dtype)
                optimizer.zero_grad()
                # forward + backward + optimize

                outputs = model.forward(features)
                loss = criterion(outputs, y_disc)
                # temporary

                loss.backward()
                optimizer.step()
                scheduler.step()
                # print statistics
                running_loss += loss.item()
                predictions.append(outputs.argmax(1).cpu().tolist())

            predictions = np.concatenate(predictions)
            labels = np.concatenate(labels)
            train_loss = np.round(running_loss / len(train_data), 5)
            train_acc = np.round(accuracy_score(y_true=labels, y_pred=predictions), 5)
            train_f1 = np.round(f1_score(y_true=labels, y_pred=predictions, average="weighted"), 5)
            train_confusion_matrix = confusion_matrix(y_true=labels, y_pred=predictions)
            # train_auc = np.round(roc_auc_score(y_true=epoch_labels, y_score=epoch_predictions, average="weighted", multi_class="ovr"), 5)
            # predict entire train set
            print(f"Batch {batch+1}, train_loss: {train_loss}, "
                  f"train_acc: {train_acc}, "
                  f"train_f1: {train_f1}, "
                  f"majority_train_acc: {majority_train_acc}")
            print(f"train_confusion_matrix: \n {train_confusion_matrix}")
            wandb.log({"train_loss": train_loss,
                       "train_acc": train_acc,
                       "train_f1": train_f1,
                       # "majority_train_acc": majority_train_acc
                       }, step=epoch)
            wandb.log({"train_conf_matrix": wandb.plot.confusion_matrix(y_true=labels, preds=predictions)}, step=epoch)
            running_loss = 0.0

            if epoch % self.config["train_loop.eval_interval"] == 0:
                # print("**************************")
                # print(f"EPOCH {epoch} EVALUATION")
                # print("**************************")
                self.evaluate_clf_epoch(model, test_data, criterion, epoch)


    def evaluate_clf_epoch(self, model: nn.Module, test_data: DataLoader, criterion: nn.Module, epoch: int):
        model.eval()
        majority_val_acc = majority_classifier_acc(y_true=test_data.dataset.dataset.y_disc)
        val_loss = 0.0
        val_acc = 0.0
        predictions = []
        labels = []
        with torch.no_grad():
            for batch, (features, _, _, y_disc) in enumerate(test_data):
                labels.append(y_disc.tolist())
                features, y_disc = features.to(self.device), y_disc.to(self.device)
                outputs = model.forward(features)
                loss = criterion(outputs, y_disc)
                val_loss += loss.item()
                val_acc += (outputs.argmax(1) == y_disc).sum().item()
                predictions.append(outputs.argmax(1).cpu().tolist())
        val_loss = np.round(val_loss / len(test_data), 5)
        predictions = np.concatenate(predictions)
        labels = np.concatenate(labels)
        val_acc = np.round(accuracy_score(labels, predictions), 5)
        val_f1 = np.round(f1_score(labels, predictions, average="weighted"), 5)
        val_conf_matrix = confusion_matrix(labels, predictions)
        print(f"val_loss: {val_loss}, "
              f"val_acc: {val_acc}, "
              f"val_f1: {val_f1}, "
              f"majority_test_acc: {majority_val_acc}")
        print(f"val_conf_matrix: \n {val_conf_matrix}")
        wandb.log({"val_loss": val_loss,
                   "val_acc": val_acc,
                   "val_f1": val_f1,
                   })
        wandb.log({"val_conf_matrix": wandb.plot.confusion_matrix(y_true=labels, preds=predictions)}, step=epoch)
        model.train()
</file>

<file path="healnet/models/explainer.py">
from pathlib import Path
from box import Box
import random
# from torch.utils.data import DataLoader
from healnet.etl import TCGADataset
from copy import deepcopy
from healnet.models import HealNet
from healnet.utils import unpickle
import seaborn as sns
import numpy as np
import torch
from typing import *
import h5py
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import cv2
import pandas as pd
from openslide import OpenSlide
from scipy.ndimage import zoom


class Explainer(object):
    def __init__(self, log_dir: str, show=False):
        self.log_dir = Path(log_dir)
        # run name
        self.show = show
        self.expl_dir = Path(f"explanations/{self.log_dir.name}")
        self.expl_dir.mkdir(parents=True, exist_ok=True)
        self.config = unpickle(self.log_dir.joinpath("config.pkl"))
        self.level = self.config["data.wsi_level"]
        self.dataset = self.config.dataset
        self.test_data_indices = unpickle(self.log_dir.joinpath("test_data_indices.pkl"))
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.prep_path = Path(self.config["tcga_path"]).joinpath(f"wsi/{self.dataset}_preprocessed_level{self.level}/")
        self.raw_path = Path(self.config.tcga_path).joinpath(f"wsi/{self.dataset}")


        # plt.rcParams["font.family"] = "Avenir"

        print("Initializing dataset...")
        self.data = TCGADataset(
            dataset=self.dataset,
            config=self.config,
            level=self.level,
            sources=self.config.sources,
            n_bins=self.config["model_params.output_dims"],
            log_dir=None,
        )
        self.omic_df = self.data.omic_df
        print("Loading model...")
        self.model = self.load_model()
        self.model.eval()

    def run(self, n_high: int = 3, n_low: int = 0,
            downsample: float = None,
            run_omic: bool = True,
            run_slides: bool = True,
            heatmap: bool = True,
            highlight_patches: bool = True,
            save_patches: bool=True):
        """
        Run explanation for n_high high risk patients and n_low low risk patients
        Args:
            n_high:
            n_low:

        Returns:

        """
        self.high_risk = self.get_patients(risk="high", n=n_high)
        self.low_risk = self.get_patients(risk="low", n=n_low)
        self.heatmap = heatmap
        self.highlight_patches = highlight_patches

        # high risk
        # for i in range(n_high):
        for i in [2,3,5]:
        # for i in [2]: # final run
            self.save_name = f"high_risk_{i}"
            self.run_sample_explanation(self.high_risk[i:i+1], downsample=downsample, run_omic=run_omic, run_slides=run_slides, save_patches=save_patches)

        # low risk
        for i in range(n_low):
            self.run_sample_explanation(self.low_risk[i:i+1], downsample=downsample, run_omic=run_omic, run_slides=run_slides, save_patches=save_patches)



    def run_sample_explanation(self, sample: str, run_omic: bool = True, run_slides: bool = True, downsample: float = None, save_patches:bool=True):
        idx, slide_id = sample.index[0], sample.iloc[0]
        patch_coords = self.load_patch_coords(sample.iloc[0])
        self.slide, region = self.load_wsi(sample.iloc[0], level=self.level)

        (omic_tensor, slide_tensor), _, _, _ = self.data[idx]

        slide_tensor = slide_tensor.unsqueeze(0).to(self.device)
        n_patches = slide_tensor.shape[1]
        # add batch dim
        omic_tensor = omic_tensor.unsqueeze(0).to(self.device)
        n_features= omic_tensor.shape[1]
        logits = self.model([omic_tensor, slide_tensor])
        probs = torch.softmax(logits, dim=1)
        attn_weights = self.model.get_attention_weights()
        slide_attn = [w for w in attn_weights if w.shape[2] == n_patches]
        omic_attn = [w for w in attn_weights if w.shape[2] == n_features]

        k = 20
        self.color = "Blues"
        self.pallete = sns.color_palette(self.color, n_colors=k)[::-1] # reverse order to get increasingly darker

        if len(omic_attn) > 0 and run_omic:
            # plot average
            self.plot_omic_attn(omic_attn, agg_layers=False, k=k)
            # plot_by layer
            # for i in range(len(omic_attn)):
            #     self.plot_omic_attn(omic_attn, layer=i, agg_layers=False)

        if len(slide_attn) > 0 and run_slides:
            print(f"Reading slide...")

            slide_img = self.slide.read_region(location=(0, 0), level=self.level, size=self.slide.level_dimensions[self.level])

            # save original image
            plt.imshow(slide_img)
            plt.axis('off')
            save_path = self.expl_dir.joinpath(f"{self.save_name}_original.png")
            slide_img.save(save_path)
            # plt.savefig(save_path, bbox_inches='tight', pad_inches=0)
            plt.show()

            slide_img = np.array(slide_img)[:, :, :3]  # Convert PIL image to array and remove alpha if present

            # pick layer with highest std deviation across patches
            layer_to_viz = np.argmax([torch.std(w).detach().cpu() for w in slide_attn])
            # None --> mean across all layers
            self.plot_slide_attn(slide_img, slide_attn, patch_coords, layer=None, downsample=downsample, save_patches=save_patches)




            # for layer in range(len(slide_attn)):
            #     try:
            #         self.plot_slide_attn(slide_img, slide_attn, patch_coords, layer=layer)
            #     except:
            #         pass


    def plot_omic_attn(self, omic_attn, k: int=20, scale_fraction: float=0.5, layer: int=0, agg_layers: bool=False):
        """

        Args:
            omic_attn:
            k:
            scale_fraction:
            layer:

        Returns:

        """
        if agg_layers:
            # average attention across 1) all layers and 2) all heads
            omic_attn = torch.stack(omic_attn).mean(dim=0).mean(dim=1).detach().cpu().numpy()
        else:
            layer_to_viz = np.argmax([torch.std(w).detach().cpu() for w in omic_attn])
            omic_attn = torch.mean(omic_attn[layer_to_viz], dim=1).detach().cpu().numpy()
        feats = self.data.features.columns.tolist()
        plot_df = pd.DataFrame({"feature": feats, "attention": omic_attn.squeeze()}).sort_values(by="attention", ascending=False)
        # filter "age" and "is_female" (not omic feature)
        plot_df = plot_df[~plot_df["feature"].str.contains("age|is_female")]
        # take top scale_fraction features
        # plot_df = plot_df.iloc[:int(scale_fraction * len(feats))]


        min_attn = plot_df["attention"].min()
        max_attn = plot_df["attention"].max()
        plot_df = plot_df.iloc[:k]
        # min_attn = np.percentile(plot_df["attention"], 40)
        plot_df["attention_scaled"] = ((plot_df["attention"] - min_attn) / (max_attn - min_attn) / k)
        # apply sigmoid
        plt.figure(figsize=(6, 10))
        sns.barplot(data=plot_df, y="feature", x="attention_scaled", palette=self.pallete, )
        lower_lim = plot_df["attention_scaled"].min() - 0.005 * plot_df["attention_scaled"].min()
        uppler_lim = plot_df["attention_scaled"].max() + 0.001 * plot_df["attention_scaled"].max()
        # lower_lim =
        # uppler_lim = plot
        plt.xlim(lower_lim, uppler_lim)
        # if agg_layers:
        #     # plt.title(f"Mean Omic Attention")
        # else:
        #     plt.title(f"Layer {layer+1} Omic Attention")
        plt.yticks(fontsize=12, rotation=30)
        plt.xticks(fontsize=12, rotation=30)
        plt.xlabel("Attention Scaled", fontsize=12)
        plt.ylabel("Feature", fontsize=12)
        plt.subplots_adjust(left=0.3)
        save_path = self.expl_dir.joinpath(f"{self.save_name}_omic_attn.png")
        print(f"Saving to {save_path}")
        plt.savefig(self.expl_dir.joinpath(f"{self.save_name}_omic_attn.png"))
        if self.show:
            plt.show()

    def plot_slide_attn(self, slide_img: np.array,
                        slide_attn: torch.Tensor,
                        patch_coords: np.array,
                        layer: int=0, downsample: float = None, save_patches: bool=True):

        patch_size = (256, 256)
        if layer is None:
            # take mean across all layers
            slide_attn = torch.stack(slide_attn).mean(dim=0).mean(dim=1).squeeze().detach().cpu().numpy()
        else:
            slide_attn = torch.mean(slide_attn[layer], dim=1).squeeze().detach().cpu().numpy()
        slide_attn = slide_attn[:len(patch_coords)]
        x_coord = patch_coords[:, 0]
        y_coord = patch_coords[:, 1]
        plot_df = pd.DataFrame({"x": x_coord, "y": y_coord, "attention": slide_attn})
        # plot_df = pd.DataFrame({"patch": patch_coords, "attention": slide_attn}).sort_values(by="attention", ascending=False)
        # normalize attention scores
        slide_dims = self.slide.level_dimensions
        original_dims  = slide_dims[0]
        # scale X depending on which level we are reading the slide at
        scale_factor = int(original_dims[0] / slide_dims[self.level][0])
        plot_df["x_scaled"] = (plot_df["x"] / scale_factor).astype(int)
        plot_df["y_scaled"] = (plot_df["y"] / scale_factor).astype(int)
        plot_df["attention_scaled"] = (plot_df["attention"] - plot_df["attention"].min()) / (plot_df["attention"].max() - plot_df["attention"].min())

        if downsample is not None:
            slide_img = zoom(np.array(slide_img), (downsample, downsample, 1))
            plot_df['x_scaled'] = (plot_df['x_scaled'] * downsample).astype(int)
            plot_df['y_scaled'] = (plot_df['y_scaled'] * downsample).astype(int)
            patch_size = (int(patch_size[0] * downsample), int(patch_size[1] * downsample))

        if self.highlight_patches:
            self.highlight_top_patches(slide_img=slide_img, df=plot_df, patch_size=patch_size, show=True, layer=layer)
        if self.heatmap:
            self.create_heatmap(slide_img=slide_img, patch_size=patch_size, df=plot_df, show=True, layer=layer)


        # save top 5 patches as images
        if save_patches:
            top_df = plot_df.sort_values(by='attention_scaled', ascending=False).head(5)
            for index, row in top_df.iterrows():
                x, y = int(row['x_scaled']), int(row['y_scaled'])
                patch = slide_img[y:y+patch_size[1], x:x+patch_size[0]]
                plt.imshow(patch)
                plt.axis('off')
                save_path = self.expl_dir.joinpath(f"{self.save_name}_patch_{index}.png")
                plt.savefig(save_path, bbox_inches='tight', pad_inches=0)
                print(f"Saving to {save_path}")
                plt.show()

            # also save in highest res
            for index, row in top_df.iterrows():
                x, y = int(row["x"]), int(row["y"])
                patch_size_orig = (256 * scale_factor, 256*scale_factor)
                print(patch_size_orig)
                patch = self.slide.read_region(location=(x, y), level=0, size=patch_size_orig)
                save_path = self.expl_dir.joinpath(f"{self.save_name}_patch_{index}_high_res.png")
                patch.save(save_path)
                plt.imshow(patch)
                plt.axis("off")
                plt.show()


    def highlight_top_patches(self, slide_img, df, patch_size, show=True, layer: int=1):


        """
        Highlights the top 5 features with highest attention scores on an OpenSlide image using a frame.

        Parameters:
        - slide_img: PIL image of the entire slide read in at specified level
        - df: DataFrame containing 'x_scaled', 'y_scaled', and 'attention_scaled' columns.
        - patch_size: size of each patch to frame.
        - show: Boolean to decide whether to display the image with highlighted features.

        Returns:
        - Image with highlighted top features.
        """
        print(f"Highlighting top patches...")

        # Sort dataframe by attention_scaled in descending order and take top 5
        top_df = df.sort_values(by='attention_scaled', ascending=False).head(5)

        # Initialize a figure for plotting
        fig, ax = plt.subplots(figsize=(10, 10))
        ax.imshow(slide_img)

        # Draw frames for the top 5 features
        for index, row in top_df.iterrows():
            x, y = int(row['x_scaled']), int(row['y_scaled'])
            rect = patches.Rectangle((x, y), patch_size[0], patch_size[1], linewidth=2, edgecolor='lime', facecolor='none')
            ax.add_patch(rect)

        # Hide axes and display image if show is True
        plt.axis('off')
        plt.savefig(self.expl_dir.joinpath(f"{self.save_name}_patch_highlights.png"), dpi=300)
        if self.show:
            # plt.title(f"Layer {layer+1} High Attention Patches", size=15)
            plt.show()



    def create_heatmap(self, slide_img, df, patch_size, show=True, layer: int=1, mask_cutoff: float=0.0):
        """
        Creates a heatmap from attention scores on an OpenSlide image.

        Parameters:
        - slide_img: PIL image of the entire slide read in at specified level
        - df: DataFrame containing 'x', 'y', and 'attention' columns.
        - patch_size: size of each patch to highlight.
        - colormap: colormap to use for heatmap.

        Returns:
        - heatmap overlayed on the slide image.
        """

        print(f"Creating heatmap...")
        # Create an empty heatmap of same size as slide
        heatmap = np.zeros(slide_img.shape[:2])

        # Convert DataFrame columns to NumPy arrays
        xs = df['x_scaled'].values.astype(int)
        ys = df['y_scaled'].values.astype(int)
        attentions = df['attention_scaled'].values
        for x, y, attention in zip(xs, ys, attentions):
            heatmap[y:y+patch_size[1], x:x+patch_size[0]] = attention
        mask = heatmap <= mask_cutoff

        # save heatmap and mask
        np.save(self.expl_dir.joinpath(f"{self.save_name}_heatmap.npy"), heatmap)
        np.save(self.expl_dir.joinpath(f"{self.save_name}_mask.npy"), mask)

        # Show the heatmap and legend if the 'show' parameter is set to True
        if show:
            plt.figure(figsize=(10, 10))  # You can adjust the figure size as needed

            plt.imshow(slide_img)
            cbar_kws = {"shrink": 0.5} # colour bar scale
            ax = sns.heatmap(heatmap, cmap=self.color, alpha=0.7,
                             cbar=True, annot=False, cbar_kws=cbar_kws, mask=mask)
            # Add colorbar to represent the attention values
            cbar = ax.collections[0].colorbar
            cbar.set_ticks([0, heatmap.max()])
            cbar.set_label('Attention', size=15)
            # plt.title(f"Layer {layer+1} Attention Heatmap", size=15)
            plt.axis('off')
            plt.savefig(self.expl_dir.joinpath(f"{self.save_name}_heatmap.png"), dpi=300)
            if self.show:
                plt.show()


        # return blended





    def load_model(self):
        state_dict = torch.load(self.log_dir.joinpath("best_model.pt"), map_location=self.device)

        feat, _, _, _ = next(iter(self.data))

        num_sources = len(self.config["sources"])
        if num_sources == 1:
            input_channels = [feat[0].shape[1]]
            input_axes = [1]
            modalities = 1
        elif num_sources == 2:
            input_channels = [feat[0].shape[1], feat[1].shape[1]]
            input_axes = [1, 1]
            modalities = 2

        # reload model
        model = HealNet(
                n_modalities=modalities,
                channel_dims=input_channels, # number of features as input channels
                num_spatial_axes=input_axes, # second axis (b n_feats c)
                out_dims=self.config[f"model_params.output_dims"],
                num_freq_bands=self.config[f"model_params.num_freq_bands"],
                depth=self.config[f"model_params.depth"],
                max_freq=self.config[f"model_params.max_freq"],
                l_c = self.config[f"model_params.num_latents"],
                l_d = self.config[f"model_params.latent_dim"],
                cross_dim_head = self.config[f"model_params.cross_dim_head"],
                latent_dim_head = self.config[f"model_params.latent_dim_head"],
                x_heads = self.config[f"model_params.cross_heads"],
                l_heads = self.config[f"model_params.latent_heads"],
                attn_dropout = self.config[f"model_params.attn_dropout"],
                ff_dropout = self.config[f"model_params.ff_dropout"],
                weight_tie_layers = self.config[f"model_params.weight_tie_layers"],
                fourier_encode_data = self.config[f"model_params.fourier_encode_data"],
                self_per_cross_attn = self.config[f"model_params.self_per_cross_attn"],
                final_classifier_head = True,
                snn = self.config[f"model_params.snn"],
            )
        model.float()
        model.to(self.device)

        # recover state
        model.load_state_dict(state_dict)

        return model


    def load_patch_coords(self, slide_id: str):
        patch_path = self.prep_path.joinpath(f"patches/{slide_id}.h5")
        h5_file = h5py.File(patch_path, "r")
        patch_coords = h5_file["coords"][:]

        return patch_coords

    # def load_slide(self, slide_id: str):

    def load_wsi(self, slide_id: str, level: int = None) -> Tuple:
        """
        Load in single slide and get region at specified resolution level
        Args:
            slide_id:
            level:
            resolution:

        Returns:
            Tuple (openslide object, tensor of region)
        """

        # load in openslide object
        # slide_path = self.wsi_paths[slide_id]
        # slide = OpenSlide(slide_path + ".svs")
        slide = OpenSlide(self.raw_path.joinpath(f"{slide_id}.svs"))

        # specify resolution level
        if level is None:
            level = slide.level_count # lowest resolution by default
        if level > slide.level_count - 1:
            level = slide.level_count - 1
        # load in region
        size = slide.level_dimensions[level]
        region = slide.read_region((0,0), level, size)
        # add transforms
        return slide, region




    def load_omic_df(self):

        data_path = Path(self.log_dir).joinpath(f"{self.dataset}_omic_overlap.csv.zip")
        df = pd.read_csv(data_path, compression="zip").drop("Unnamed: 0", axis=1)

        return df

    def best_model(self, model):
        torch.save(model.state_dict(), self.log_dir.joinpath("best_model.pt"))

    def get_patients(self, n: int=5, risk: str = "high") -> List[str]:
        """

        Args:
            n:

        Returns:
            List of slide IDs
        """
        assert risk in ["high", "low"], "Invalid risk type"

        filtered = self.omic_df.iloc[self.test_data_indices]
        ascending = True if risk == "high" else False

        filtered = filtered.sort_values(by=["y_disc", "survival_months"], ascending=ascending)

        risk_ids = filtered.iloc[:n]["slide_id"]
        # filter file extensions
        risk_ids = risk_ids.apply(lambda x: x[:-4])

        return risk_ids



if __name__ == "__main__":
    # log_path = "logs/devoted-universe-2701" # ucec level 1, omic attentio
    # log_path = "logs/pleasant-smoke-2704" # kirp level 2 (dev), omic attention
    # log_path= "logs/rural-sky-2709" # kirp level 1, omic attention
    # log_path = "logs/glad-meadow-3551" # ucec level 2, omic attention
    log_path="logs/smart-sweep-35" # ucec level 1, omic attention


    torch.multiprocessing.set_start_method("fork")

    e = Explainer(log_path, show=True)
    # e.run(n_high=3, n_low=0, downsample=None)
    e.run(n_high=10,
          n_low=0,
          downsample=0.25,
          run_omic=False,
          run_slides=True,
          heatmap=True,
          highlight_patches=False,
          save_patches=True,
          )

    # e.run(e.high_risk[2:3])

    #
    # for id in e.high_risk:
    #     e.run(id)
    # for id in e.low_risk:
    #     e.run(id)
</file>

<file path="healnet/models/healnet.py">
from math import pi, log
from functools import wraps
from typing import *

import torch
from torch import nn, einsum
import torch.nn.functional as F

from einops import rearrange, repeat
from einops.layers.torch import Reduce



class HealNet(nn.Module):
    def __init__(
        self,
        *,
        n_modalities: int,
        channel_dims: List,
        num_spatial_axes: List, 
        out_dims: int,
        depth: int = 3,
        num_freq_bands: int = 2,
        max_freq: float=10.,
        l_c: int = 128,
        l_d: int = 128,
        x_heads: int = 8,
        l_heads: int = 8,
        cross_dim_head: int = 64,
        latent_dim_head: int = 64,
        attn_dropout: float = 0.,
        ff_dropout: float = 0.,
        weight_tie_layers: bool = False,
        fourier_encode_data: bool = True,
        self_per_cross_attn: int = 1,
        final_classifier_head: bool = True,
        snn: bool = True,
    ):
        """
        Network architecture for easy-to-use multimodal fusion for any number and type of modalities.
        
        The input for each modality should be of shape ``(b, (*spatial_dims) c)``, where ``c`` corresponds to the dimensions 
        where positional encoding does not matter (e.g., color channels, set-based features, or tabular features). 

        Args:
            n_modalities (int): Maximum number of modalities for forward pass. Note that fewer modalities can be passed
                if modalities for individual samples are missing (see ``.forward()``)
            channel_dims (List[int]): Number of channels or tokens for each modality. Length must match ``n_modalities``. 
                The channel_dims are non-spatial dimensions where positional encoding is not required. 
            num_spatial_axes (List[int]): Spatial axes for each modality.The each spatial axis will be assigned positional 
                encodings, so that ``num_spatial_axis`` is 2 for 2D images, 3 for Video/3D images. 
            out_dims (int): Output shape of task-specific head. Forward pass returns logits of this shape. 
            num_freq_bands (int, optional): Number of frequency bands for positional encodings. Defaults to 2.
            max_freq (float, optional): Maximum frequency for positional encoding. Defaults to 10.
            l_c (int, optional): Number of channels for latent bottleneck array (akin to a "learned query array"). Defaults to 128.
            l_d (int, optional): Dimensions for latent bottleneck. Defaults to 128.
            x_heads (int, optional): Number of heads for cross attention. Defaults to 8.
            l_heads (int, optional): Number of heads for latent attention. Defaults to 8.
            cross_dim_head (int, optional): Dimension of each cross attention head. Defaults to 64.
            latent_dim_head (int, optional): Dimension of each latent attention head. Defaults to 64.
            attn_dropout (float, optional): Dropout rate for attention layers. Defaults to 0.
            ff_dropout (float, optional): Dropout rate for feed-forward layers. Defaults to 0.
            weight_tie_layers (bool, optional): False for weight sharing between fusion layers, True for specific 
                weights for each layer. Note that the number of parameters will multiply by ``depth`` if True. 
                Defaults to False.
            fourier_encode_data (bool, optional): Whether to use positional encoding. Recommended if meaningful spatial 
                spatial structure should be preserved. Defaults to True.
            self_per_cross_attn (int, optional): Number of self-attention layers per cross-attention layer. Defaults to 1.
            final_classifier_head (bool, optional): Whether to include a final classifier head. Defaults to True.
            snn (bool, optional): Whether to use a self-normalizing network. Defaults to True.

        Example: 
        
            ```python
            from healnet import HealNet
            from healnet.etl import MMDataset
            import torch
            import einops

            # synthetic data example
            n = 100 # number of samples
            b = 4 # batch size
            img_c = 3 # image channels
            tab_c = 1 # tabular channels
            tab_d = 2000 # tabular features
            # 2D dims
            h = 224 # image height
            w = 224 # image width
            # 3d dim
            d = 12

            tab_tensor = torch.rand(size=(n, tab_c, tab_d)) 
            img_tensor_2d = torch.rand(size=(n, h, w, img_c)) # h w c
            img_tensor_3d = torch.rand(size=(n, d, h, w, img_c)) # d h w c
            dataset = MMDataset([tab_tensor, img_tensor_2d, img_tensor_3d])

            [tab_sample, img_sample_2d, img_sample_3d] = dataset[0]

            # batch dim for illustration purposes
            tab_sample = einops.repeat(tab_sample, 'c d -> b c d', b=1) # spatial axis: None (pass as 1)
            img_sample_2d = einops.repeat(img_sample_2d, 'h w c -> b h w c', b=1) # spatial axes: h w
            img_sample_3d = einops.repeat(img_sample_3d, 'd h w c -> b d h w c', b=1) # spatial axes: d h w

            tensors = [tab_sample, img_sample_2d, img_sample_3d]


            model = HealNet(
                        n_modalities=3, 
                        channel_dims=[2000, 3, 3], # (2000, 3, 3) number of channels/tokens per modality
                        num_spatial_axes=[1, 2, 3], # (1, 2, 3) number of spatial axes (will be positionally encoded to preserve spatial information)
                        out_dims = 4
                    )

            # example forward pass
            logits = model(tensors)
            ```
        """
        
        
        super().__init__()
        assert len(channel_dims) == len(num_spatial_axes), 'input channels and input axis must be of the same length'
        assert len(num_spatial_axes) == n_modalities, 'input axis must be of the same length as the number of modalities'

        self.input_axes = num_spatial_axes
        self.input_channels=channel_dims
        self.max_freq = max_freq
        self.num_freq_bands = num_freq_bands
        self.modalities = n_modalities
        self.self_per_cross_attn = self_per_cross_attn

        self.fourier_encode_data = fourier_encode_data

        # get fourier channels and input dims for each modality
        fourier_channels = []
        input_dims = []
        for axis in num_spatial_axes:
            fourier_channels.append((axis * ((num_freq_bands * 2) + 1)) if fourier_encode_data else 0)
        for f_channels, i_channels in zip(fourier_channels, channel_dims):
            input_dims.append(f_channels + i_channels)


        # initialise shared latent bottleneck
        self.latents = nn.Parameter(torch.randn(l_c, l_d))

        # modality-specific attention layers
        funcs = []
        for m in range(n_modalities):
            funcs.append(lambda m=m: PreNorm(l_d, Attention(l_d, input_dims[m], heads = x_heads, dim_head = cross_dim_head, dropout = attn_dropout), context_dim = input_dims[m]))
        cross_attn_funcs = tuple(map(cache_fn, tuple(funcs)))

        get_latent_attn = lambda: PreNorm(l_d, Attention(l_d, heads = l_heads, dim_head = latent_dim_head, dropout = attn_dropout))
        get_cross_ff = lambda: PreNorm(l_d, FeedForward(l_d, dropout = ff_dropout, snn = snn))
        get_latent_ff = lambda: PreNorm(l_d, FeedForward(l_d, dropout = ff_dropout, snn = snn))

        get_cross_ff, get_latent_attn, get_latent_ff = map(cache_fn, (get_cross_ff, get_latent_attn, get_latent_ff))

        self.layers = nn.ModuleList([])


        for i in range(depth):
            should_cache = i > 0 and weight_tie_layers
            cache_args = {'_cache': should_cache}

            self_attns = nn.ModuleList([])

            for block_ind in range(self_per_cross_attn):
                self_attns.append(get_latent_attn(**cache_args, key = block_ind))
                self_attns.append(get_latent_ff(**cache_args, key = block_ind))


            cross_attn_layers = []
            for j in range(n_modalities):
                cross_attn_layers.append(cross_attn_funcs[j](**cache_args))
                cross_attn_layers.append(get_cross_ff(**cache_args))


            self.layers.append(nn.ModuleList(
                [*cross_attn_layers, self_attns])
            )

        self.to_logits = nn.Sequential(
            Reduce('b n d -> b d', 'mean'),
            nn.LayerNorm(l_d),
            nn.Linear(l_d, out_dims)
        ) if final_classifier_head else nn.Identity()




    def forward(self,
                tensors: List[Union[torch.Tensor, None]],
                mask: Optional[torch.Tensor] = None,
                return_embeddings: bool = False, 
                verbose: bool = False
                ):

        missing_idx = [i for i, t in enumerate(tensors) if t is None]
        if verbose: 
            print(f"Missing modalities indices: {missing_idx}")
        for i in range(len(tensors)):
            if i in missing_idx: 
                continue
            else: 
                data = tensors[i]
                # sanity checks
                b, *axis, _, device, dtype = *data.shape, data.device, data.dtype
                assert len(axis) == self.input_axes[i], (f'input data for modality {i+1} must hav'
                                                            f' the same number of axis as the input axis parameter')

            # fourier encode for each modality
            if self.fourier_encode_data:
                axis_pos = list(map(lambda size: torch.linspace(-1., 1., steps=size, device=device, dtype=dtype), axis))
                pos = torch.stack(torch.meshgrid(*axis_pos, indexing = 'ij'), dim = -1)
                enc_pos = fourier_encode(pos, self.max_freq, self.num_freq_bands)
                enc_pos = rearrange(enc_pos, '... n d -> ... (n d)')
                enc_pos = repeat(enc_pos, '... -> b ...', b = b)
                data = torch.cat((data, enc_pos), dim = -1)
                

            # concat and flatten axis for each modality
            data = rearrange(data, 'b ... d -> b (...) d')
            tensors[i] = data


        x = repeat(self.latents, 'n d -> b n d', b = b) # note: batch dim should be identical across modalities

        for layer_idx, layer in enumerate(self.layers):
            for i in range(self.modalities):
                if i in missing_idx: 
                    if verbose: 
                        print(f"Skipping update in fusion layer {layer_idx + 1} for missing modality {i+1}")
                        continue
                cross_attn=layer[i*2]
                cross_ff = layer[(i*2)+1]
                try:
                    x = cross_attn(x, context = tensors[i], mask = mask) + x
                    x =  cross_ff(x) + x
                except:
                    pass

                if self.self_per_cross_attn > 0:
                    self_attn, self_ff = layer[-1]

                    x = self_attn(x) + x
                    x = self_ff(x) + x

        if return_embeddings:
            return x

        return self.to_logits(x)

    def get_attention_weights(self) -> List[torch.Tensor]:
        """
        Helper function which returns all attention weights for all attention layers in the model
        Returns:
            all_attn_weights: list of attention weights for each attention layer
        """
        all_attn_weights = []
        for module in self.modules():
            if isinstance(module, Attention):
                all_attn_weights.append(module.attn_weights)
        return all_attn_weights



# HELPERS/UTILS
"""
Helper class implementations based on: https://github.com/lucidrains/perceiver-pytorch
"""


def exists(val):
    return val is not None

def default(val, d):
    return val if exists(val) else d

def cache_fn(f):
    cache = dict()
    @wraps(f)
    def cached_fn(*args, _cache = True, key = None, **kwargs):
        if not _cache:
            return f(*args, **kwargs)
        nonlocal cache
        if key in cache:
            return cache[key]
        result = f(*args, **kwargs)
        cache[key] = result
        return result
    return cached_fn

def fourier_encode(x, max_freq, num_bands = 4):
    x = x.unsqueeze(-1)
    device, dtype, orig_x = x.device, x.dtype, x

    scales = torch.linspace(1., max_freq / 2, num_bands, device = device, dtype = dtype)
    scales = scales[(*((None,) * (len(x.shape) - 1)), Ellipsis)]

    x = x * scales * pi
    x = torch.cat([x.sin(), x.cos()], dim = -1)
    x = torch.cat((x, orig_x), dim = -1)
    return x

# helper classes

class PreNorm(nn.Module):
    def __init__(self, dim, fn, context_dim = None):
        super().__init__()
        self.fn = fn
        self.norm = nn.LayerNorm(dim)
        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None

    def forward(self, x, **kwargs):
        x = self.norm(x)

        if exists(self.norm_context):
            context = kwargs['context']
            normed_context = self.norm_context(context)
            kwargs.update(context = normed_context)

        return self.fn(x, **kwargs)

class GELU(nn.Module):
    def forward(self, x):
        x, gates = x.chunk(2, dim = -1)
        return x * F.gelu(gates)

class SELU(nn.Module):
    def forward(self, x):
        x, gates = x.chunk(2, dim = -1)
        return x * F.selu(gates)

class RELU(nn.Module):
    def forward(self, x):
        x, gates = x.chunk(2, dim = -1)
        return x * F.relu(gates)


class FeedForward(nn.Module):
    def __init__(self, dim, mult = 4, dropout = 0., snn: bool = False):
        super().__init__()
        activation = SELU() if snn else GELU()
        self.net = nn.Sequential(
            nn.Linear(dim, dim * mult * 2),
            activation,
            nn.Linear(dim * mult, dim),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        return self.net(x)


def temperature_softmax(logits, temperature=1.0, dim=-1):
    """
    Temperature scaled softmax
    Args:
        logits:
        temperature:
        dim:

    Returns:
    """
    scaled_logits = logits / temperature
    return F.softmax(scaled_logits, dim=dim)



class Attention(nn.Module):
    def __init__(self, query_dim, context_dim = None, heads = 8, dim_head = 64, dropout = 0.):
        super().__init__()
        inner_dim = dim_head * heads
        context_dim = default(context_dim, query_dim)

        self.scale = dim_head ** -0.5
        self.heads = heads

        self.to_q = nn.Linear(query_dim, inner_dim, bias = False)
        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)

        self.dropout = nn.Dropout(dropout)
        # add leaky relu
        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, query_dim),
            nn.LeakyReLU(negative_slope=1e-2)
        )

        self.attn_weights = None
        # self._init_weights()

    def _init_weights(self):
    # Use He initialization for Linear layers
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
                # Initialize bias to zero if there's any
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x, context = None, mask = None):
        h = self.heads

        q = self.to_q(x)
        context = default(context, x)
        k, v = self.to_kv(context).chunk(2, dim = -1)

        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))

        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale

        if exists(mask):
            mask = rearrange(mask, 'b ... -> b (...)')
            max_neg_value = -torch.finfo(sim.dtype).max
            mask = repeat(mask, 'b j -> (b h) () j', h = h)
            sim.masked_fill_(~mask, max_neg_value)

        # attention, what we cannot get enough of
        # attn = sim.softmax(dim = -1)
        attn = temperature_softmax(sim, temperature=0.5, dim=-1)
        self.attn_weights = attn
        attn = self.dropout(attn)


        out = einsum('b i j, b j d -> b i d', attn, v)
        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)
        return self.to_out(out)
</file>

<file path="healnet/models/optimizer.py">

</file>

<file path="healnet/models/survival_loss.py">
import torch
import torch.nn as nn
import numpy as np
import torch
import torch.nn.functional as F



def nll_loss(hazards, S, Y, c, weights=None, alpha=0.4, eps=1e-7):
    """
    hazards: (n_batches, n_classes)
        The neural network output discrete survival predictions such that hazards = sigmoid(h).
    Y: (n_batches, 1)
        The true time bin index label.
    c: (n_batches, 1)
        The censoring status indicator.
    alpha: float
    eps: float
        Numerical constant; lower bound to avoid taking logs of tiny numbers.
    reduction: str
        Do we sum or average the loss function over the batches. Must be one of ['mean', 'sum']
    """
    batch_size = len(Y)
    Y = Y.view(batch_size, 1) # ground truth bin, 1,2,...,k
    c = c.view(batch_size, 1).float() #censorship status, 0 or 1
    if S is None:
        S = torch.cumprod(1 - hazards, dim=1) # surival is cumulative product of 1 - hazards
    S_padded = torch.cat([torch.ones_like(c), S], 1) #S(-1) = 0, all patients are alive from (-inf, 0) by definition
    # after padding, S(0) = S[1], S(1) = S[2], etc, h(0) = h[0]
    uncensored_loss = -(1 - c) * (torch.log(torch.gather(S_padded, 1, Y).clamp(min=eps)) + torch.log(torch.gather(hazards, 1, Y).clamp(min=eps)))
    censored_loss = - c * torch.log(torch.gather(S_padded, 1, Y+1).clamp(min=eps))
    neg_l = censored_loss + uncensored_loss
    if weights is not None:
        # Normalize weights and ensure it's a 2D tensor with the same number of rows as the input
        weights = weights / torch.sum(weights)
        weights = weights.view(1, -1).expand_as(hazards)
        # Use gather to select the weights corresponding to the target classes
        gathered_weights = torch.gather(weights, 1, Y)
        neg_l *= gathered_weights

    loss = (1-alpha) * neg_l + alpha * uncensored_loss
    loss = loss.mean()
    return loss

def nll_loss_alternative(h, y, c, alpha=0.0, eps=1e-7, reduction='mean'):
    """
    The negative log-likelihood loss function for the discrete time to event model (Zadeh and Schmid, 2020).
    Parameters
    ----------
    h: (n_batches, n_classes)
        The neural network output discrete survival predictions such that hazards = sigmoid(h).
    y: (n_batches, 1)
        The true time bin index label.
    c: (n_batches, 1)
        The censoring status indicator.
    alpha: float
    eps: float
        Numerical constant; lower bound to avoid taking logs of tiny numbers.
    reduction: str
        Do we sum or average the loss function over the batches. Must be one of ['mean', 'sum']
    """
    # print("h shape", h.shape)

    # make sure these are ints
    y = y.type(torch.int64)
    c = c.type(torch.int64)

    hazards = torch.sigmoid(h)

    S = torch.cumprod(1 - hazards, dim=1)

    S_padded = torch.cat([torch.ones_like(c), S], 1)

    s_prev = torch.gather(S_padded, dim=1, index=y).clamp(min=eps)
    h_this = torch.gather(hazards, dim=1, index=y).clamp(min=eps)
    s_this = torch.gather(S_padded, dim=1, index=y+1).clamp(min=eps)

    uncensored_loss = -(1 - c) * (torch.log(s_prev) + torch.log(h_this))
    censored_loss = - c * torch.log(s_this)


    neg_l = censored_loss + uncensored_loss
    if alpha is not None:
        loss = (1 - alpha) * neg_l + alpha * uncensored_loss

    if reduction == 'mean':
        loss = loss.mean()
    elif reduction == 'sum':
        loss = loss.sum()
    else:
        raise ValueError("Bad input for reduction: {}".format(reduction))

    return loss


class CrossEntropySurvLoss(object):
    def __init__(self, alpha=0.15):
        self.alpha = alpha

    def __call__(self, hazards, survival, y_disc, censorship, alpha=None):
        if alpha is None:
            return ce_loss(hazards, survival, y_disc, censorship, alpha=self.alpha)
        else:
            return ce_loss(hazards, survival, y_disc, censorship, alpha=alpha)

def ce_loss(hazards, survival, y_disc, c, alpha=0.4, eps=1e-7):
    """

    Args:
        hazards:
        survival (torch.Tensor): Survival
        y_disc (torch.Tensor): ground truth bin (y_disc)
        c (torch.Tensor): censorship status indicator
        alpha:
        eps:

    Returns:

    """
    batch_size = len(y_disc)
    y_disc = y_disc.view(batch_size, 1) # ground truth bin, 1,2,...,k
    c = c.view(batch_size, 1).float() #censorship status, 0 or 1
    if survival is None:
        survival = torch.cumprod(1 - hazards, dim=1) # surival is cumulative product of 1 - hazards
    S_padded = torch.cat([torch.ones_like(c), survival], 1)
    reg = -(1 - c) * (torch.log(torch.gather(S_padded, 1, y_disc) + eps) + torch.log(torch.gather(hazards, 1, y_disc).clamp(min=eps)))
    ce_l = - c * torch.log(torch.gather(survival, 1, y_disc).clamp(min=eps)) - (1 - c) * torch.log(1 - torch.gather(survival, 1, y_disc).clamp(min=eps))
    loss = (1-alpha) * ce_l + alpha * reg
    loss = loss.mean()
    return loss


class CoxPHSurvLoss(nn.Module):

    """
    CoxPHSurvLoss: Cox Proportional Hazards Loss Function. Read more about this on
    https://en.wikipedia.org/wiki/Proportional_hazards_model
    """
    def __init__(self):
        super().__init__()

    def __call__(self, hazards, survival, censorship, **kwargs):
        """

        Args:
            survival (torch.Tensor): calculated as torch.cumprod(1 - hazards, dim=1)
            censorship (torch.Tensor): censoring status indicator
            **kwargs:

        Returns:
            float: cox loss
        """
        # This calculation credit to Travers Ching https://github.com/traversc/cox-nnet
        # Cox-nnet: An artificial neural network method for prognosis prediction of high-throughput omics data
        current_batch_len = len(survival)
        R_mat = np.zeros([current_batch_len, current_batch_len], dtype=int)
        for i in range(current_batch_len):
            for j in range(current_batch_len):
                R_mat[i,j] = survival[j] >= survival[i]

        R_mat = torch.FloatTensor(R_mat).to(device)
        theta = hazards.reshape(-1)
        exp_theta = torch.exp(theta)
        loss_cox = -torch.mean((theta - torch.log(torch.sum(exp_theta*R_mat, dim=1))) * (1 - censorship))
        return loss_cox
</file>

<file path="healnet/tests/test_baselines.py">
import pytest
import torch
import einops
import torch.nn as nn
from healnet.baselines.multimodn import ResNet, MLPEncoder, ClassDecoder, PatchEncoder, MultiModN
from healnet.baselines.multimodn.better_multimodn import MultiModNModule
from typing import *


@pytest.fixture(autouse=True, scope="module")
def vars():
    b = 1
    # tabular
    # Note - dimensions always denoted as
    t_c = 1 # number of channels (1 for tabular) ; note that channels correspond to modality input/features
    t_d = 2189 # dimensions of each channel
    i_c = 100 # number of patches
    i_d = 1024 # dimensions per patch
    l_c = 256 # number of latent channels (num_latents)
    l_d = 32 # latent dims
    # latent_dim
    query = torch.randn(b, t_c, t_d)
    latent = torch.randn(b, l_c, l_d)

    tab_tensor = torch.randn(b, t_d, t_c)  # expects (b dims channels)
    img_tensor = torch.randn(b, i_c, i_d)
    return b, t_c, t_d, i_c, i_d, l_c, l_d, query, latent, tab_tensor, img_tensor


def test_multmodn_encoders(vars):
    b, t_c, t_d, i_c, i_d, l_c, l_d, query, latent, tab_tensor, img_tensor = vars

    # init state
    # only allows 1D stat
    state = nn.Parameter(torch.randn(b, l_d), requires_grad=True) #  doesn't allow channels :(

    # expects raw image
    img_tensor = torch.randn(b, 3, 255, 255) # standard resnet18 dims

    # image encoder pass
    img_enc = ResNet(
        state_size=l_d,
        )
    upd_state = img_enc(state=state, x=img_tensor)

    assert upd_state.shape == (b, l_d)

    # # patched image
    patch_tensor = torch.randn(b, i_c, i_d)
    # for patched images, it makes sense to use their RNN encoder
    patch_enc = PatchEncoder(
        state_size=l_d,
        hidden_layers=[128, 64],
        n_features=i_d,
    )

    upd_state = patch_enc(state=state, x=patch_tensor)

    assert upd_state.shape == (b, l_d)


    # omic
    omic_tensor = torch.randn(b, t_d)
    omic_enc = MLPEncoder(
        state_size=l_d,
        hidden_layers=[128, 64],
        n_features=t_d,
    )
    upd_state = omic_enc(state=state, x=omic_tensor)
    assert upd_state.shape == (b, l_d)

#
def test_multimodn_decoders(vars):
    b, t_c, t_d, i_c, i_d, l_c, l_d, query, latent, tab_tensor, img_tensor = vars

    # latent

    latent = nn.Parameter(torch.randn(b, l_d))
    # the "decoders" are actually task-specific FF classifier heads
    head = ClassDecoder(state_size=l_d,
                        activation=torch.sigmoid,
                        n_classes=4
                        )

    logits = head(latent)
    assert logits.shape == (b, 4)

def test_multimodn_task(vars):
    b, t_c, t_d, i_c, i_d, l_c, l_d, query, latent, tab_tensor, img_tensor = vars

    # ModN model spec
    tab_tensor = torch.randn(b, t_d)
    encoders = [
        MLPEncoder(state_size=l_d, hidden_layers=[128, 64], n_features=t_d),
        PatchEncoder(state_size=l_d, hidden_layers=[128, 64], n_features=i_d)
    ]
    decoders = [ClassDecoder(state_size=l_d, activation=torch.sigmoid, n_classes=4)]

    target = torch.rand((b, 4))

    model = MultiModNModule(
        state_size=l_d,
        encoders=encoders,
        decoders=decoders,
    )

    loss, logits = model(x=[tab_tensor, img_tensor], target=target)

    assert logits.shape == (b, 4)
</file>

<file path="healnet/tests/test_healnet.py">
import pytest
import einops
from healnet.models import *
import torch


@pytest.fixture(autouse=True, scope="module")
def vars():
    b = 10
    # tabular
    # Note - dimensions always denoted as
    t_c = 1 # number of channels (1 for tabular) ; note that channels correspond to modality input/features
    t_d = 2189 # tabular dimensions
    i_c = 100 # number of images
    i_w = 224 # image dims
    i_h = 224    
    # i_d = 1024 # dimensions per patch
    l_c = 256 # number of latent channels (num_latents)
    l_d = 32 # latent dims
    # latent_dim
    query = torch.randn(b, t_c, t_d)
    latent = torch.randn(b, l_c, l_d)
    return b, t_c, t_d, i_c, i_h, i_w, l_c, l_d, query, latent


def test_attention(vars):
    b, t_c, t_d, i_c, i_h, i_w, l_c, l_d, query, latent = vars
    # attention
    attention = Attention(query_dim=l_d, context_dim=t_d)
    updated_latent = attention(x=latent, context=query)

    assert updated_latent.shape == (b, l_c, l_d)



def test_healnet(vars):
    b, t_c, t_d, i_c, i_h, i_w, l_c, l_d, query, latent = vars

    tabular_data = torch.randn(b, t_c, t_d)  # tabular data uses each feature as a token
    image_data = torch.randn(b, i_h, i_w, i_c) # each channel is a token, rest positional encoded features

    # unimodal case smoke test
    m1 = HealNet(n_modalities=1,
                 channel_dims=[t_d],
                 num_spatial_axes=[1],  # 
                 out_dims=5
                 )
    logits1 = m1([tabular_data])
    assert logits1.shape == (b, 5)

    # bi-modal case
    m2 = HealNet(n_modalities=2,
                 channel_dims=[t_d, i_c],  # correct deimansion
                 num_spatial_axes=[1, 2],
                 out_dims=4
                 )
    logits2 = m2([tabular_data, image_data])
    assert logits2.shape == (b, 4) # default num_classes

    print(m2)

    # check misaligned args (1 mod but list of tensors)
    with pytest.raises(AssertionError):
        m3 = HealNet(n_modalities=1,
                     channel_dims=[t_d, i_c],  # level of attention
                     num_spatial_axes=[1, 1],
                     out_dims=4,
                     )
</file>

<file path="healnet/utils/__init__.py">
from healnet.utils.config import Config, flatten_config
from healnet.utils.train_utils import EarlyStopping, calc_reg_loss
from healnet.utils.loading import pickle_obj, unpickle

__all__ = ["Config", "flatten_config", "pickle_obj", "unpickle"]
</file>

<file path="healnet/utils/config.py">
"""
Config manager
"""

import yaml
import getpass
import os
import yaml
from typing import *
from box import Box

class CustomYamlLoader(yaml.FullLoader):
    """Add a custom constructor "!include" to the YAML loader.
    "!include" allows to read parameters in another YAML file as if it was
    the main one.
    Examples:
        To read the parameters listed in credentials.yml and assign them to
        credentials in logging.yml:
        ``credentials: !include credentials.yml``
        To call: config.credentials
    """

    def __init__(self, stream):
        self._root = os.path.split(stream.name)[0]
        super(CustomYamlLoader, self).__init__(stream)

    def include(self, node: yaml.nodes.ScalarNode) -> Box:
        """Read yaml files as Box objects and overwrite user specific files
        Example: !include model.yml, will be overwritten by model.$USER.yml
        """

        filename: str = os.path.join(self._root, self.construct_scalar(node))
        subconfig: Box = _read(filename, loader=CustomYamlLoader)
        subconfig = _overwrite_with_user_specific_file(subconfig, filename=filename)

        return subconfig


CustomYamlLoader.add_constructor("!include", CustomYamlLoader.include)


class Config:
    def __init__(self, config_path: str):
        self._config_path = config_path

    def read(self) -> Box:
        """Reads main config file"""
        if os.path.isfile(self._config_path) and os.access(self._config_path, os.R_OK):
            config = _read(filename=self._config_path, loader=CustomYamlLoader)
            config = _overwrite_with_user_specific_file(
                config, filename=self._config_path
            )
            return config
        else:
            raise FileNotFoundError(self._config_path)

def _user_specific_file(filename: str) -> Union[None, str]:
    """Find user specific files for a filename.
    E.g. user_specific_file(config.yml) = config.$USER.yml if the file
    exists, else returns None
    """
    username = getpass.getuser().lower().replace(" ", "_")
    filepath, file_extension = os.path.splitext(filename)
    user_filename = filepath + "." + username + file_extension
    if os.path.isfile(user_filename) and os.access(user_filename, os.R_OK):
        user_filename = user_filename
    else:
        user_filename = None
    return user_filename


def _read(filename: str, loader) -> Box:
    """Read any yaml file as a Box object"""

    if os.path.isfile(filename) and os.access(filename, os.R_OK):
        with open(filename, "r") as f:
            try:
                config_dict = yaml.load(f, Loader=loader)
            except yaml.YAMLError as exc:
                print(exc)
        return Box(config_dict)
    else:
        raise FileNotFoundError(filename)


def _overwrite_with_user_specific_file(config: Box, filename: str) -> Box:
    """Overwrite the config files with user specific files """
    user_filename = _user_specific_file(filename)
    if user_filename:
        print(f"{filename} overwritten by {user_filename}")
        user_config: Box = _read(user_filename, loader=CustomYamlLoader)
        config.merge_update(user_config)

    return config



def flatten_config(dictionary, parent_key='', sep='.'):
    """
    Flatten a nested dictionary - this is required to easily update the regular config file
    with the wandb config
    Args:
        dictionary:
        parent_key:
        sep:

    Returns:
        Box: Python box object with flattened config. Elements that were previously callable via ['key']['subkey']
            are now callable via ['key.subkey']

    """
    flattened_dict = {}
    for key, value in dictionary.items():
        new_key = f"{parent_key}{sep}{key}" if parent_key else key
        if isinstance(value, dict):
            flattened_dict.update(flatten_config(value, parent_key=new_key, sep=sep))
        else:
            flattened_dict[new_key] = value
    return Box(flattened_dict)
</file>

<file path="healnet/utils/loading.py">
import pickle


def pickle_obj(obj, path):
    with open(path, 'wb') as f:
        pickle.dump(obj, f)

def unpickle(path):
    with open(path, 'rb') as f:
        obj = pickle.load(f)
    return obj
</file>

<file path="healnet/utils/train_utils.py">
from typing import *
import torch
import torch.nn as nn

def calc_reg_loss(model, l1: float, model_topo: str, sources: List[str]):

    if model_topo == "fcnn": # don't regularise FCNN
        reg_loss = 0
    elif model_topo == "mcat" and sources == ["omic"]:
        reg_loss = 0
    else:
        l1_norm = sum(p.abs().sum() for p in model.parameters())
        reg_loss = float(l1) * l1_norm
    return reg_loss


def count_parameters(model: nn.Module) -> int:
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

class EarlyStopping:
    def __init__(self, patience=5, verbose=False, mode='min'):
        """
        Constructor for early stopping.

        Parameters:
        - patience (int): How many epochs to wait before stopping once performance stops improving.
        - verbose (bool): If True, prints out a message for each validation metric improvement.
        - mode (str): One of ['min', 'max']. Minimize (e.g., loss) or maximize (e.g., accuracy) the metric.
        """
        assert mode in ['min', 'max'], "Mode must be 'min' or 'max'"
        self.patience = patience
        self.verbose = verbose
        self.counter = 0

        if mode == 'min':
            self.best_metric = float('inf')
            self.operator = torch.lt
        else:
            self.best_metric = float('-inf')
            self.operator = torch.gt

        self.best_model_weights = None
        self.should_stop = False

    def step(self, metric, model):
        """
        Check the early stopping conditions.

        Parameters:
        - metric (float): The latest validation metric (loss, accuracy, etc.).
        - model (torch.nn.Module): The model being trained.

        Returns:
        - bool: True if early stopping conditions met, False otherwise.
        """
        if type(metric) == float: # convert to tensor if necessary
            metric = torch.tensor(metric)

        if self.operator(metric, self.best_metric):
            if self.verbose:
                print(f"Validation metric improved from {self.best_metric:.4f} to {metric:.4f}. Saving model weights.")
            self.best_metric = metric
            self.counter = 0
            self.best_model_weights = model.state_dict().copy()
        else:
            self.counter += 1
            if self.verbose:
                print(f"Validation metric did not improve. Patience: {self.counter}/{self.patience}.")
            if self.counter >= self.patience:
                self.should_stop = True

        return self.should_stop

    def load_best_weights(self, model):
        """
        Load the best model weights.

        Parameters:
        - model (torch.nn.Module): The model to which the best weights should be loaded.
        """
        if self.verbose:
            print(f"Loading best model weights with validation metric value: {self.best_metric:.4f}")
        model.load_state_dict(self.best_model_weights)
        return model
</file>

<file path="healnet/utils/wb.py">
"""
Configuring weights and biases tracking/setup
"""
import wandb
from typing import *
import random

def wb_tracking(config: Dict) -> None:

    # start a new wandb run to track this script
    wandb.init(
        # set the wandb project where this run will be logged
        project="healnet",

        # track hyperparameters and run metadata
        config=config
        # {
        # "learning_rate": 0.02,
        # "architecture": "CNN",
        # "dataset": "CIFAR-100",
        # "epochs": 10,
        # }
    )
</file>

<file path="healnet/__init__.py">
from .models import HealNet
</file>

<file path="healnet/fusion.py">
import torch
from torch import nn

class LateFusion(nn.Module):

    def __init__(self, image_classifier: nn.Module, tabular_classifier: nn.Module, num_classes: int):
        super().__init__()
        self.image_classifier = image_classifier
        self.tabular_classifier = tabular_classifier
        # fully-connected fusion layer
        self.fc = nn.Linear(
            in_features=image_classifier.fc.out_features + tabular_classifier.fc.out_features,
            out_features=num_classes
        )

    def forward(self, images, tabular_data):
        image_output = self.image_classifier(images)
        tabular_output = self.tabular_classifier(tabular_data)
        combined_output = torch.cat((image_output, tabular_output), dim=1)
        return self.fc(combined_output)


class EarlyFusion(nn.Module):
    def __init__(self, num_classes):
        super().__init__()


    def forward(self, data):
        # need to pass in MMDataSet object
        pass
</file>

<file path="healnet/main.py">
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import KFold, ParameterGrid
import multiprocessing
import argparse
from argparse import Namespace
import yaml
from tqdm import tqdm

from healnet.utils import EarlyStopping, calc_reg_loss, pickle_obj
from healnet.models.survival_loss import CrossEntropySurvLoss, CoxPHSurvLoss, nll_loss
from healnet.baselines import RegularizedFCNN, MMPrognosis, MCAT, SNN, MILAttentionNet, MultiModNModule
from healnet.baselines.multimodn import MLPEncoder, PatchEncoder, ClassDecoder
from healnet.models import HealNet
from healnet.utils import Config, flatten_config
from healnet.etl import TCGADataset

from torch.utils.data import Dataset, DataLoader
import numpy as np
from sksurv.metrics import concordance_index_censored
from torch import optim
import pandas as pd
from box import Box
from pathlib import Path
from datetime import datetime
pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 50)
import wandb



class Pipeline:
    """
    Main experimental pipeline class for training and evaluating models, config handling, and logging
    """

    def __init__(self, config: Box, args: Namespace, wandb_name: str=None):
        self.config = flatten_config(config)
        self.dataset = self.config.dataset
        self.args = args
        self.log_dir = None
        self._check_config()
        self.wandb_name = wandb_name
        self.output_dims = int(self.config[f"model_params.output_dims"])
        self.sources = self.config.sources
        # create log directory for run
        # date
        self.local_run_id = datetime.now().strftime("%d-%m-%Y_%H-%M-%S")
        # initialise cuda device (will load directly to GPU if available)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if self.device == "cuda":
            print(f"Setting default cuda tensor to double")
            torch.set_default_tensor_type(torch.cuda.DoubleTensor)

        # set up wandb logging
        self.wandb_setup()

        if self.config.explainer:
            self.log_dir = Path(self.config.log_path).joinpath(f"{wandb.run.name}")
            self.log_dir.mkdir(parents=True, exist_ok=True)


    def wandb_setup(self) -> None:

        if self.args.mode == "sweep":
            with open(self.args.sweep_config, "r") as f:
                sweep_config = yaml.safe_load(f)

            sweep_id = wandb.sweep(sweep=sweep_config, project="healnet")
            wandb.agent(sweep_id, function=self.main)
        else:
            wandb_config = dict(self.config)
            wandb.init(project="healnet", name=self.wandb_name, config=wandb_config, resume=True)
        return None


    def _check_config(self) -> None:
        """
        Assert that the config only contains valid arguments
        Returns:
            None
        """
        valid_sources = ["omic", "slides"]
        assert all([source in valid_sources for source in self.config.sources]), f"Invalid source specified. Valid sources are {valid_sources}"

        valid_survival_losses = ["nll", "ce_survival", "cox"]
        assert self.config["survival.loss"] in valid_survival_losses, f"Invalid survival loss specified. " \
                                                                   f"Valid losses are {valid_survival_losses}"

        valid_datasets = ["blca", "brca", "kirp", "ucec", "hnsc", "paad", "luad", "lusc"]
        assert self.config.dataset in valid_datasets, f"Invalid dataset specified. Valid datasets are {valid_datasets}"

        # # check that model parameters are specified
        # assert self.config.dataset in self.config.model_params.keys(), f"Model parameters not specified for dataset {self.config.dataset}"

        valid_models = ["healnet", "fcnn", "healnet_early", "mcat", "mm_prognosis", "multimodn"]
        assert self.config.model in valid_models, f"Invalid model specified. Valid models are {valid_models}"

        valid_class_weights = ["inverse", "inverse_root", "None"]
        assert self.config[f"model_params.class_weights"] in valid_class_weights, f"Invalid class weight specified. " \
                                                                                f"Valid weights are {valid_class_weights}"

        return None


    def main(self):

        # Initialise wandb run (do here for sweep)
        if self.args.mode == "sweep":
            # update config with sweep config
            wandb.init(project="healnet", name=None, resume=True) # init sweep run
            for key, value in wandb.config.items():
                if key in self.config.keys():
                    self.config[key] = value



        train_c_indeces, val_c_indeces, test_c_indeces = [], [], []
        # test_dataloaders = []
        test_data_indices = []
        missing_perfs = []
        models = []
        for fold in range(1, self.config["n_folds"]+1):
            print(f"*****FOLD {fold}*****")
            # fix random seeds for reproducibility
            torch.manual_seed(fold)
            np.random.seed(fold)

            train_data, val_data, test_data = self.load_data(fold=fold)
            # get test data indices
            test_data_indices.append(test_data.dataset.indices)
            # test_dataloaders.append(test_data)
            model = self.make_model(train_data)
            wandb.watch(model)
            model, _, train_c_index, _, val_c_index, _, test_c_index, missing_performance = self.train_survival_fold(model, train_data, val_data, test_data, fold=fold)
            train_c_indeces.append(train_c_index)
            val_c_indeces.append(val_c_index)
            test_c_indeces.append(test_c_index)
            missing_perfs.append(missing_performance)
            models.append(model)

        # log average and standard deviation across folds
        wandb.log({"mean_train_c_index": np.mean(train_c_indeces),
                   "mean_val_c_index": np.mean(val_c_indeces),
                   "std_train_c_index": np.std(train_c_indeces),
                    "std_val_c_index": np.std(val_c_indeces),
                    "mean_test_c_index": np.mean(test_c_indeces),
                    "std_test_c_index": np.std(test_c_indeces)})


        best_fold = np.argmax(test_c_indeces)
        best_model = models[best_fold]

        # if missing, log also that
        if self.config.missing_ablation:
            missing_50_c_index, missing_omic_c_index, missing_wsi_c_index = np.mean(missing_perfs, axis=0)
            wandb.log({"missing_50_c_index": missing_50_c_index,
                       "missing_omic_c_index": missing_omic_c_index,
                       "missing_wsi_c_index": missing_wsi_c_index})


        if self.config.explainer:
            torch.save(best_model.state_dict(), self.log_dir.joinpath("best_model.pt"))
            # save config
            pickle_obj(self.config, self.log_dir.joinpath("config.pkl"))
            # save test data indices
            pickle_obj(test_data_indices[best_fold], self.log_dir.joinpath("test_data_indices.pkl"))

        wandb.finish()

    def load_data(self, fold: int = None) -> tuple:

        level_dict = {
            "blca": 2,
            "brca": 2,
            "kirp": 2,
            "ucec": 2,
            "hnsc": 2,
            "paad": 2,
            "luad": 2,
            "lusc": 2
        }


        data = TCGADataset(self.config["dataset"],
                           self.config,
                           level=int(self.config["data.wsi_level"]),
                           survival_analysis=True,
                           sources=self.sources,
                           n_bins=self.output_dims,
                           log_dir=self.log_dir,
                           )
        train_size = 0.7
        test_size = 0.15
        val_size = 0.15

        print(f"Train samples: {int(train_size*len(data))}, Val samples: {int(val_size * len(data))}, "
              f"Test samples: {int(test_size * len(data))}")
        train, test, val = torch.utils.data.random_split(data, [train_size, test_size, val_size])

        target_distribution = lambda idx, data: dict(np.round(data.omic_df.iloc[idx]["y_disc"].value_counts().sort_values() / len(idx), 2))

        print(f"Train distribution: {target_distribution(train.indices, data)}")
        print(f"Val distribution: {target_distribution(val.indices, data)}")
        print(f"Test distribution: {target_distribution(test.indices, data)}")

        # calculate class weights
        if self.config[f"model_params.class_weights"] == "None":
            self.class_weights = None
        else:
            self.class_weights = torch.Tensor(self._calc_class_weights(train)).to(self.device)

        train_data = DataLoader(train,
                                batch_size=self.config["train_loop.batch_size"],
                                shuffle=True,
                                # num_workers=int(multiprocessing.cpu_count()),
                                num_workers=8,
                                pin_memory=True,
                                multiprocessing_context=MP_CONTEXT,
                                persistent_workers=True,
                                prefetch_factor=2
                                )
        val_data = DataLoader(val,
                             batch_size=self.config["train_loop.batch_size"],
                             shuffle=False,
                             num_workers=int(multiprocessing.cpu_count()),
                             pin_memory=True,
                             multiprocessing_context=MP_CONTEXT,
                             persistent_workers=True,
                             prefetch_factor=2)

        test_data = DataLoader(test,
                               batch_size=self.config["train_loop.batch_size"],
                               shuffle=False,
                               num_workers=int(multiprocessing.cpu_count()),
                               pin_memory=True,
                               multiprocessing_context=MP_CONTEXT,
                               persistent_workers=True,
                               prefetch_factor=2)




        return train_data, val_data, test_data

    def _calc_class_weights(self, train):

        # if self.config.model in ["healnet", "healnet_early"]:
        if self.config[f"model_params.class_weights"] in ["inverse", "inverse_root"]:
            train_targets = np.array(train.dataset.y_disc)[train.indices]
            _, counts = np.unique(train_targets, return_counts=True)
            if self.config[f"model_params.class_weights"] == "inverse":
                class_weights = 1. / counts
            elif self.config[f"model_params.class_weights"] == "inverse_root":
                class_weights = 1. / np.sqrt(counts)
        else:
            class_weights = None
        return class_weights

    def make_model(self, train_data: DataLoader):
        """
        Instantiates model and moves to CUDA device if available
        Args:
            train_data:

        Returns:
            nn.Module: model used for training
        """
        feat, _, _, _ = next(iter(train_data))
        if self.config.model in  ["healnet", "healnet_early"]:

            num_sources = len(self.config["sources"])
            if num_sources == 1:
                input_channels = [feat[0].shape[2]]
                input_axes = [1]
                modalities = 1
            elif num_sources == 2 and self.config.model == "healnet":
                input_channels = [feat[0].shape[2], feat[1].shape[2]]
                input_axes = [1, 1] # one axis (MIL and tabular)
                modalities = 2

            # early fusion healnet (concatenation, so just one modality)
            elif num_sources == 2 and self.config.model == "healnet_early":
                modalities = 1 # same model just single modality
                input_channels = [feat[0].shape[2]]
                input_axes = [1]
            model = HealNet(
                n_modalities=modalities,
                channel_dims=input_channels, # number of features as input channels
                num_spatial_axes=input_axes, # second axis (b n_feats channels)
                out_dims=self.output_dims,
                num_freq_bands=self.config[f"model_params.num_freq_bands"],
                depth=self.config[f"model_params.depth"],
                max_freq=self.config[f"model_params.max_freq"],
                l_c = self.config[f"model_params.num_latents"],
                l_d = self.config[f"model_params.latent_dim"],
                cross_dim_head = self.config[f"model_params.cross_dim_head"],
                latent_dim_head = self.config[f"model_params.latent_dim_head"],
                x_heads = self.config[f"model_params.cross_heads"],
                l_heads = self.config[f"model_params.latent_heads"],
                attn_dropout = self.config[f"model_params.attn_dropout"],
                ff_dropout = self.config[f"model_params.ff_dropout"],
                weight_tie_layers = self.config[f"model_params.weight_tie_layers"],
                fourier_encode_data = self.config[f"model_params.fourier_encode_data"],
                self_per_cross_attn = self.config[f"model_params.self_per_cross_attn"],
                final_classifier_head = True,
                snn = self.config[f"model_params.snn"],
            )
            model.float()
            model.to(self.device)

        elif self.config.model == "fcnn":
            model = RegularizedFCNN(output_dim=self.output_dims)
            model.to(self.device)

        elif self.config.model == "multimodn":
            l_d = 2000
            tab_features = feat[0].shape[1]
            patch_dims = feat[1].shape[2]
            encoders = [
                MLPEncoder(state_size=l_d, hidden_layers=[1024, 256, 128, 64], n_features=tab_features),
                PatchEncoder(state_size=l_d, hidden_layers=[512, 256, 128, 64], n_features=patch_dims)
            ]
            decoders = [ClassDecoder(state_size=l_d, n_classes=self.output_dims, activation=torch.sigmoid)]


            model = MultiModNModule(
                state_size=l_d,
                encoders=encoders,
                decoders=decoders
            )
            model.float()
            model.to(self.device)

        elif self.config.model == "mm_prognosis":
            if len(self.config["sources"]) == 1:
                input_dim = feat[0].shape[1]
                # input_dim = feat[0].shape[2] + feat[1].shape[2]
            model = MMPrognosis(sources=self.sources,
                                output_dims=self.output_dims,
                                config=self.config
                                )
            model.float()
            model.to(self.device)

        elif self.config.model == "mcat":
            if len(self.config["sources"]) == 2:
                model = MCAT(
                    n_classes=self.output_dims,
                    omic_shape=feat[0].shape[1:],
                    wsi_shape=feat[1].shape[1:]
                )
            elif self.config["sources"][0] == "omic":
                model = SNN(
                    n_classes=self.output_dims,
                    input_dim=feat[0].shape[1]
                )
            elif self.config["sources"][0] == "slides":
                model = MILAttentionNet(
                    input_dim=feat[0].shape[1:],
                    n_classes=self.output_dims
                )
            model.float()
            model.to(self.device)

        return model


    def train_survival_fold(self,
                            model: nn.Module,
                            train_data: DataLoader,
                            test_data: DataLoader,
                            val_data: DataLoader,
                            fold: int,
                            gc: int = 16,
                            **kwargs):
        """
        Trains model for survival analysis
        Args:
            model (nn.Module): model to train
            train_data (DataLoader): training data
            test_data (DataLoader): test data
            **kwargs:

        Returns:
            Tuple: tuple of the model and all performance metrics for given fold
        """
        print(f"Training survival model using {self.config.model}")
        optimizer = optim.Adam(model.parameters(), lr=self.config["optimizer.lr"])
        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,
                                                  max_lr=self.config["optimizer.max_lr"],
                                                  epochs=self.config["train_loop.epochs"],
                                                  steps_per_epoch=len(train_data))


        early_stopping = EarlyStopping(patience=self.config["train_loop.patience"],
                                       mode="min", verbose=True)
                                       # delta=self.config["train_loop.delta"],
                                       # maximize=True) # val_c_index as stopping criterion

        model.train()


        for epoch in range(1, self.config["train_loop.epochs"]+1):
            print(f"Epoch {epoch}")
            risk_scores = []
            censorships = []
            event_times = []
            train_loss_surv, train_loss = 0.0, 0.0 # train_loss includes regularisation, train_loss_surve doesn't and is used for logging
            grad_norms = []

            for batch, (features, censorship, event_time, y_disc) in enumerate(tqdm(train_data)):
                # only move to GPU now (use CPU for preprocessing)
                features = [feat.to(self.device) for feat in features] # features available for patient
                censorship = censorship.to(self.device) # status 0 or 1
                event_time = event_time.to(self.device) # survival months (continuous)
                y_disc = y_disc.to(self.device) # discretized survival time bucket

                if batch == 0 and epoch == 1: # print model summary
                    print(f"Modality shapes: ")
                    [print(feat.shape) for feat in features]
                    print(f"Modality dtypes:")
                    [print(feat.dtype) for feat in features]

                optimizer.zero_grad()
                # forward + backward + optimize
                if self.config["model"] == "multimodn":
                    # note that we need to pass the target here for the intermediate loss calc
                    model_loss, logits = model.forward(features, F.one_hot(y_disc, num_classes=self.output_dims))
                else:
                    logits = model.forward(features)
                    model_loss = 0.0
                y_hat = torch.topk(logits, k=1, dim=1)[1]
                hazards = torch.sigmoid(logits)  # sigmoid to get hazards from predictions for surv analysis
                survival = torch.cumprod(1-hazards, dim=1)  # as per paper, survival = cumprod(1-hazards)
                risk = -torch.sum(survival, dim=1).detach().cpu().numpy()  # risk = -sum(survival)

                if self.config["survival.loss"] == "nll":
                    # loss_fn = NLLSurvLoss()
                    # loss = loss_fn(h=hazards, y=y_disc, c=censorship)
                    surv_loss = nll_loss(hazards=hazards, S=survival, Y=y_disc, c=censorship, weights=self.class_weights)
                elif self.config["survival.loss"] == "ce_survival":
                    loss_fn = CrossEntropySurvLoss()
                    surv_loss = loss_fn(hazards=hazards, survival=survival, y_disc=y_disc, censorship=censorship)
                elif self.config["survival.loss"] == "cox":
                    loss_fn = CoxPHSurvLoss()
                    loss_fn(hazards=hazards, survival=survival, censorship=censorship)


                dataset = self.config.dataset
                reg_loss = calc_reg_loss(model, self.config[f"model_params.l1"], self.config.model, self.config.sources)

                # log risk, censorship and event time for concordance index
                risk_scores.append(risk)
                censorships.append(censorship.detach().cpu().numpy())
                event_times.append(event_time.detach().cpu().numpy())

                loss_value = surv_loss.item()
                train_loss_surv += loss_value
                train_loss += loss_value + reg_loss
                # backward pass
                surv_loss = surv_loss / gc + reg_loss + model_loss  # gradient accumulation step
                surv_loss.backward()
                optimizer.step()
                optimizer.zero_grad()
                scheduler.step()

            train_loss /= len(train_data)
            train_loss_surv /= len(train_data)

            risk_scores_full = np.concatenate(risk_scores)
            censorships_full = np.concatenate(censorships)
            event_times_full = np.concatenate(event_times)


            # calculate epoch-level concordance index
            train_c_index = concordance_index_censored((1-censorships_full).astype(bool), event_times_full, risk_scores_full, tied_tol=1e-08)[0]
            wandb.log({f"fold_{fold}_train_loss": train_loss_surv, f"fold_{fold}_train_c_index": train_c_index}, step=epoch if fold == 1 else None)
            print('Epoch: {}, train_loss: {:.4f}, train_c_index: {:.4f}'.format(epoch, train_loss_surv, train_c_index))



            # evaluate at interval or if final epoch
            # if epoch % self.config["train_loop.eval_interval"] == 0 or epoch == self.config["train_loop.epochs"]:
            print(f"Running validation")
            val_loss, val_c_index = self.evaluate_survival_epoch(epoch, model, val_data)

            print('Epoch: {}, val_loss: {:.4f}, val_c_index: {:.4f}'.format(epoch, val_loss, val_c_index))
            wandb.log({f"fold_{fold}_val_loss": val_loss, f"fold_{fold}_val_c_index": val_c_index}, step=epoch if fold == 1 else None)

            if self.config["train_loop.early_stopping"] and early_stopping.step(val_loss, model):
                print(f"Early stopping at epoch {epoch}")
                model = early_stopping.load_best_weights(model)
                break

        # once stopped and best model is loaded, evaluate on test set
        print(f"Running test set evaluation")
        test_loss, test_c_index = self.evaluate_survival_epoch(epoch, model, test_data)
        print('Epoch: {}, test_loss: {:.4f}, test_c_index: {:.4f}'.format(epoch, test_loss, test_c_index))
        wandb.log({f"fold_{fold}_test_loss": test_loss, f"fold_{fold}_test_c_index": test_c_index}, step=epoch if fold == 1 else None)

        # run ablation
        missing_performance = None
        if self.config.missing_ablation:
            _, missing_50_c_index = self.evaluate_survival_epoch(epoch=self.config["train_loop.epochs"],
                                                                               model=model,
                                                                               test_data=test_data,
                                                                               missing_mode="50")
            _, missing_omic_c_index = self.evaluate_survival_epoch(epoch=self.config["train_loop.epochs"],
                                                                               model=model,
                                                                               test_data=test_data,
                                                                               missing_mode="omic")
            _, missing_wsi_c_index = self.evaluate_survival_epoch(epoch=self.config["train_loop.epochs"],
                                                                               model=model,
                                                                               test_data=test_data,
                                                                               missing_mode="wsi")

            missing_performance = (missing_50_c_index, missing_omic_c_index, missing_wsi_c_index)



        # return values of final epoch
        return model, train_loss, train_c_index, val_loss, val_c_index, test_loss, test_c_index, missing_performance

    def _sample_missing(self, features, use_omic, mode):
        assert mode in ["50", "omic", "wsi"], "Invalid missing ablation mode"

        if mode == "50":
            if use_omic:
                use_omic = False
                return [features[0]], use_omic
            else:
                use_omic = True
                return [features[1]], use_omic
        elif mode == "omic":
            # return only WSIs
            return [features[1]], None
        elif mode == "wsi":
            # return only omic
            return [features[0]], None


    def evaluate_survival_epoch(self,
                                epoch: int,
                                model: nn.Module,
                                test_data: DataLoader,
                                missing_mode: str=None,
                                # loss_reg: float=0.0,
                                **kwargs):

        model.eval()
        risk_scores = []
        censorships = []
        event_times = []
        predictions = []
        labels = []
        val_loss_surv, val_loss = 0.0, 0.0
        use_omic = True

        for batch, (features, censorship, event_time, y_disc) in enumerate(tqdm(test_data)):
            # only move to GPU now (use CPU for preprocessing)
            if missing_mode is not None: # handle for missing modality ablation
                features, use_omic = self._sample_missing(features, use_omic, missing_mode)
            features = [feat.to(self.device) for feat in features]
            censorship = censorship.to(self.device)
            event_time = event_time.to(self.device)
            y_disc = y_disc.to(self.device)

            if self.config["model"] == "multimodn":
                model_loss, logits = model.forward(features, F.one_hot(y_disc, num_classes=self.output_dims))
            else:
                logits = model.forward(features)
                model_loss = 0.0
            hazards = torch.sigmoid(logits)
            survival = torch.cumprod(1-hazards, dim=1)
            risk = -torch.sum(survival, dim=1).detach().cpu().numpy()

            if self.config["survival.loss"] == "nll":
                # loss_fn = NLLSurvLoss()
                # loss = loss_fn(h=hazards, y=y_disc, c=censorship)
                loss = nll_loss(hazards=hazards, S=survival, Y=y_disc, c=censorship, weights=self.class_weights)
            elif self.config["survival.loss"] == "ce_survival":
                loss_fn = CrossEntropySurvLoss()
                loss = loss_fn(hazards=hazards, survival=survival, y_disc=y_disc, censorship=censorship)
            elif self.config["survival.loss"] == "cox":
                loss_fn = CoxPHSurvLoss()
                loss_fn(hazards=hazards, survival=survival, censorship=censorship)

            reg_loss = calc_reg_loss(model, self.config[f"model_params.l1"], self.config.model, self.config.sources)

            # log risk, censorship and event time for concordance index
            risk_scores.append(risk)
            censorships.append(censorship.detach().cpu().numpy())
            event_times.append(event_time.detach().cpu().numpy())

            loss_value = loss.item()
            val_loss_surv += loss_value
            val_loss += loss_value + reg_loss + model_loss

            predictions.append(logits.argmax(1).cpu().tolist())
            labels.append(y_disc.detach().cpu().tolist())

        # calculate epoch-level stats
        predictions = np.concatenate(predictions)
        labels = np.concatenate(labels)

        val_loss_surv /= len(test_data)
        val_loss /= len(test_data)

        risk_scores_full = np.concatenate(risk_scores)
        censorships_full = np.concatenate(censorships)
        event_times_full = np.concatenate(event_times)

        # calculate epoch-level concordance index
        val_c_index = concordance_index_censored((1-censorships_full).astype(bool), event_times_full, risk_scores_full)[0]

        model.train()
        # return unregularised loss for logging
        return val_loss_surv, val_c_index

    def calc_gradient_norm(self, model):
        total_norm = 0
        for p in model.parameters():
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
        total_norm = total_norm ** (1. / 2)
        return total_norm



if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="Run main training pipeline of healnet")

    # assumes execution
    parser.add_argument("--config_path", type=str, default="config/main_gpu.yml", help="Path to config file")
    parser.add_argument("--mode", type=str, default="single_run", choices=["single_run", "sweep", "run_plan", "reg_ablation"])
    parser.add_argument("--sweep_config", type=str, default="config/sweep_bayesian.yaml", help="Hyperparameter sweep configuration")
    parser.add_argument("--dataset", type=str, default=None, help="Dataset for run plan")
    parser.add_argument("--datasets", type=list, default=["blca", "brca", "ucec", "kirp"], help="Datasets for run plan")

    # call config
    args = parser.parse_args()
    MP_CONTEXT = "fork"
    # set up multiprocessing context for PyTorch
    torch.multiprocessing.set_start_method(MP_CONTEXT)
    config_path = args.config_path
    config = Config(config_path).read()
    if args.dataset is not None: # for command line sweeps
        config["dataset"] = args.dataset
    # get best hyperparameters for datKaset
    hyperparams = Config(config["hyperparams"]).read()[config.dataset]
    config["model_params"] = hyperparams

    if args.mode == "run_plan":
        if args.dataset is not None:
            datasets = [args.dataset]
        else:
            datasets = args.datasets

        grid = ParameterGrid(
            {"dataset": datasets,
             "sources": [["omic", "slides"]],
             "model": ["healnet"]
             })

        n_folds = 5

        for iteration, params in enumerate(grid):
            dataset, sources, model = params["dataset"], params["sources"], params["model"]
            print(f"Run plan iteration {iteration+1}/{len(grid)}")
            print(f"Dataset: {dataset}, Sources: {sources}, Model: {model}")

            # skip healnet_early on single modality (same as regular healnet)
            if model == "healnet_early" and len(sources) == 1:
                continue
            config["dataset"] = dataset
            config["sources"] = sources
            config["model"] = model
            config["n_folds"] = n_folds
            try:
                pipeline = Pipeline(
                        config=config,
                        args=args,
                    )
                pipeline.main()
            except Exception as e:
                print(f"Exception: {e}")
                continue

        print(f"Successfully finished runplan: "
              f"{list(grid)}")

    elif args.mode == "reg_ablation":
        config["sources"] = ["omic", "slides"]
        config["model"] = "healnet"
        config["n_folds"] = 1
        config["train_loop.early_stopping"] = False
        config["train_loop.epochs"] = 50
        regs  = [2.0,1.0]
        snn = [True, False] # False
        sets = ["blca", "brca", "ucec", "kirp"]

        for dataset in sets:
            config["dataset"] = dataset
            best_reg = config["model_params.l1"]
            for reg in regs:
                config["model_params.l1"] = best_reg / reg
                for s in snn:
                    config["model_params.l1"] = reg
                    config["model_params.snn"] = s
                    pipeline = Pipeline(
                            config=config,
                            args=args,
                        )
                    pipeline.main()


    else: # single_run or sweep
        pipeline = Pipeline(
                    config=config,
                    args=args,
                )
        pipeline.main()
</file>

<file path="healnet/train_utils.py">

</file>

<file path="healnet/train.py">
# Implement train loop
import torch
import torch.optim as optim
import torch.nn as nn
from typing import *
from tqdm.notebook import tqdm
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
from sklearn.dummy import DummyClassifier
import numpy as np


def train_loop(
    preprocess_fn: Callable,
    model: nn.Module,
    trainloader: torch.utils.data.DataLoader,
    testloader: torch.utils.data.DataLoader,
    epochs: int=1,
    verbose:bool=True,
    max_lr: float = 0.05,
    lr: float = 0.005,
    momentum=0.9,
    **kwargs
) -> None:

    n_batches = len(trainloader)
    majority_val_acc = majority_classifier_acc(y_true=testloader.dataset.targets)
    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)
    # set efficient OneCycle scheduler, significantly reduces required training iters
    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, max_lr=max_lr, epochs=epochs, steps_per_epoch=n_batches)

    criterion = nn.CrossEntropyLoss()
    logs_per_epoch = 10

    if n_batches < logs_per_epoch:
        logs_per_epoch = n_batches

    model.train()
    for epoch in range(epochs):  # loop over the dataset multiple times

        running_loss = 0.0
        for i, data in enumerate(tqdm(trainloader)):

            inputs, labels = data
            if preprocess_fn is not None:
                inputs = preprocess_fn(inputs)
            # zero the parameter gradients
            optimizer.zero_grad()

            # forward + backward + optimize
            outputs = model.forward(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            scheduler.step()
            # print statistics
            running_loss += loss.item()

            if verbose and i % (int(n_batches/logs_per_epoch)) == 0:
                print(f'[Epoch {epoch + 1}, Batch {i + 1:5d}] train_loss: {running_loss / 2000:.3f}')

            running_loss = 0.0
    print('Finished Training')
    accuracy, f1, precision, recall = validate(model, testloader, majority_val_acc)


def validate(model: nn.Module, data_loader: torch.utils.data.DataLoader, majority_val_acc: float):

    print("Running validation...")
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    with torch.no_grad():
        y_true = []
        y_pred = []
        for batch in data_loader:
            inputs, labels = batch
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            y_true.extend(labels.to(device).numpy())
            y_pred.extend(predicted.to(device).numpy())

        accuracy = accuracy_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred, average='weighted')
        precision = precision_score(y_true, y_pred, average='weighted')
        recall = recall_score(y_true, y_pred, average='weighted')
        # auc = roc_auc_score(y_true, y_pred, average="macro", multi_class="ovr")

    print(f"n_classes: {len(set(y_true))}")
    print(f"accuracy: {accuracy:.4f}, f1: {f1:.4f}, precision: {precision:.4f}, recall: {recall:.4f}, "
          f"majority_val_acc: {majority_val_acc:.4f}")

    # resume train mode
    model.train()
    return accuracy, f1, precision, recall

def majority_classifier_acc(y_true: List):
    """
    Returns accuracy of majority class classifier
    """
    X = np.ones(len(y_true))
    dummy = DummyClassifier(strategy="most_frequent").fit(X=X, y=y_true)
    y_majority = dummy.predict(X)
    return accuracy_score(y_true, y_majority)
</file>

<file path="healnet/utils.py">

</file>

<file path="tutorial/01_Getting_Started.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae2c8fd3734c812",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "787ff0241bf48627",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from healnet.models import HealNet\n",
    "from healnet.etl import MMDataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from typing import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de637307d506881",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Synthetic data example\n",
    "\n",
    "To illustrate how HEALNet can be used in any pipeline, we create three synthetic modalities, i.e., three possible modalities: \n",
    "\n",
    "* Tabular data: `(1, 2000)`\n",
    "    * Table with with 2000 features `tab_d`. For 1D modalities, we add a channel dimension with `tab_c=1`\n",
    "* 2D Image: `(224, 224, 3)`\n",
    "    * Image corresponding to height, width, and colour channel. \n",
    "* 3D Image: `(12, 224, 224, 4)`\n",
    "    * Image dims corresponding to depth, height, weight, and colour channel\n",
    "* Target: `(n, )`\n",
    "    * Asusming `n` observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "233819ff1c11e04a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 100 # number of samples\n",
    "b = 4 # batch size\n",
    "\n",
    "# latent channels x dims\n",
    "l_c = 16\n",
    "l_d = 16\n",
    "\n",
    "# 2D image\n",
    "img_c = 3 # image channels\n",
    "h = 224 # image height\n",
    "w = 224 # image width\n",
    "# 3D image\n",
    "d = 12 # depth\n",
    "\n",
    "# 1D tabular\n",
    "tab_c = 1 # tabular channels\n",
    "tab_d = 2000 # tabular features\n",
    "# \n",
    "n_classes = 4 # classification\n",
    "\n",
    "tab_tensor = torch.rand(size=(n, tab_c, tab_d)) \n",
    "img_2d_tensor = torch.rand(size=(n, h, w, img_c))\n",
    "img_3d_tensor = torch.rand(size=(n, d, h, w, img_c))\n",
    "\n",
    "\n",
    "# derive a target\n",
    "target = torch.rand(size=(n,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa2ddc",
   "metadata": {},
   "source": [
    "Given the original data as tensors, we instantiate `MMDataset`, a lightweight wrapper for the torch `Dataset` and pass this into a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df31a7b4a69e7aa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = MMDataset([tab_tensor, img_2d_tensor, img_3d_tensor], target)\n",
    "train, test, val = torch.utils.data.random_split(data, [0.7, 0.15, 0.15]) # create 70-15-15 train-val-test split\n",
    "\n",
    "loader_args = {\n",
    "    \"shuffle\": True, \n",
    "    \"batch_size\": 16, \n",
    "}\n",
    "\n",
    "train_loader = DataLoader(train, **loader_args)\n",
    "val_loader = DataLoader(val, **loader_args)\n",
    "test_loader = DataLoader(test, **loader_args)\n",
    "# fetch batch \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "[tab_sample, img_sample_2d, img_sample_3d], target = next(iter(train_loader))\n",
    "\n",
    "tab_sample = tab_sample.to(device)\n",
    "img_sample_2d = img_sample_2d.to(device)\n",
    "img_sample_3d = img_sample_3d.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9409a237",
   "metadata": {},
   "source": [
    "### Instantiate HEALNet\n",
    "\n",
    "The non-optional arguments to instantiate HEALNet are: \n",
    "\n",
    "* **n_modalities** (int): Maximum number of modalities for forward pass. Note that fewer modalities can be passed if modalities for individual samples are missing (see `.forward()`)\n",
    "*  **channel_dims** (List[int]): Number of channels or tokens for each modality. Length must match ``n_modalities``. The channel_dims are non-spatial dimensions where positional encoding is not required. \n",
    "* **num_spatial_axes** (List[int]): Spatial axes for each modality.The each spatial axis will be assigned positional encodings, so that ``num_spatial_axis`` is 2 for 2D images, 3 for Video/3D images. \n",
    "* **out_dims** (int): Output shape of task-specific head. Forward pass returns logits of this shape. \n",
    "\n",
    "\n",
    "As such, the input for each modality should be of shape ``(b, (*spatial_dims) c)``, where ``c`` corresponds to the dimensions where positional encoding does not matter (e.g., color channels, set-based features, or tabular features). The `spatial_dims` are the dimensions where preserving structural signal is crucial for the model to learn (e.g., the height x width x depth of the 3D image). \n",
    "\n",
    "#### On tabular modalities\n",
    "\n",
    "One common exception to this are tabular modalities. Many tabular modalities do not contain inherent structure and are just an unordered bag of features. In this case, positional encodings add noise (as they don't mean anything. Therefore, we encode this as 2000 channels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "504d15c029cf68e0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spatial_axes=[1, 2, 3], channels=[2000, 3, 3]\n",
      "HealNet(\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x ModuleList(\n",
      "      (0): PreNorm(\n",
      "        (fn): Attention(\n",
      "          (to_q): Linear(in_features=16, out_features=512, bias=False)\n",
      "          (to_kv): Linear(in_features=2005, out_features=1024, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=16, bias=True)\n",
      "            (1): LeakyReLU(negative_slope=0.01)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_context): LayerNorm((2005,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): PreNorm(\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=16, out_features=128, bias=True)\n",
      "            (1): SELU()\n",
      "            (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): PreNorm(\n",
      "        (fn): Attention(\n",
      "          (to_q): Linear(in_features=16, out_features=512, bias=False)\n",
      "          (to_kv): Linear(in_features=13, out_features=1024, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=16, bias=True)\n",
      "            (1): LeakyReLU(negative_slope=0.01)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_context): LayerNorm((13,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): PreNorm(\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=16, out_features=128, bias=True)\n",
      "            (1): SELU()\n",
      "            (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): PreNorm(\n",
      "        (fn): Attention(\n",
      "          (to_q): Linear(in_features=16, out_features=512, bias=False)\n",
      "          (to_kv): Linear(in_features=18, out_features=1024, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=16, bias=True)\n",
      "            (1): LeakyReLU(negative_slope=0.01)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_context): LayerNorm((18,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): PreNorm(\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=16, out_features=128, bias=True)\n",
      "            (1): SELU()\n",
      "            (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_q): Linear(in_features=16, out_features=512, bias=False)\n",
      "            (to_kv): Linear(in_features=16, out_features=1024, bias=False)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=16, bias=True)\n",
      "              (1): LeakyReLU(negative_slope=0.01)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
      "              (1): SELU()\n",
      "              (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (to_logits): Sequential(\n",
      "    (0): Reduce('b n d -> b d', 'mean')\n",
      "    (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): Linear(in_features=16, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "spatial_axes = []\n",
    "channels = []\n",
    "for tensor in [tab_sample, img_sample_2d, img_sample_3d]:\n",
    "    b, *spatial_dims, c = tensor.shape\n",
    "    spatial_axes.append(len(spatial_dims))\n",
    "    channels.append(c)\n",
    "    \n",
    "print(f\"{spatial_axes=}, {channels=}\")\n",
    "\n",
    "model = HealNet(\n",
    "            n_modalities=3, \n",
    "            channel_dims=channels, # (2000 (tabular), 3 (2 D img), 3 (2D image))\n",
    "            num_spatial_axes=spatial_axes, # spatial/temporal tensor dimensions\n",
    "            out_dims = n_classes,  \n",
    "            l_d=l_d, \n",
    "            l_c=l_c, \n",
    "            fourier_encode_data=True, \n",
    "        )\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb017f7",
   "metadata": {},
   "source": [
    "Additionally, HEALNet can be tuned with a number of optional parameters: \n",
    "\n",
    "| Parameter              | Type    | Description                                                                                                                                                                           | Default  |\n",
    "|------------------------|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|\n",
    "| num_freq_bands         | int     | Number of frequency bands for positional encodings.                                                                                                                                   | 2        |\n",
    "| max_freq               | float   | Maximum frequency for positional encoding.                                                                                                                                            | 10       |\n",
    "| l_c                    | int     | Number of channels for latent bottleneck array (akin to a \"learned query array\").                                                                                                     | 128      |\n",
    "| l_d                    | int     | Dimensions for latent bottleneck.                                                                                                                                                     | 128      |\n",
    "| x_heads                | int     | Number of heads for cross attention.                                                                                                                                                  | 8        |\n",
    "| l_heads                | int     | Number of heads for latent attention.                                                                                                                                                 | 8        |\n",
    "| cross_dim_head         | int     | Dimension of each cross attention head.                                                                                                                                               | 64       |\n",
    "| latent_dim_head        | int     | Dimension of each latent attention head.                                                                                                                                              | 64       |\n",
    "| attn_dropout           | float   | Dropout rate for attention layers.                                                                                                                                                    | 0        |\n",
    "| ff_dropout             | float   | Dropout rate for feed-forward layers.                                                                                                                                                 | 0        |\n",
    "| weight_tie_layers      | bool    | False for weight sharing between fusion layers, True for specific weights for each layer. If True, the number of parameters multiplies by `depth`.                                      | False    |\n",
    "| fourier_encode_data    | bool    | Whether to use positional encoding. Recommended if meaningful spatial structure should be preserved.                                                                                   | True     |\n",
    "| self_per_cross_attn    | int     | Number of self-attention layers per cross-attention layer.                                                                                                                             | 1        |\n",
    "| final_classifier_head  | bool    | Whether to include a final classifier head.                                                                                                                                           | True     |\n",
    "| snn                    | bool    | Whether to use a self-normalizing network.                                                                                                                                            | True     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9e77fe00f07c904",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tab_sample.shape=torch.Size([16, 1, 2000])\n",
      "img_sample_2d.shape=torch.Size([16, 224, 224, 3])\n",
      "img_sample_3d.shape=torch.Size([16, 12, 224, 224, 3])\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "logits = model([tab_sample, img_sample_2d, img_sample_3d])\n",
    "\n",
    "print(f\"{tab_sample.shape=}\")\n",
    "print(f\"{img_sample_2d.shape=}\")\n",
    "print(f\"{img_sample_3d.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9452f27",
   "metadata": {},
   "source": [
    "## Handling missing modalities\n",
    "\n",
    "HEALNet natively handles missing modalities through its iterative architecture. If you encounter a missing data point in your pipeline, you can simply skip it by passing in `None` instead of the tensor. The model will stil return the embedding or prediction based on the present modalities. \n",
    "\n",
    "Note that `verbose=True` will log during each forward pass, so it's recommended to turn this off in the train loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b188c055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing modalities indices: [1]\n",
      "Skipping update in fusion layer 1 for missing modality 2\n",
      "Skipping update in fusion layer 2 for missing modality 2\n",
      "Skipping update in fusion layer 3 for missing modality 2\n",
      "Missing modalities indices: [0, 2]\n",
      "Skipping update in fusion layer 1 for missing modality 1\n",
      "Skipping update in fusion layer 1 for missing modality 3\n",
      "Skipping update in fusion layer 2 for missing modality 1\n",
      "Skipping update in fusion layer 2 for missing modality 3\n",
      "Skipping update in fusion layer 3 for missing modality 1\n",
      "Skipping update in fusion layer 3 for missing modality 3\n"
     ]
    }
   ],
   "source": [
    "logits_missing = model([tab_sample, None, img_sample_3d], verbose=True)\n",
    "latent_missing = model([None, img_sample_2d, None], return_embeddings=True, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path=".gitattributes">
data/tcga/omic/tcga_gbmlgg_all_clean.csv.zip filter=lfs diff=lfs merge=lfs -text
data/tcga/omic/tcga_hnsc_all_clean.csv.zip filter=lfs diff=lfs merge=lfs -text
data/tcga/omic/tcga_blca_all_clean.csv.zip filter=lfs diff=lfs merge=lfs -text
data/tcga/omic/tcga_brca_all_clean.csv.zip filter=lfs diff=lfs merge=lfs -text
data/tcga/omic/tcga_kirc_all_clean.csv.zip filter=lfs diff=lfs merge=lfs -text
data/tcga/omic/tcga_kirp_all_clean.csv.zip filter=lfs diff=lfs merge=lfs -text
data/tcga/omic/tcga_lihc_all_clean.csv.zip filter=lfs diff=lfs merge=lfs -text
data/tcga/omic/tcga_luad_all_clean.csv.zip filter=lfs diff=lfs merge=lfs -text
data/tcga/omic/tcga_skcm_all_clean.csv.zip filter=lfs diff=lfs merge=lfs -text
data/tcga/omic/tcga_coadread_all_clean.csv.zip filter=lfs diff=lfs merge=lfs -text
data/tcga/omic/tcga_paad_all_clean.csv.zip filter=lfs diff=lfs merge=lfs -text
data/tcga/omic/tcga_ucec_all_clean.csv.zip filter=lfs diff=lfs merge=lfs -text
data/tcga/omic/tcga_lusc_all_clean.csv.zip filter=lfs diff=lfs merge=lfs -text
data/tcga/omic/tcga_stad_all_clean.csv.zip filter=lfs diff=lfs merge=lfs -text
data/tcga/omic_xena/brca_master.csv filter=lfs diff=lfs merge=lfs -text
data/tcga/omic_xena/kirp_master.csv filter=lfs diff=lfs merge=lfs -text
data/tcga/omic_xena/ucec_master.csv filter=lfs diff=lfs merge=lfs -text
data/tcga/omic_xena/blca_master.csv filter=lfs diff=lfs merge=lfs -text
</file>

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# subrepos
PORPOISE/
MultiModN/
gdc-client/
CLAM/
explanations/
logs/
preprocessed_dir/

wandb/

.idea/

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# PEP 582; used by e.g. github.com/David-OConnor/pyflow
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/
/CLAM/
</file>

<file path="environment.yml">
name: healnet
channels: 
  - anaconda
  - conda-forge
  - defaults
  - pytorch
  - nvidia
dependencies:
  - einops
  - jupyter
  - pytest
  - python=3.9
  - pytorch=1.13.1=cuda112py39hb0b7ed5_200
  - cudatoolkit=11.3.1
  - cudnn=8.4.1.50
  - pandas=1.5.3
  - scikit-learn
  - scikit-survival
  - seaborn
  - torchmetrics=1.2.1
  - torchvision=0.14.1=cuda112py39h1a1de93_0
  - tqdm
  - python-box
  - pip
  - pip:
      - h5py
      - invoke
      - torchsummary
      - torch_optimizer
      - wandb==0.15.8
      - opencv-python
      - openslide-python
</file>

<file path="False">
2023-04-12 16:35:15,806: INFO: [32mSuccessfully downloaded[0m: 2
</file>

<file path="LICENSE">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
</file>

<file path="pyproject.toml">
[build-system]
requires=["setuptools >= 61.0"]
build-backend = "setuptools.build_meta"
</file>

<file path="README.md">
# HEALNet

Code repository for paper [**_HEALNet: Multimodal Fusion for Heterogeneous Biomedical Data_**](https://arxiv.org/abs/2311.09115)

## An architecture for flexible and robust multimodal pipelines

[[pdf](https://arxiv.org/pdf/2311.09115) | [Installation](#Quickstart) | [Experimental Data](#data) | [Getting Started](./tutorial/01_Getting_Started.ipynb) | [Cite](#citation)]

<img src="assets/healnet_overview.png" width="850">


## Why use this model? 

* **Preserve modality-specific signal**: HEALNet learns modality-specific weights for each modality and projects it into a shared embedding. Positional encodings capture spatial signal for the specified number of spatial dimensions. 
* **Learn cross-modal interactions**: By passing a latent embedding through the fusion layers (Figure B), we 1) iteratively encode each modality into the share embedding which 2) consequently becomes the context for the next modality. As such, this latent becomes a "learned query" that is updated in each layer pass.
* **Handling missing modalities**: The model's iterative architecture allows skipping missing modalities for individual samples at train or inference time without adding much noise. This allows to train on **all** data without being restricted to the intersection of available modalities. 
* **Model inspection**: The model can be inspected through the modality-specific attention weights. 

## Updates

* **8/12/2024**: Camera-ready release (v0.1.0) available! 
* **25/09/2024**: HEALNet has been accepted to NeurIPS 2024. [Reach out](mailto:konstantin.hemker@cl.cam.ac.uk) to chat in Vancouver! 


## Quickstart 

### Installation

First, locally install HEALNet using pip.

```bash 
git clone git@github.com:konst-int-i/healnet.git
cd healnet
conda create --name healnet python=3.9
```

We provide two sets of dependencies for installation:
* Lightweight: access to `healnet.models` 
* All: access to entire experimental pipeline

#### Lightweight dependencies

We recommend the lightweight installation if you only want to use the `healnet.models` to build on top of HEALNet in a different pipeline.

```bash
pip install -e .
```

#### Full dependencies

The full experiments require some further dependencies which can be installed using

```bash
pip install -e .[all]
```

Note that you require the `.[all]` installation to run the tutorial. 


You can test the installation by running the `pytests`

```bash
pytest -v healnet/tests/
```


### Usage

```python
from healnet import HealNet
from healnet.etl import MMDataset
import torch
import einops

# synthetic data example
n = 100 # number of samples
b = 4 # batch size
img_c = 3 # image channels
tab_c = 1 # tabular channels
tab_d = 2000 # tabular features
# 2D dims
h = 224 # image height
w = 224 # image width
# 3d dim
d = 12

tab_tensor = torch.rand(size=(n, tab_c, tab_d)) 
img_tensor_2d = torch.rand(size=(n, h, w, img_c)) # h w c
img_tensor_3d = torch.rand(size=(n, d, h, w, img_c)) # d h w c
dataset = MMDataset([tab_tensor, img_tensor_2d, img_tensor_3d])

[tab_sample, img_sample_2d, img_sample_3d] = dataset[0]

# batch dim for illustration purposes
tab_sample = einops.repeat(tab_sample, 'c d -> b c d', b=1) # spatial axis: None (pass as 1)
img_sample_2d = einops.repeat(img_sample_2d, 'h w c -> b h w c', b=1) # spatial axes: h w
img_sample_3d = einops.repeat(img_sample_3d, 'd h w c -> b d h w c', b=1) # spatial axes: d h w

tensors = [tab_sample, img_sample_2d, img_sample_3d]


model = HealNet(
            n_modalities=3, 
            channel_dims=[2000, 3, 3], # (2000, 3, 3) number of channels/tokens per modality
            num_spatial_axes=[1, 2, 3], # (1, 2, 3) number of spatial axes (will be positionally encoded to preserve spatial information)
            out_dims = 4
        )

# example forward pass
logits = model(tensors)
```

Please view our [Getting Started Notebook](./tutorial/01_Getting_Started.ipynb) for a more detailed example.


## Reproducing experiments

If you want to reproduce the results in the paper instead of using HEALNet as a standalone module, you need to install a few more dependencies. 

### Conda/Mamba environment

Install or update the conda/mamba environment using and then activate. For a faster installation, we recommend using `mamba`. 

```
conda env update -f environment.yml
```


### CLI for additional dependenceis 

On Mac or Linux, you can install the below dependencies using the command line

```bash
invoke install --system <system>
```
for both `linux` and `mac`. 

This will auto-install the requirements below (OpenSlide and GDC client). Please follow detailed instructions below if our pre-written installation fails.  

#### Openslide
Note that for `openslide-python` to work, you need to install `openslide` separately on your system. 
See [here](https://openslide.org/download/) for instructions. 

#### GDC client
To download the WSI data, you need to install the [gdc-client](https://docs.gdc.cancer.gov/Data_Transfer_Tool/Users_Guide/Data_Download_and_Upload/) for your respective platform


### Data

#### Multiomic download

We are using git-lfs to store the pre-processed mutation, CNV, and gene expression data. 

```bash
sudo apt-get install git-lfs
git lfs install
git lfs pull
```

This will pull the data into `data/tcga/omic` and `data/tcga/omic_xena`. 


#### WSI Download
From the root of the repository, run

1. Specify the path to the gdc-client executable in `main.yml` (this will likely be the repository root if you installed the dependencies using `invoke install`). 
2. Run `invoke download --dataset <dataset> --config_path <config>`, e.g., invoke download --dataset brca

If you are unsure about which arguments are available, you can always run `invoke download --help`.

The script downloads the data using the given manifest files in `data/tcga/gdc_manifests/full` and save it in the data folder under `tcga/wsi/<dataset>` taking the following structure: 

```
tcga/wsi/<dataset>/
	 slide_1.svs
	 slide_2.svs
	 ...
```

If a data manifest file is not available for a given cancer site, you can select the files and download the manifest using the [NIH Genomic Data Commons Data Portal](https://portal.gdc.cancer.gov/). You can filter the .svs tissue and diagnostics slide files   

### Preprocessing

To ensure comparability with baselines, want to have the option to run the model in the WSI patches and extracted features using the [CLAM](https://github.com/mahmoodlab/CLAM) package. 

To extract he patches, run

```bash 
invoke preprocess --dataset <dataset> --config <config> --level <level>
```
Which will extract to the following structure

```
tcga/wsi/<dataset>_preprocessed/
	 masks
    		 slide_1.png
    		 slide_2.png
    		 ...
	 patches
    		 slide_1.h5
    		 slide_2.h5
    		 ...
	 stitches
    		 slide_1.png
    		 slide_2.png
    		 ...
	 process_list_autogen.csv
```

Note that the slide.h5 files contain the coordinates of the patches that are to be read in 
via OpenSlide (x, y coordinates). 

On first run of the pipeline, the script will add an additional folder called `patch_features` which contains the ResNet50 extracted features after patch normalisation as a 2048-dimensional tensor (using PyTorch serialisation). 

```
	 patch_features
    		 slide_1.pt
    		 slide_2.pt
    		 ...
```


### Datasets

This repo contains the manifests and scripts to easily download the following 8 cancer sites from The Cancer Genome Atlas. You can use the GDC Data Access Tool and use the same scripts if you require additional data.  
 

#### TCGA

- [BLCA](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=16056367): Urothelial Bladder Carcinoma 
- [BRCA](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=3539225): Breast Invasive Carcinoma 
- [UCEC](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=19039602): Uterine Corpus Endometrial Carcinoma
- [KIRP](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=11829555): cevical Kidney Renal Papillary Cell Carcinoma
- [LUAD](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=6881474): Lung Adenocarcinoma 
- [LUSC](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=16056484): Lung Squamous Cell Carcinoma
- [PAAD](https://gdc.cancer.gov/resources-tcga-users/tcga-code-tables/tcga-study-abbreviations): Pancreatic adenocarcinoma
- [HNSC](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=11829589): Head and Neck Squamous Cell Carcinoma 

#### Biobank

To be added

## Running Experiments

### Single run

Given the configuration in `config.yml`, you can launch a single run using. Note that all below commands assume that you are in the repository root. 

```bash
python3 healnet/main.py
```

To prevent import errors, you may have to add your local path to the `PYTHONPATH`

```bash
export PYTHONPATH=<path_to_repository>:$PYTHONPATH
```

You can view the available command line arguments using 

```bash
python3 healnet/main.py --help
```

### Full run

```bash
python3 healnet/main.py --mode run_plan
```

### Hyperparameter search

You can launch a hyperparameter search by passing the `--hyperparameter_sweep` argument. 

```bash
python3 healnet/main.py --hyperparameter_sweep
```

Note that the sweep parameters are specified in the `config/sweep.yaml` file. If a parameter is not specified as part of the parameter sweep, the program will default to whatever is configured in `config/main_gpu.yml`
</file>

<file path="run_plan.sh">
#!/bin/bash

# Check number of arguments
if [ "$#" -lt 1 ]; then
    echo "You need to specify at least one dataset."
    exit 1
fi

# Check available GPUs
AVAILABLE_GPUS=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
if [ "$#" -gt "$AVAILABLE_GPUS" ]; then
    echo "You have requested to run on $# GPUs but only $AVAILABLE_GPUS GPUs are available."
    exit 1
fi

# Run the command for each dataset
counter=0
for dataset in "$@"; do
    CUDA_VISIBLE_DEVICES=$counter python healnet/main.py --mode run_plan --dataset $dataset &
    counter=$((counter + 1))
done

# Wait for all processes to complete
wait
</file>

<file path="setup.cfg">
[metadata]
name = healnet
version = 0.1.0
description = Scalable multi-modal deep learning architecture in PyTorch
long_description = file: README.md
author = Konstantin Hemker
author_email = konstantin.hemker@cl.cam.ac.uk
license = Apache 2.0
classifiers =
    Programming Language :: Python :: 3

[options]
packages = find:
package_dir =
    = . 
install_requires =
    einops
    torch>=1.13.1
    torchvision>=0.14.1
    pandas>=1.5.3
    tqdm
    python-box
    scikit-learn
    pytest
    pyyaml
    torchsummary
    torchmetrics

python_requires = >=3.6

[options.extras_require]
all = 
    jupyter
    h5py
    openslide-python
    seaborn
    invoke
    scikit-survival
    wandb

[options.packages.find]
include = healnet
</file>

<file path="sweep.yaml">
method: bayes
metric:
  goal: minimize
  name: train_loss
parameters:
  clf.n_classes:
    value: 4
  data.level:
    value: 3
  data_path:
    value: /net/archive/export/tcga
  gdc_client:
    value: /home/kh701/gdc-client
  log_path:
    value: /home/kh701/pycharm/x-perceiver/logs
  model.output_dims:
    value: 4
  model_params.class_weights:
    values:
      - "True"
      - "False"
  model_params.output_dims:
    value: 4
  model_params.weight_decay:
    distribution: uniform
    min: 0
    max: 0.001
  model_params.num_freq_bands:
    distribution: int_uniform
    min: 2
    max: 8
  model_params.depth:
    distribution: int_uniform
    min: 1
    max: 4
  model_params.max_freq:
    distribution: int_uniform
    min: 2.
    max: 8.
  model_params.num_latents:
    values:
      - 4
      - 16
      - 32
      - 64
  model_params.latent_dim:
    values:
      - 4
      - 16
      - 32
      - 64
  model_params.attn_dropout:
    distribution: uniform
    min: 0
    max: 0.9
  model_params.ff_dropout:
    distribution: uniform
    min: 0
    max: 0.9
  optimizer.lr:
    distribution: uniform
    max: 0.002
    min: 0.0005
  optimizer.max_lr:
    value: 0.002
  optimizer.momentum:
    distribution: uniform
    max: 0.9
    min: 0.0
  seed:
    value: 42
  survival.loss:
    value: nll
  task:
    value: classification
  tcga_path:
    value: /net/archive/export/tcga/tcga
  train_loop.batch_size:
    value: 4
  train_loop.checkpoint_interval:
    value: 20
  train_loop.epochs:
    value: 20
  train_loop.eval_interval:
    value: 10
  wandb:
    value: "True"
program: x_perceiver/main.py
</file>

<file path="tasks.py">
from invoke import task
from healnet.utils import Config
from torchvision import transforms
from pathlib import Path
from openslide import OpenSlide
import pandas as pd
import torch
from tqdm import tqdm
import os
import h5py
import torchvision.models as models


@task
def install(c, system: str):
    assert system in ["linux", "mac"], "Invalid OS specified, must be one of 'linux' or 'mac'"

    print(f"Installing gdc-client for {system}...")
    if system == "linux":
        c.run("curl -0 https://gdc.cancer.gov/files/public/file/gdc-client_v1.6.1_Ubuntu_x64.zip "
              "--output gdc-client.zip")
        c.run("unzip gdc-client.zip")
    if system == "mac":
        c.run("curl -0 https://gdc.cancer.gov/files/public/file/gdc-client_v1.6.1_OSX_x64.zip "
              "--output gdc-client.zip")
        c.run("unzip gdc-client.zip")
    print(f"Installed gdc-client at {os.getcwd()}")
    # cleanup
    os.remove("gdc-client.zip")

@task
def download(c, dataset:str, config:str="config/main_gpu.yml", samples: int = None):
    valid_datasets = ["brca", "blca", "kirp", "ucec", "hnsc", "paad", "luad", "lusc"]
    conf = Config(config).read()
    download_dir = Path(conf.tcga_path).joinpath(f"wsi/{dataset}")

    # create download dir if doesn't exist (first time running)
    if not download_dir.exists():
        download_dir.mkdir(parents=True)

    assert dataset in valid_datasets, f"Invalid dataset arg, must be one of {valid_datasets}"

    manifest_path = Path(f"./data/tcga/gdc_manifests/filtered/{dataset}_wsi_manifest_filtered.txt")
    # manifest_path = Path(conf.tcga_path).joinpath(f"gdc_manifests/filtered/{dataset}_wsi_manifest_filtered.txt")
    # Download entire manifest unless specified otherwise
    if samples is not None:
        manifest = pd.read_csv(manifest_path, sep="\t")
        manifest = manifest.sample(n=int(samples), random_state=42)
        tmp_path = manifest_path.parent.joinpath(f"{dataset}_tmp.txt")
        manifest.to_csv(tmp_path, sep="\t", index=False)
        print(f"Downloading {manifest.shape[0]} files from {dataset} dataset...")
        c.run(f"{conf.gdc_client} download -m {tmp_path} -d {download_dir}")
        # cleanup
        os.remove(tmp_path)

    else:
        command = f"{conf.gdc_client} download -m {manifest_path} -d {download_dir}"
        try:
            c.run(command)
        except Exception as e:
            print(f"Error occurred: {e}")
            print(f"Command: {command}")

    # flatten directory structure (required to easily run CLAM preprocessing)
    flatten(c, dataset, config)

@task
def flatten(c, dataset: str, config: str):
    """
    Flattens directory structure for WSI images after download using the GDC client from
     `data_dir/*.svs` instead of `data_dir/hash_subdir/*.svs`.
    Args:
        c:
        dataset:
        config:

    Returns:
    """
    conf = Config(config).read()
    download_dir = Path(conf.tcga_path).joinpath(f"wsi/{dataset}")
    # flatten directory structure
    c.run(f"find {download_dir} -type f -name '*.svs' -exec mv {{}} {download_dir} \;")
    # remove everything that's not a .svs file
    c.run(f"find {download_dir} ! -name '*.svs' -delete")

@task
def preprocess(c, dataset: str, level: int, config: str="config/main_gpu.yml", step:str= "patch"):
    """
    Preprocesses WSI images for downstream tasks.
    Args:
        c:
        dataset:
        config:

    Returns:

    """
    conf = Config(config).read()
    raw_path = Path(conf.tcga_path).joinpath(f"wsi/{dataset}")
    prep_path = Path(conf.tcga_path).joinpath(f"wsi/{dataset}_preprocessed_level{level}")
    # create prep dir
    prep_path.mkdir(parents=True, exist_ok=True)

    assert os.path.exists(raw_path), f"Raw data path not found: {raw_path}"
    valid_steps = ["patch", "features"]
    assert step in valid_steps, f"Invalid step arg, must be one of {valid_steps}"

    # clone CLAM repo if doesn't exist
    if not os.path.exists("CLAM/"):
        c.run("git clone git@github.com:mahmoodlab/CLAM.git")


    if not os.path.exists(prep_path.joinpath("valid_prep_ids.csv")):
        # check which slides have specified level available (only pass those to preprocessing)
        valid_ids = []
        for slide_id in os.listdir(raw_path):
            # check whether specified level is available in slide
            slide = OpenSlide(raw_path.joinpath(f"{slide_id}"))
            try:
                slide.level_dimensions[int(level)]
                valid_ids.append(slide_id)
            except IndexError as e:
                print(f"Level {level} not available for slide {slide_id}")
                continue

        # write temp csv file with valid slide ids to pass to CLAM
        valid_slide_df = pd.DataFrame({"slide_id": valid_ids})
        valid_slide_df.to_csv(prep_path.joinpath("valid_prep_ids.csv"), index=False)

    if step == "patch":
        c.run(f"python CLAM/create_patches_fp.py --source {raw_path} --save_dir {prep_path} --process_list valid_prep_ids.csv "
          f"--patch_size {int(conf.data.patch_size)} --patch_level {int(level)} --seg --patch --stitch")

    if step == "features":
        # only take preprocessed slides
        slide_ids = [x.rstrip(".h5") for x in os.listdir(prep_path.joinpath("patches")) if x.endswith(".h5")]
        # load patch coords
        coords = {}
        for slide_id in slide_ids:
            patch_path = prep_path.joinpath(f"patches/{slide_id}.h5")
            try:
                h5_file = h5py.File(patch_path, "r")
                patch_coords = h5_file["coords"][:]
                coords[slide_id] = patch_coords
            except FileNotFoundError as e:
                print(f"No patches available for file {patch_path}")
                pass
        max_patches = max([coords.get(key).shape[0] for key in coords.keys()])
        print(f"Max patches: {max_patches}")

        # load in resnet50 model
        # patch_encoder = models.resnet50(pretrained=True)
        patch_encoder = models.resnet50(weights=models.resnet.ResNet50_Weights.IMAGENET1K_V2)
        patch_encoder = torch.nn.Sequential(*(list(patch_encoder.children())[:-1])) # remove classifier head
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        patch_encoder.to(device)
        patch_encoder.eval()

        patch_tensors = torch.zeros(max_patches, 2048)
        num_slides = len(slide_ids)
        # extract features
        for slide_count, slide_id in enumerate(coords.keys()):
            feat_path = prep_path.joinpath("patch_features")
            save_path = feat_path.joinpath(f"{slide_id}.pt")
            # check if features already extracted
            if os.path.exists(save_path):
                print(f"Features already extracted for slide {slide_id}, skipping...")
                continue

            slide = OpenSlide(raw_path.joinpath(f"{slide_id}.svs"))
            print(f"slide {slide_count+1}/{num_slides}")

            for idx, coord in enumerate(tqdm(coords[slide_id])):
                # print(f"{dataset.upper()} Level {level}: Processing patch {idx} of {len(coords[slide_id])} for "
                #       f"slide {slide_count+1}/{num_slides}")
                x, y = coord
                region_transform = transforms.Compose([
                transforms.Lambda(lambda image: image.convert("RGB")), # need to convert to RGB for ResNet encoding
                transforms.ToTensor(),
                transforms.Resize((224, 224)), # resize in line with ResNet50
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalisation
                ])
                patch_region = region_transform(slide.read_region((x, y), level=int(level), size=(256, 256)))
                patch_region = patch_region.to(device)
                patch_region = patch_region.unsqueeze(0)
                patch_features = patch_encoder(patch_region)
                patch_tensors[idx] = patch_features.cpu().detach().squeeze()

            # save features
            if not feat_path.exists():
                feat_path.mkdir(parents=False)
            torch.save(patch_tensors, save_path)
</file>

</files>
